rk4VD6Ymf_0,"This work proposes DIVE ( distributional inclusion vector embeddings ) , an unsupervised method for hypernymy discovery that preserves the inclusion property of old fashioned sparse BOW representations ( i.e. , that hypernyms tend to have higher counts in more varies contexts than hyponyms ) .",fact
rk4VD6Ymf_1,The learned representations are evaluated on a large set of datasets and are shown to work well .,evaluation
rk4VD6Ymf_2,"The work is very thorough , and almost reads as a systematic study in places .",evaluation
rk4VD6Ymf_3,"Unfortunately , this makes it hard to follow sometimes ,",evaluation
rk4VD6Ymf_4,"and although the work is generally interesting ,",evaluation
rk4VD6Ymf_5,I am left to wonder about its novelty and general impact on the field .,evaluation
rk4VD6Ymf_6,The main proposition essentially comes down to a slight tweak in word2vec 's SGNS method :,evaluation
rk4VD6Ymf_7,"instead of shifting the PMI value of word co-occurrences by 1/k , we shift by <VAR> and add a non-negativity constraint , i.e. , we explicitly force Eq . 2 to be true in perfect reconstructions .",fact
rk4VD6Ymf_8,This makes sense and I am surprised nobody has done it before .,evaluation
rk4VD6Ymf_9,"It feels like a minor tweak ,",evaluation
rk4VD6Ymf_10,"however , and any major improvement seems to be largely dependent on the scoring function , rather than on this slightly different objective .",evaluation
rk4VD6Ymf_11,"Consequently , I feel like the paper is in two minds about what it wants to be : a systematic study of hypernymy detection methods , or an introduction of a novel algorithm for learning distributional inclusion embeddings .",evaluation
rk4VD6Ymf_12,"I like the main ideas of this work and I think it 's great that the study is so thorough ,",evaluation
rk4VD6Ymf_13,but I do n't think it should be accepted in its current form .,evaluation
rk4VD6Ymf_14,"Main concerns : - Presentation : the experiments show that ( 1 ) DIVE outperforms GE , HyperScore and H-Feature baselines ;",fact
rk4VD6Ymf_15,( 2 ) which scoring function works best for DIVE ;,request
rk4VD6Ymf_16,"( 3 ) DIVE outperforms SBOW in many cases , but not always , though better on average ;",fact
rk4VD6Ymf_17,and ( 4 ) we can use DIVE for WSD ( only shown qualitatively ) .,fact
rk4VD6Ymf_18,"The paper spans 13 pages , excluding appendices ,",fact
rk4VD6Ymf_19,which is rather long .,evaluation
rk4VD6Ymf_20,"I feel that ( 2 ) , ( 3 ) and ( 4 ) are part of a systematic study/review paper ,",evaluation
rk4VD6Ymf_21,"while ( 1 ) could be interesting in and of itself , if it included a full comparison against alternative methods .",evaluation
rk4VD6Ymf_22,"The way results are presented in the tables is confusing ,",evaluation
rk4VD6Ymf_23,and it 's unclear why only one baseline is included in each case .,evaluation
rk4VD6Ymf_24,- Comparison : it appears that there are methods missing from the results tables .,fact
rk4VD6Ymf_25,"On HyperLex , for example , results as high as 0.512 ( Poincare embeddings ) , 0.540 ( HyperVec ) and 0.686 ( LEAR ) have been reported ,",fact
rk4VD6Ymf_26,while the highest result in the paper is 34.5 .,fact
rk4VD6Ymf_27,That 's a big difference .,evaluation
rk4VD6Ymf_28,"On Weeds ' BLESS , it 's 68.6 versus e.g. 0.75 for Kiela et al. ( unsupervised , using images ) and 0.850 for HyperVec .",fact
rk4VD6Ymf_29,"If these results were omitted due to space , I think experiment ( 2 ) and ( 4 ) can safely be moved to the appendix .",evaluation
rk4VD6Ymf_30,"Especially for something that hinges on being a review paper , such as this work , it is important to be complete .",request
rk4VD6Ymf_31,"In short , I think the presentation does n't help ;",evaluation
rk4VD6Ymf_32,I am left to wonder what the main contribution is of the work ;,evaluation
rk4VD6Ymf_33,and I think the comparison to previous work is incomplete .,evaluation
rk4VD6Ymf_34,Questions : - Why use WaCkypedia ?,non-arg
rk4VD6Ymf_35,"It 's old and , by now , small .",evaluation
rk4VD6Ymf_36,"It would be interesting to try all scoring functions and types of model ( PPMI , PPMI with shift , PPMI with inclusion shift ) trained on the exact same corpus and same negatives , and showing that it works best there .",request
rk4VD6Ymf_37,"- General question : Is Vendrov 's test set only yes/no , e.g. if I set the threshold really low , I get 100 % accuracy , or does it contain negatives ?",non-arg
rk4VD6Ymf_38,Same for BLESS .,non-arg
rk4VD6Ymf_39,If so how valuable is this evaluation ?,evaluation
rk4VD6Ymf_40,- To what extent are the baselines tuned in the semi-supervised case ?,request
rk4VD6Ymf_41,"If the numbers are from papers , that should be mentioned .",request
rk4VD6Ymf_42,"It would be better to use the same corpus , and the same amount of attention to tuning the results , for both cases .",request
rk4VD6Ymf_43,Minor/Typos : - Medicl,request
rk4VD6Ymf_44,"- "" From the recent review ( Santus et al ) ... suggested by the review study ( Vulic et al ) "" this way of citing reviews is n't very pretty",evaluation
Hk-OLVKeM_0,"The authors propose to speed up RL techniques , such as DQN , by utilizing expert demonstrations .",fact
Hk-OLVKeM_1,"The expert demonstrations are sequences of consecutive states that do not include actions , which is closer to a real setting of imitation learning .",fact
Hk-OLVKeM_2,The goal of this process is to extract a function that maps any given state to a subgoal .,fact
Hk-OLVKeM_3,"Subgoals are then used to learn different Q-value functions , one per subgoal .",fact
Hk-OLVKeM_4,"To learn the function that maps states into subgoals , the authors propose a surrogate reward model that corresponds to the angle between : the difference between two consecutive states ( which captures velocity or direction ) and a given subgoal .",fact
Hk-OLVKeM_5,A von Mises - Fisher distribution policy is then assumed to be used by the expert to generate actions that guide the agent toward the subgoal .,fact
Hk-OLVKeM_6,"Finally , the mapping function state - > subgoal is learned by performing a gradient descent on the expected total cost ( based on the surrogate reward function , which also has free parameters that need to be learned ) .",fact
Hk-OLVKeM_7,"Finally , the authors use the DQN platform to learn a Q-value function using the learned surrogate reward function that guides the agent to specific subgoals , depending on the situation .",fact
Hk-OLVKeM_8,"The paper is overall well-written ,",evaluation
Hk-OLVKeM_9,and the proposed idea seems interesting .,evaluation
Hk-OLVKeM_10,"However , there are rather little explanations provided to argue for the different modeling choices made , and the intuition behind them .",evaluation
Hk-OLVKeM_11,"From my understanding , the idea of subgoal learning boils down to a non-parametric ( or kernel ) regression where each state is mapped to a subgoal based on its closeness to different states in the expert 's demonstration .",fact
Hk-OLVKeM_12,It is not clear how this method would generalize to new situations .,evaluation
Hk-OLVKeM_13,There is also the issue of keeping tracking of a large number of demonstration states in memory .,fact
Hk-OLVKeM_14,"This technique reminds me of some common methods in learning from demonstrations , such as those using GPs or GMMs ,",evaluation
Hk-OLVKeM_15,"but the novelty of this technique is the fact that the subgoal mapping function is learned in an IRL fashion , by tacking into account the sum of surrogate rewards in the expert 's demonstration .",evaluation
Hk-OLVKeM_16,"The architecture of the action value estimator does not seem novel ,",evaluation
Hk-OLVKeM_17,it 's basically just an extension of DQN with an extra parameter ( subgoal g ) .,fact
Hk-OLVKeM_18,The empirical evaluation seems rather mixed .,evaluation
Hk-OLVKeM_19,"Figure 3 shows that the proposed method learns faster than DQN ,",fact
Hk-OLVKeM_20,"but Table I shows that the improvement is not statistically significant , except in two games , DefendCenter and PredictPosition .",fact
Hk-OLVKeM_21,Are these the results after all agents had converged ?,non-arg
Hk-OLVKeM_22,"Overall , this is a good paper ,",evaluation
Hk-OLVKeM_23,but focusing on only a single game ( Doom ) is a weakness that needs to be addressed,request
Hk-OLVKeM_24,because one can not tell if the choices were tailored to make the method work well for this game .,fact
Hk-OLVKeM_25,"Since the paper does not provide significant theoretical or algorithmic contribution ,",evaluation
Hk-OLVKeM_26,at least more realistic and diverse experiments should be performed .,request
HyyQKdklf_0,"Summary : Based on ideas within the context of kernel theory , the authors consider post-training of NNs as an extra training step , which only optimizes the last layer of the network .",fact
HyyQKdklf_1,"This additional step makes sure that the embedding , or representation , of the data is used in the best possible way for the considered task",evaluation
HyyQKdklf_2,( which is also reflected in the experiments ) .,evaluation
HyyQKdklf_3,"According to the authors , the contributions are the following : 1 . Post-training step : keeping the rest of the NN frozen ( after training ) , the method trains the last layer in order to "" make sure "" that the representation learned is used in the most efficient way .",fact
HyyQKdklf_4,2 . Highlighting connections with kernel techniques and RKHS optimization ( like kernel ridge regression ) .,fact
HyyQKdklf_5,3 . Experimental results .,fact
HyyQKdklf_6,"Clarity : The paper is well-written , the main ideas well-clarified .",evaluation
HyyQKdklf_7,"Importance : While the majority of papers nowadays focuses on the representation part ( i.e. , how we get to <VAR> ) ,",evaluation
HyyQKdklf_8,this paper assumes this is given and proposes how to optimize the weights in the final step of the algorithm .,fact
HyyQKdklf_9,"This by itself is not enough to boost the performance universally ( e.g. , if <VAR> is not well-trained , the problem is deeper than training the last layer ) ;",evaluation
HyyQKdklf_10,"however , it proposes an additional step that can be used in most NN architectures .",fact
HyyQKdklf_11,"From that front ( i.e. , proposing to do something different than simply training a NN ) , I find the paper interesting , that might attract some attention at the conference .",evaluation
HyyQKdklf_12,"On the other hand , to my humble opinion , the experimental results do not show a significant gain in the performances of all networks ( esp . Figure 3 and Table 1 are within the range of statistical error ) .",evaluation
HyyQKdklf_13,"In order to state something like this universally , either one needs to perform experiments with more than just MNIST/CIFAR datasets , or even more preferably , prove that the algorithm performs better .",request
HyyQKdklf_14,"Originality : It would be great to have some more theory ( if any ) for the post-training step , or investigate more cases , rather than optimizing only the last layer .",request
HyyQKdklf_15,"Comments : 1 . I assume the authors focused in the last layer of the NN for simplicity ,",evaluation
HyyQKdklf_16,but is there a reason why one might want to focus only on the last layer ?,request
HyyQKdklf_17,One reason is convexity in W of the problem ( 2 ) .,evaluation
HyyQKdklf_18,Any other ?,request
HyyQKdklf_19,2 . Have the authors considered ( even in practice only ) to include training of the last 2 layers of the NN ?,non-arg
HyyQKdklf_20,"The authors state this question in the future direction ,",fact
HyyQKdklf_21,but it would make the paper more complete to consider it here .,request
SyVv9AjWG_0,The authors present a neural embedding technique using a hyperbolic space .,fact
SyVv9AjWG_1,The idea of embedding data into a space that is not Euclidean is not new .,fact
SyVv9AjWG_2,There have been attempts to project onto ( hyper ) spheres .,fact
SyVv9AjWG_3,"Also , the proposal bears some resemblance with what is done in t-SNE , where an ( exponential ) distortion of distances is induced .",evaluation
SyVv9AjWG_4,Discussing this potential similarity would certainly broaden the readership of the paper .,request
SyVv9AjWG_5,"The organisation of the paper might be improved , with a clearer red line and fewer digressions .",request
SyVv9AjWG_6,The call to the very small appendix via eq . 17 is an example .,request
SyVv9AjWG_7,The position of Table in the paper is odd as well .,evaluation
SyVv9AjWG_8,The order of examples in Fig. 5 differs from the order in the list .,fact
SyVv9AjWG_9,The experiments are well illustrative but rather small sized .,evaluation
SyVv9AjWG_10,The qualitative assessment is always interesting,evaluation
SyVv9AjWG_11,and it is completed with some label prediction task .,fact
SyVv9AjWG_12,"Due the geometrical consideretations developed in the paper , other quality criteria like e.g. how well neighbourhoods are preserved in the embeddings would give some more insights .",evaluation
SyVv9AjWG_13,All in all the idea developed in the paper sounds interesting,evaluation
SyVv9AjWG_14,but the paper organisation seems a bit loose,evaluation
SyVv9AjWG_15,and additional aspects should be investigated .,request
SkugmHtgf_0,This paper proposes an optimization problem whose Lagrangian duals contain many existing objective functions for generative models .,fact
SkugmHtgf_1,"Using this framework , the paper tries to generalize the optimization problems by defining computationally-tractable family which can be expressed in terms of existing objective functions .",fact
SkugmHtgf_2,The paper has interesting elements,evaluation
SkugmHtgf_3,and the results are original .,evaluation
SkugmHtgf_4,The main issue is that the significance is unclear .,evaluation
SkugmHtgf_5,"The writing in Section 3 is unclear for me ,",evaluation
SkugmHtgf_6,which further made it challenging to understand the consequences of the theorems presented in that section .,evaluation
SkugmHtgf_7,Here is a big-picture question that I would like to know answer for .,non-arg
SkugmHtgf_8,Do the results of sec 3 help us identify a more useful/computationally tractable model than exiting approaches ?,request
SkugmHtgf_9,Clarification on this will help me evaluate the significance of the paper .,evaluation
SkugmHtgf_10,I have three main clarification points .,non-arg
SkugmHtgf_11,"First , what is the importance of T1 , T2 , and T3 classes defined in Def . 7 , i.e. , why are these classes useful in solving some problems ?",request
SkugmHtgf_12,"Second , is the opposite relationship in Theorem 1 , 2 , and 3 true as well , e.g. , is every linear combination of beta-ELBO and VMI is equivalent to a likelihood-based computable-objective of KL info-encoding family ?",request
SkugmHtgf_13,Is the same true for other theorems ?,request
SkugmHtgf_14,"Third , the objective of section 3 is to show that "" only some choices of lambda lead to a dual with a tractable equivalent form "" .",fact
SkugmHtgf_15,"Could you rewrite the theorems so that they truly reflect this , rather than stating something which only indirectly imply the main claim of the paper .",request
SkugmHtgf_16,Some small comments : - Eq . 4 . It might help to define MI to remind readers .,request
SkugmHtgf_17,"- After Eq . 7 , please add a proof ( may be in the Appendix ) .",request
SkugmHtgf_18,It is not that straightforward to see this .,evaluation
SkugmHtgf_19,"Also , I suppose you are saying Eq . 3 but with f from Eq . 4 .",evaluation
SkugmHtgf_20,"- Line after Eq . 8 , D_i is "" one "" of the following ...",fact
SkugmHtgf_21,Is it always the same D_i for all i or it could be different ?,request
SkugmHtgf_22,Make this more clear to avoid confusion .,request
SkugmHtgf_23,"- Last line in Para after Eq . 15 , "" This neutrality corresponds to the observations made in . . """,quote
SkugmHtgf_24,"It might be useful to add a line explaining that particular "" observation """,request
SkugmHtgf_25,"- Def . 7 , the names did not make much sense to me .",evaluation
SkugmHtgf_26,You can add a line explaining why this name is chosen .,request
SkugmHtgf_27,"- Def . 8 , the last equation is unclear .",evaluation
SkugmHtgf_28,Does the first equivalence impy the next one ?,request
SkugmHtgf_29,- Writing in Sec. 3.3 can be improved .,request
SkugmHtgf_30,"e.g. , "" all linear operations on log prob . "" is very unclear ,",evaluation
SkugmHtgf_31, stated computational constraints  which constraints ?,request
SkNXYkbkz_0,This paper introduces an algorithm for learning Wasserstein GANs for discrete distributions .,fact
SkNXYkbkz_1,Taking the dual form of the discrete Wasserstein distance introduced by [ Evans 1997 ] ( which produces a constrained optimization problem ) and using this as a basis of a GAN training algorithm .,fact
SkNXYkbkz_2,"The key algorithmic distinction from conventional GAN approaches is that the critic takes pairs of real and simulated datapoints as inputs and returns a measure of which it thinks is the real datapoint , namely <VAR> where x_r is the data point and <VAR> the generator output , rather than the critic corresponding to f directly .",fact
SkNXYkbkz_3,An architecture is proposed for this framework that guarantees that the constraints from the formulation of [ Evans 1997 ] as satisfied .,fact
SkNXYkbkz_4,The topic of this paper is timely and of clear interest to the ICLR community .,evaluation
SkNXYkbkz_5,The underlying idea is interesting,evaluation
SkNXYkbkz_6,and the architecture seems appropriate for the chosen target .,evaluation
SkNXYkbkz_7,"Further , the paper is relatively clear and easy to follow .",evaluation
SkNXYkbkz_8,"However , the experimental evaluation is not sufficiently strenuous and produces very underwhelming results .",evaluation
SkNXYkbkz_9,"Relatedly , I think the motivation behind using the discrete Wasserstein distance , though seemingly reasonable , needs more careful consideration .",request
SkNXYkbkz_10,"Unfortunately , the serious shortfalls in the experiments mean that the paper , in my opinion , falls noticeably below the acceptance threshold for ICLR .",evaluation
SkNXYkbkz_11,"Having said that , my stance might change substantially if more impressive experimental results can be obtained ;",evaluation
SkNXYkbkz_12,"without this , I am unconvinced the method actually behaves as intended .",evaluation
SkNXYkbkz_13,"Regretfully , this is probably beyond the scope of a revision during the rebuttal period",evaluation
SkNXYkbkz_14,though as it will most likely require significant algorithmic changes .,evaluation
SkNXYkbkz_15,"% % % % Shortcomings with Experiments % % % % % Put simply , I do not think that the experiments demonstrate that the approach works and actually suggest the opposite .",evaluation
SkNXYkbkz_16,The so-called “ mode collapse ” issue is effectively a sugar-coated way of saying that the method has learned to return a single output rather than learning a generative model .,evaluation
SkNXYkbkz_17,This is a huge issue and needs sorting before the paper can be seriously considered for publication .,request
SkNXYkbkz_18,The attempted at a fix at the end of the paper might be a step in the right direction but is not evaluated sufficiently,evaluation
SkNXYkbkz_19,and the preliminary results provided are not particularly promising .,evaluation
SkNXYkbkz_20,"Going past this issue , there are still a lot of problems with the experimental evaluation .",evaluation
SkNXYkbkz_21,More numerous and more difficult problems need to be considered .,request
SkNXYkbkz_22,"For example , an NLP problem would fit well with the motivation for the approach given in the intro .",evaluation
SkNXYkbkz_23,It is also necessary to demonstrate that the GAN is doing something different to just memorizing previous examples .,request
SkNXYkbkz_24,"For example , this is a problem where one can actually reasonably compare to just sampling from the data by checking that more unseen correct samples are generated then incorrect samples .",evaluation
SkNXYkbkz_25,It is worrying that the convergence plots show a single line that ostensibly comes from the “ best result ” .,evaluation
SkNXYkbkz_26,This is not a reasonable scientific comparison method,evaluation
SkNXYkbkz_27,and should be replaced by mean ( or median ) performance with uncertainty estimates ( i.e. + / - some numbers of standard deviations or quantiles ) .,request
SkNXYkbkz_28,The small number of samples from the MNIST problem really does n’t convey anything meaningful,evaluation
SkNXYkbkz_29,"– even if this looked exceptional , a small number of unqualified samples is hardly a serious evaluation metric .",evaluation
SkNXYkbkz_30,"More iterations for Figure 4a or needed to see if the WGAN keeps improving beyond on the DWGAN , noting that this has not converged at 100 % .",request
SkNXYkbkz_31,All four methods should be added to Figure 5 .,request
SkNXYkbkz_32,% % % % Is the Discrete Metric Always Better for Training the Generative Model ? % % % % %,request
SkNXYkbkz_33,"Though I think it is probably true that the discrete metric should more beneficial from the point of the view of the critic ,",evaluation
SkNXYkbkz_34,"I am not completely convinced it is always beneficial from the point of view of training the generator , which is at the end of the day what really matters .",evaluation
SkNXYkbkz_35,"Note that I am not trying to argue that discrete metric is worse ,",evaluation
SkNXYkbkz_36,just that you have n’t done enough to convince me that it is always better .,evaluation
SkNXYkbkz_37,"If indeed it is always better , please tear apart my following argument ,",request
SkNXYkbkz_38,which will hopefully provide stimulation for improving the motivation of the metric in the paper .,evaluation
SkNXYkbkz_39,"If it is not always better , the paper should be updated to outline some of the potential pathologies and the cases were you expect the approach to work well and when you do not .",request
SkNXYkbkz_40,"To demonstrate my argument , consider the training sample <VAR> and the following four example generated outputs ( 1 ) <VAR> ( 2 ) <VAR> ( 3 ) <VAR> ( 4 ) <VAR> giving respective discrete distances to of 0 , 0 , 1 , and 2 ; and continuous distances of 1.330668 , 0 , 0.667334 , and 1.334668 .",request
SkNXYkbkz_41,Now imagine we are training a GAN with the one example datapoint <VAR> .,request
SkNXYkbkz_42,"Even though ( 1 ) will lead to the target sample <VAR> after passing through the argmax function and ( 3 ) will not ,",fact
SkNXYkbkz_43,( 1 ) is also very close to ( 4 ) which has the maximum possible discrete distance .,evaluation
SkNXYkbkz_44,"Consequently , the generator for ( 1 ) is most likely not at a stable optimum and very close in the space of neural net weights to some very poor generators .",evaluation
SkNXYkbkz_45,"During training , we would thus like to guide our network towards generating ( 2 ) ,",evaluation
SkNXYkbkz_46,"as this is a stable solution , particularly given we are using stochastic optimization methods .",fact
SkNXYkbkz_47,"From this perspective , then ( 3 ) is perhaps a better generated output than ( 1 ) , a fact conveyed by the continuous metric but not the discrete metric .",fact
SkNXYkbkz_48,"In other words , even though ( 1 ) is arguably a better final solution than ( 3 ) , from the perspective of effective training , it may be favorable to use a target function that prefers ( 3 ) to ( 1 ) to better guide the training to a stable solution .",evaluation
SkNXYkbkz_49,"Once we consider the fact that our aim is not to replicate a single datapoint but learn a generator that in some way interpolates between datapoints ,",fact
SkNXYkbkz_50,it becomes even less clear if the discrete metric is better .,evaluation
SkNXYkbkz_51,"For example , small changes to the input z for ( 1 ) are likely to lead to generating samples that are very different to anything seen in the training data ( which is , in this case , a single point ) .",evaluation
SkNXYkbkz_52,"Though I do not think this argument undermines the suggested approach ,",evaluation
SkNXYkbkz_53,I do think it highlights why the supplied motivation for the approach is insufficient and needs more explicitly linking back to the training procedure .,evaluation
SkNXYkbkz_54,"At the very least , I think the above argument shows why it is not immediately clear cut that the suggested approach will perform better",evaluation
SkNXYkbkz_55,"and so needs backing up with strong empirical evidence ,",request
SkNXYkbkz_56,"which unfortunately the paper does not currently have , and/or a more convincing argument for why the discrete metric is better .",evaluation
SkNXYkbkz_57,"% % % % Other points % % % % - Tables 1 , 2 , and 3 are not worth spending more than a few lines on , let alone nearly a page .",request
SkNXYkbkz_58,They should be substantially compressed or preferably just cut ( particularly Tables 2 and 3 ) .,request
SkNXYkbkz_59,The point they convey is obvious and actually somewhat tangential to the key questions .,evaluation
SkNXYkbkz_60,"- It worries me that the method learns an h that explicitly takes y as input , not x.",evaluation
SkNXYkbkz_61,"This is a notational issue in the exposition , but also , more importantly , raises questions about whether the original linear programming problem is actually being solved",evaluation
SkNXYkbkz_62,because the inputs from the generative method are not discrete variables .,fact
SkNXYkbkz_63,"- It should be made clear that the Appendices are directly following the derivation of Evans 1997 , rather than being a new derivation .",request
SkNXYkbkz_64,"- At times the paper is little sloppy at making distinguishing between y ∈ [ 0 , 1 ] and y ∈ { 0 , 1 } .",evaluation
SkNXYkbkz_65,"This makes it a bit confusing at times what is output from the generator , particularly when you talk about it being one-hot encoded .",evaluation
SkNXYkbkz_66,"As I understand it , the output is always the former ,",fact
SkNXYkbkz_67,while data lives in the latter,fact
SkNXYkbkz_68,but this is not always clear .,evaluation
SkNXYkbkz_69,I think you should also make a bigger deal of this in terms of the problem with previous methods that ignore that the critic might be able to distinguish based on the fact that the training and generated data points have an explicitly different type ( discrete and continuous respectively ) .,request
SkNXYkbkz_70,"- On the fourth line of the abstract , both instances of GAN should be GANs .",request
SkNXYkbkz_71,- What do you mean “ explained in the sequel ” ?,request
SkNXYkbkz_72,- Does k need to be the same for each variable ?,request
SkNXYkbkz_73,<CIT>,reference
H1aEoGAgG_0,"The authors provide a novel , interesting , and simple algorithm capable of training with limited memory .",evaluation
H1aEoGAgG_1,"The algorithm is well-motivated and clearly explained ,",evaluation
H1aEoGAgG_2,and empirical evidence suggests that the algorithm works well .,evaluation
H1aEoGAgG_3,"However , the paper needs additional examination in how the algorithm can deal with larger data inputs and outputs .",request
H1aEoGAgG_4,"Second , the relationship to existing work needs to be explained better .",request
H1aEoGAgG_5,"Pro : The algorithm is clearly explained , well-motivated , and empirically supported .",evaluation
H1aEoGAgG_6,Con : The relationship to stochastic gradient markov chain monte carlo needs to be explained better .,request
H1aEoGAgG_7,"In particular , the update form was first introduced in [ 1 ] ,",fact
H1aEoGAgG_8,"the annealing scheme was analyzed in [ 2 ] ,",fact
H1aEoGAgG_9,and the reflection step was introduced in [ 3 ] .,fact
H1aEoGAgG_10,These relationships need to be explained clearly .,request
H1aEoGAgG_11,The evidence is presented on very small input data .,evaluation
H1aEoGAgG_12,"With something like natural images , the parameterization is much larger and with more data , the number of total parameters is much larger .",evaluation
H1aEoGAgG_13,Is there any evidence that the proposed algorithm could continue performing comparatively as the total number of parameters in state-of-the-art networks increases ?,request
H1aEoGAgG_14,This would require a smaller ratio of included parameters .,evaluation
H1aEoGAgG_15,[1] <CIT>,reference
H1aEoGAgG_16,[2] <CIT>,reference
H1aEoGAgG_17,[3] <CIT>,reference
SkF57lqez_0,The authors propose a new regularization term modifying the VAE ( Kingma et al 2013 ) objective to encourage learning disentangling representations .,fact
SkF57lqez_1,"Specifically , the authors suggest to add penalization to ELBO in the form of <VAR> , which encourages a more global criterion than the local ELBOs .",fact
SkF57lqez_2,"In practice , the authors decide that the objective they want to optimize is unwieldy",fact
SkF57lqez_3,and resort to moment matching of covariances of <VAR> and <VAR> via gradient descent .,fact
SkF57lqez_4,The final objective uses a persistent estimate of the covariance matrix of q and upgrades it at each mini-batch to perform learning .,fact
SkF57lqez_5,The authors use this objective function to perform experiments measuring disentanglement,fact
SkF57lqez_6,and find minor benefits compared to other objectives in quantitative terms .,evaluation
SkF57lqez_7,Comments : 1 . The originally proposed modification in Equation ( 4 ) appears to be rigorous,evaluation
SkF57lqez_8,and as far as I can tell still poses a lower bound to <VAR> .,fact
SkF57lqez_9,The proof could use the result posed earlier : <VAR> is smaller than <VAR> .,fact
SkF57lqez_10,2 . The proposed moment matching scheme performing decorrelation resembles approaches for variational PCA and especially independent component analysis .,evaluation
SkF57lqez_11,The relationship to these techniques is not discussed adequately .,evaluation
SkF57lqez_12,"In addition , this paper could really benefit from an empirical figure of the marginal statistics of z under the different regularizers in order to establish what type of structure is being imposed here and what it results in .",request
SkF57lqez_13,3 . The resulting regularizer with the decorrelation terms could be studied as a modeling choice .,request
SkF57lqez_14,"In the probabilistic sense , regularizers can be seen as structural and prior assumptions on variables .",evaluation
SkF57lqez_15,"As it stands , it is unnecessarily vague which assumptions this extra regularizer is making on variables .",evaluation
SkF57lqez_16,4 . Why is using the objective in Equation ( 4 ) not tried and tested and compared to ?,request
SkF57lqez_17,It could be thought that subsampling would be enough to evaluate this extra KL term without any need for additional variational parameters <VAR> .,evaluation
SkF57lqez_18,The reason for switching to the moment matching scheme seems not well motivated here without showing explicitly that Eq ( 4 ) has problems .,evaluation
SkF57lqez_19,"5 . The model seems to be making on minor progress in its stated goal , disentanglement .",evaluation
SkF57lqez_20,It would be more convincing to clarify the structural properties of this regularizer in a statistical sense more clearly given that experimentally it seems to only have a minor effect .,request
SkF57lqez_21,6 . Is there a relationship to NICE ( Laurent Dinh et al ) ?,non-arg
SkF57lqez_22,7 . The infogan is also an obvious point of reference and comparison here .,request
SkF57lqez_23,"8 . The authors claim that there are no models which can combine GANs with inference in a satisfactory way ,",fact
SkF57lqez_24,which is obviously not accurate nowadays given the progress on literature combining GANs and variational inference .,evaluation
SkF57lqez_25,All in all I find this paper interesting,evaluation
SkF57lqez_26,but would hope that a more careful technical justification and derivation of the model would be presented given that it seems to not be an empirically overwhelming change .,request
Hkns5PSlM_0,[ Summary ] The paper is overall well written,evaluation
Hkns5PSlM_1,and the literature review fairly up to date .,evaluation
Hkns5PSlM_2,The main issue is the lack of novelty .,evaluation
Hkns5PSlM_3,The proposed method is just a straightforward dimensionality reduction based on convolutional and max pooling layers .,evaluation
Hkns5PSlM_4,Using CNNs to handle variable length time series is hardly novel .,evaluation
Hkns5PSlM_5,"In addition , as always with metric learning , why learning the metric if you can just learn the classifier ?",request
Hkns5PSlM_6,"If the metric is not used in some compelling application , I am not convinced .",evaluation
Hkns5PSlM_7,"[ Detailed comments and suggestions ] * Since "" assumptions "" is the only subsection in Section 2 ,",fact
Hkns5PSlM_8,I would use \ texbf { Assumptions . } rather than \ subsection { Assumptions } .,request
Hkns5PSlM_9,"* Same remark for Section 4.1 "" Complexity analysis "" .",request
Hkns5PSlM_10,* Some missing relevant citations :,evaluation
Hkns5PSlM_11,<CIT>,reference
Hkns5PSlM_12,<CIT>,reference
Hkns5PSlM_13,<CIT>,reference
Hkns5PSlM_14,<CIT>,reference
B1FEuWcez_0,This paper describes an attempt of improving information flow in deep networks ( but is used and tested here with seq2seq models although it is reality unrelated to seq2seq models per se ) .,fact
B1FEuWcez_1,Slightly different from Resnet the information flow is improved by not just adding the outputs from previous layers but instead concatenating the outputs from previous layers with the current outputs .,evaluation
B1FEuWcez_2,The authors claim better convergence speed and better results for a similar number of parameters although the differences seems to be in the noise .,fact
B1FEuWcez_3,Overall this is an OK technique but in my opinion not really novel enough to justify a whole paper about it,evaluation
B1FEuWcez_4,as it seems more like a relatively minor architecture tweak .,evaluation
B1FEuWcez_5,The results seem to indicate that there were some problems with getting deeper networks to work for the baseline ( why is in Table 3 baseline-6L worse than baseline-4L ? ) for which the reason could be a multitude of issues probably related to hyper-parameter tuning .,fact
B1FEuWcez_6,"What is also missing is a an analysis of the negative consequences of this technique -- for example , does n't the number of parameters increase with the depth of the network because of the concatenation ?",request
B1FEuWcez_7,"Also , it would have been good to see more experiments with smaller baseline networks as well to match the smaller DenseNet networks in Table 1 and 2 .",request
B1FEuWcez_8,"Finally , the writing of the paper could be improved a lot :",request
B1FEuWcez_9,"The basic idea is not well described ( however , many times repeated )",evaluation
B1FEuWcez_10,and the grammar is often wrong,evaluation
B1FEuWcez_11,and also there are some typos .,evaluation
S1MHyoFgf_0,Thank you for your contribution to ICLR .,non-arg
S1MHyoFgf_1,The paper covers a very interesting topic and presents some though-provoking ideas .,evaluation
S1MHyoFgf_2,"The paper introduces "" covariant compositional networks "" with the purpose of learning graph representations .",fact
S1MHyoFgf_3,An example application also covered in the experimental section is graph classification .,fact
S1MHyoFgf_4,"Given a finite set S , a compositional network is simply a partially ordered set P where each element of P is a subset of S and where P contains all sets of cardinality 1 and the set S itself .",fact
S1MHyoFgf_5,"Unfortunately , the presentation of the approach is extremely verbose and introduces old concepts ( e.g. , partially ordered set ) under new names .",evaluation
S1MHyoFgf_6,The basic idea ( which is not new ) of this work is that we need to impose some sort of hierarchical order on the nodes of the graph so as to learn hierarchical feature representations .,fact
S1MHyoFgf_7,"Moreover , the hierarchical order of the nodes should be invariant to valid permutations of the nodes , that is , two isomorphic graphs should have the same hierarchical order on their nodes and the same feature representations .",fact
S1MHyoFgf_8,Since this is the case for graph embedding methods that collect feature representations from their neighbors in the graph ( and where the feature aggregation functions are symmetric ),fact
S1MHyoFgf_9,"it makes sense that "" compositional networks "" generalize graph convolutional networks ( and the more general message passing neural networks framework ) .",evaluation
S1MHyoFgf_10,"The most challenging problem , however , namely the problem of finding a concrete and suitable permutation invariant hierarchical decomposition of the nodes plus some aggregation/pooling functions to compute the feature representations is not addressed in sufficient detail .",evaluation
S1MHyoFgf_11,The paper spends a lot of time on some theoretical definitions and ( trivial ) proofs,evaluation
S1MHyoFgf_12,but then fails to make the connection to an approach that works in practice .,fact
S1MHyoFgf_13,The description of the experiments and which compositional network is chosen and how it is chosen seems to be missing .,fact
S1MHyoFgf_14,"The only part hinting at the model that was actually used in the experiments is the second paragraph of the section ' Experimental Setup ' , consisting of one long sentence that is incomprehensible to me .",evaluation
S1MHyoFgf_15,"Instead of spending a lot of effort on the definitions and ( somewhat trivial ) propositions in the first half of the paper , the authors should spend much more time on detailing the experiments and the actual model that they used .",request
S1MHyoFgf_16,"In an effort to make the framework as general as possible , you ended up making the paper highly verbose and difficult to follow .",evaluation
S1MHyoFgf_17,Please address the following points or clarify in your rebuttal if I misunderstood something : - what precisely is the novel contribution of your work,request
S1MHyoFgf_18,"( it can not be "" compositional networks "" and the propositions concerning those",fact
S1MHyoFgf_19,because these are just old concepts under new names ) ?,evaluation
S1MHyoFgf_20,- explain precisely ( and/or more directly/less convoluted ) how your model used in the experiments looks like ;,request
S1MHyoFgf_21,why do you think it is better than the other methods ?,request
S1MHyoFgf_22,"- given that compositional network is a very general concept ( partially ordered set imposed on subsets of the graph vertices ) ,",fact
S1MHyoFgf_23,what is the principled set of steps one has to follow to arrive at such a compositional network tailored to a particular graph collection ?,request
S1MHyoFgf_24,is n't ( or should n't ) that be the contribution of this work ?,fact
S1MHyoFgf_25,Am I missing something ?,non-arg
S1MHyoFgf_26,"In general , you should write the paper much more to the point and leave out unnecessary math ( or move to an appendix ) .",request
S1MHyoFgf_27,The paper is currently highly inaccessible .,evaluation
BynNGX9eG_0,This paper collects a cloze-style fill-in-the-missing-word dataset constructed manually by English teachers to test English proficiency .,fact
BynNGX9eG_1,Experiments are given which are claimed to show that this dataset is difficult for machines relative to human performance .,fact
BynNGX9eG_2,The dataset seems interesting,evaluation
BynNGX9eG_3,but I find the empirical evaluations unconvincing .,evaluation
BynNGX9eG_4,The models used to evaluate machine difficulty are basic language models .,fact
BynNGX9eG_5,The problems are multiple choice with at most four choices per question .,fact
BynNGX9eG_6,This allows multiple choice reading comprehension architectures to be used .,fact
BynNGX9eG_7,"A window of words around the blank could be used as the "" question "" .",fact
BynNGX9eG_8,A simple reading comprehension baseline is to encode the question ( a window around the blank ) and use the question vector to compute an attention over the passage .,evaluation
BynNGX9eG_9,One can then compute a question-specific representation of the passage and score each candidate answer by the inner product of the question-specific sentence representation and the vector representation of the candidate answer .,fact
BynNGX9eG_10,"See "" A thorough examination of the CNN/Daily Mail reading comprehension task "" by Chen , Bolton and Manning .",reference
ByC55Gcxz_0,"Clearly presented paper , including a number of reasonable techniques to improve LSTM-LMs .",evaluation
ByC55Gcxz_1,"The proposed techniques are heuristic ,",fact
ByC55Gcxz_2,but are reasonable and appear to yield improvements in perplexity .,evaluation
ByC55Gcxz_3,Some specific comments follow .,non-arg
ByC55Gcxz_4,"re . "" ASGD "" for Averaged SGD : ASGD usually stands for Asynchronous SGD ,",evaluation
ByC55Gcxz_5,have the authors considered an alternative acronym ?,request
ByC55Gcxz_6,AvSGD ?,request
ByC55Gcxz_7,"re . Optimization criterion on page 2 , note that SGD is usually taken to minimizing expected loss , not just empirical loss ( Bottou thesis 1991 ) .",fact
ByC55Gcxz_8,Is there any theoretical analysis of convergence for Averaged SGD ?,request
ByC55Gcxz_9,"re . paragraph starting with "" To prevent such inefficient data usage , we randomly select the sequence length for the forward and backward pass in two steps "" :",quote
ByC55Gcxz_10,the explanation is a bit unclear .,evaluation
ByC55Gcxz_11,"What is the "" base sequence length "" exactly ?",request
ByC55Gcxz_12,"Also , re . the motivation above this paragraph , I 'm not sure what "" elements "" really refers to , though I can guess .",evaluation
ByC55Gcxz_13,"What is the number of training tokens of the datasets used , PTB and WT2 ?",request
ByC55Gcxz_14,"Can the authors provide more explanation for what "" neural cache models "" are , and how they relate to "" pointer models "" ?",request
ByC55Gcxz_15,"Why do the sections "" Pointer models "" , "" Ablation analysis "" , and "" AWD-QRNN "" come after the Experiments section ?",request
Bk15lpF1G_0,This paper studies the adjustment of dropout rates,fact
Bk15lpF1G_1,which is a useful tool to prevent the overfitting of deep neural networks .,evaluation
Bk15lpF1G_2,The authors derive a generalization error bound in terms of dropout rates .,fact
Bk15lpF1G_3,"Based on this , the authors propose a regularization framework to adaptively select dropout rates .",fact
Bk15lpF1G_4,Experimental results are also given to verify the theory .,fact
Bk15lpF1G_5,Major comments : ( 1 ) The Empirical Rademacher complexity is not defined .,fact
Bk15lpF1G_6,"For completeness , it would be better to define it at least in the appendix .",request
Bk15lpF1G_7,( 2 ) I can not follow the inequality ( 5 ) .,evaluation
Bk15lpF1G_8,"Especially , according to the main text , <VAR> is a vector-valued function .",fact
Bk15lpF1G_9,"Therefore , it is not clear to me the meaning of <VAR> in ( 5 ) .",evaluation
Bk15lpF1G_10,( 3 ) I can also not see clearly the third equality in ( 9 ) .,evaluation
Bk15lpF1G_11,Note that <VAR> is a vector-valued function .,fact
Bk15lpF1G_12,It is not clear to me how it is related to a summation over j there .,evaluation
Bk15lpF1G_13,( 4 ) There is a linear dependency on the number of classes in Theorem 3.1 .,fact
Bk15lpF1G_14,Is it possible to further improve this dependency ?,non-arg
Bk15lpF1G_15,"Minor comments : ( 1 ) Section 4 : <VAR> <VAR> <VAR> is not consistent with <VAR> , <VAR> , <VAR>",fact
Bk15lpF1G_16,"( 2 ) Abstract : there should be a space before "" Experiments "" .",request
Bk15lpF1G_17,"( 3 ) It would be better to give more details ( e.g. , page , section ) in citing a book in the proof of Theorem 3.1",request
Bk15lpF1G_18,Summary : The mathematical analysis in the present version is not rigorous .,evaluation
Bk15lpF1G_19,The authors should improve the mathematical analysis .,request
Byyu-H4-f_0,The authors present an end to end training of a CNN architecture that combines CT image signal processing and image analysis .,fact
Byyu-H4-f_1,This is an interesting paper .,evaluation
Byyu-H4-f_2,"Time will tell whether a disease specific signal processing will be the future of medical image analysis ,",evaluation
Byyu-H4-f_3,"but - to the best of my knowledge - this is one of the first attempts to do this in CT image analysis ,",fact
Byyu-H4-f_4,"a field that is of significance both to researchers dealing with image reconstruction ( denoising , etc. ) and image analysis ( lesion detection ) .",evaluation
Byyu-H4-f_5,"As such I would be positive about the topic of the paper and the overall innovation it promises both in image acquisition and image processing ,",evaluation
Byyu-H4-f_6,"although I would share the technical concerns pointed out by Reviewer2 ,",evaluation
Byyu-H4-f_7,and the authors would need good answers to them before this study would be ready to be presented .,evaluation
ByFZUzFlf_0,Active learning for deep learning is an interesting topic,evaluation
ByFZUzFlf_1,and there is few useful tool available in the literature .,evaluation
ByFZUzFlf_2,It is happy to see such paper in the field .,evaluation
ByFZUzFlf_3,This paper proposes a batch mode active learning algorithm for CNN as a core-set problem .,fact
ByFZUzFlf_4,"The authors provide an upper bound of the core-set loss , which is the gap between the training loss on the whole set and the core-set .",fact
ByFZUzFlf_5,"By minimizing this upper bound , the problem becomes a K-center problem which can be solved by using a greedy approximation method , 2-OPT .",fact
ByFZUzFlf_6,"The experiments are performed on image classification problem ( CIFAR , CALTECH , SVHN datasets ) , under either supervised setting or weakly-supervised setting .",fact
ByFZUzFlf_7,Results show that the proposed method outperforms the random sampling and uncertainty sampling by a large margin .,evaluation
ByFZUzFlf_8,"Moreover , the authors show that 2-OPT can save tractable amount of time in practice with a small accuracy drop .",evaluation
ByFZUzFlf_9,The proposed algorithm is new,evaluation
ByFZUzFlf_10,and writing is clear .,evaluation
ByFZUzFlf_11,"However , the paper is not flawless .",evaluation
ByFZUzFlf_12,"The proposed active learning framework is under ERM and cover-set , which are currently not supported by deep learning .",fact
ByFZUzFlf_13,"To validate such theoretical result , a non-deep-learning model should be adopted .",request
ByFZUzFlf_14,"The ERM for active learning has been investigated in the literature , such as "" Querying discriminative and representative samples for batch mode active learning "" in KDD 2013 , which also provided an upper bound loss of the batch mode active learning",fact
ByFZUzFlf_15,and seems applicable for the problem in this paper .,evaluation
ByFZUzFlf_16,Another interesting question is most of the competing algorithm is myoptic active learning algorithms .,evaluation
ByFZUzFlf_17,The comparison is not fair enough .,evaluation
ByFZUzFlf_18,The authors should provide more competing algorithms in batch mode active learning .,request
HyLL47JGf_0,"Overall strength : In this paper , the authors proposed target-aware memory networks to model sentiment interactions between target aspects and the context words with attentions .",fact
HyLL47JGf_1,This work has a well-established motivation :,evaluation
HyLL47JGf_2,traditional attention for target-dependent sentiment classification can not model the interaction between target term and context words when making predictions .,fact
HyLL47JGf_3,"To solve this problem , the authors proposed five formulations in the final prediction layer .",fact
HyLL47JGf_4,"The illustration about the problem is clear , as well as the explanation for the formulations .",evaluation
HyLL47JGf_5,"Major concerns : 1 . This work brings some modifications to the prediction layer ,",fact
HyLL47JGf_6,which is a bit trivial .,evaluation
HyLL47JGf_7,"Although the effect has been shown ,",fact
HyLL47JGf_8,"the model is too specific to a narrow area , and is not general to be applied in a broad sense .",evaluation
HyLL47JGf_9,"It could have more contribution if the authors model the interactions within the attention model itself , instead of a simple prediction layer , which is problem-dependent .",request
HyLL47JGf_10,2 . The experiments are insufficient to show the effectiveness .,evaluation
HyLL47JGf_11,It would be better to provide some statistics showing how the target-context interaction model outperforms the traditional ones in the special cases like the one shown in Table 4 .,request
HyLL47JGf_12,Only two examples are not convincing .,evaluation
HyLL47JGf_13,"3 . In section 3 , the authors claimed that ( 5 ) models the target and context independently .",fact
HyLL47JGf_14,"However , in section 4 , in ( 7 ) , the authors claimed the target vector v_t will affect the context shifting their representation to c ’ _ i.",fact
HyLL47JGf_15,This should also work for ( 5 ) .,fact
HyLL47JGf_16,"4 . There are too many typos in the paper , e.g. , \ alpha is replaced by a , etc .",evaluation
HyLL47JGf_17,"Other concerns : 1 . It seems that one needs to train at least three embedding matrices : A , C , D which represent input embeddings , output embeddings , and interactive embeddings , respectively .",fact
HyLL47JGf_18,I wonder if this brings redundant parameters that do not guarantee convergence .,evaluation
HyLL47JGf_19,Why not use one matrix instead ?,request
HyLL47JGf_20,Did the authors try experiments with less embedding matrices ?,request
HyLL47JGf_21,2 . There is another work that also considers the target-context interaction using interactive attention model .,fact
HyLL47JGf_22,Please refer to this paper “ Interactive Attention Networks for Aspect-Level Sentiment Classification ” .,request
HyLL47JGf_23,A comparison is needed .,request
HyLL47JGf_24,"3 . It is better to provide results in terms of accuracy for both datasets ,",request
HyLL47JGf_25,as previous methods usually use accuracy for comparison .,fact
HyLL47JGf_26,How ’s the score of the proposed model compared with the above paper as well as [ Tang et al. 2016 ] ?,request
B143HDlWM_0,The authors present a variant of the adversarial feature learning ( AFL ) approach by Edwards & Storkey .,fact
B143HDlWM_1,"AFL aims to find a data representation that allows to construct a predictive model for target variable Y ,",fact
B143HDlWM_2,and at the same time prevents to build a predictor for sensitive variable S.,fact
B143HDlWM_3,"The key idea is to solve a minimax problem where the log-likelihood of a model predicting Y is maximized , and the log-likelihood of an adversarial model predicting S is minimized .",fact
B143HDlWM_4,"The authors suggest the use of multiple adversarial models , which can be interpreted as using an ensemble model instead of a single model .",fact
B143HDlWM_5,The way the log-likelihoods of the multiple adversarial models are aggregated does not yield a probability distribution as stated in Eq . 2 .,fact
B143HDlWM_6,While there is no requirement to have a distribution here,fact
B143HDlWM_7,- a simple loss term is sufficient,evaluation
B143HDlWM_8,- the scale of this term differs compared to calibrated log-likelihoods coming from a single adversary .,fact
B143HDlWM_9,"Hence , lambda in Eq . 3 may need to be chosen differently depending on the adversarial model .",fact
B143HDlWM_10,"Without tuning lambda for each method , the empirical experiments seem unfair .",evaluation
B143HDlWM_11,"This may also explain why , for example , the baseline method with one adversary effectively fails for Opp-L .",fact
B143HDlWM_12,A better comparison would be to plot the performance of the predictor of S against the performance of Y for varying lambdas .,request
B143HDlWM_13,The area under this curve allows much better to compare the various methods .,evaluation
B143HDlWM_14,There are little theoretical contributions .,evaluation
B143HDlWM_15,"Basically , instead of a single adversarial model - e.g. , a single-layer NN or a multi-layer NN - the authors propose to train multiple adversarial models on different views of the data .",fact
B143HDlWM_16,An alternative interpretation is to use an ensemble learner where each learner is trained on a different ( overlapping ) feature set .,fact
B143HDlWM_17,"Though , there is no theoretical justification why ensemble learning is expected to better trade-off model capacity and robustness against an adversary .",fact
B143HDlWM_18,Tuning the architecture of the single multi-layer NN adversary might be as good ?,evaluation
B143HDlWM_19,"In short , in the current experiments , the trade-off of the predictive performance and the effectiveness of obtaining anonymized representations effectively differs between the compared methods .",fact
B143HDlWM_20,This renders the comparison unfair .,evaluation
B143HDlWM_21,"Given that there is also no theoretical argument why an ensemble approach is expected to perform better ,",fact
B143HDlWM_22,I recommend to reject the paper .,evaluation
S1sHPAWgz_0,Overall : I really enjoyed reading this paper and think the question is super important .,evaluation
S1sHPAWgz_1,I have some reservations about the execution of the experiments as well as some of the conclusions drawn .,evaluation
S1sHPAWgz_2,For this reason I am currently a weak reject,evaluation
S1sHPAWgz_3,( weak because I believe the question is very interesting ) .,evaluation
S1sHPAWgz_4,"However , I believe that many of my criticisms can be assuaged during the rebuttal period .",evaluation
S1sHPAWgz_5,"Paper Summary : For RL to play video games , it has to play many many many many times .",evaluation
S1sHPAWgz_6,"In fact , many more times than a human",evaluation
S1sHPAWgz_7,where prior knowledge lets us learn quite fast in new ( but related ) environments .,evaluation
S1sHPAWgz_8,"The authors study , using experiments , what aspects of human priors are the important parts .",fact
S1sHPAWgz_9,"The authors ’ Main Claim appears to be : “ While common wisdom might suggest that prior knowledge about game semantics such as ladders are to be climbed , jumping on spikes is dangerous or the agent must fetch the key before reaching the door are crucial to human performance , we find that instead more general and high-level priors such as the world is composed of objects , object like entities are used as subgoals for exploration , and things that look the same , act the same are more critical . ”",fact
S1sHPAWgz_10,"Overall , I find this interesting .",evaluation
S1sHPAWgz_11,"However , I am not completely convinced by some of the experimental demonstrations .",evaluation
S1sHPAWgz_12,Issue 0 : The experiments seem underpowered / not that well analyzed .,evaluation
S1sHPAWgz_13,There are only 30 participants per condition,evaluation
S1sHPAWgz_14,and so it ’s hard to tell whether the large differences in conditions are due to noise and what a stable ranking of conditions actually looks like .,evaluation
S1sHPAWgz_15,I would recommend that the authors triple the sample size and be more clear about reporting the outcomes in each of the conditions .,request
S1sHPAWgz_16,"It ’s not clear what the error bars in figure 1 represent ,",evaluation
S1sHPAWgz_17,are they standard deviations of the mean ?,request
S1sHPAWgz_18,Are they standard deviations of the data ?,request
S1sHPAWgz_19,Are they confidence intervals for the mean effect ?,request
S1sHPAWgz_20,Did you collect any extra data about participants ?,request
S1sHPAWgz_21,One potentially helpful example is asking how familiar participants are with platformer video games .,request
S1sHPAWgz_22,This would give at least some proxy to study the importance of priors about “ how video games are generally constructed ” rather than priors like “ objects are special ” .,evaluation
S1sHPAWgz_23,Issue 1 : What do you mean by “ objects ” ?,request
S1sHPAWgz_24,The authors interpret the fact that performance falls so much between conditions b and c to mean that human priors about “ objects are special ” are very important .,fact
S1sHPAWgz_25,"However , an alternative explanation is that people explore things which look “ different ” ( ie . Orange when everything else is black ) .",fact
S1sHPAWgz_26,The problem here comes from an unclear definition of what the authors mean by an “ object ”,evaluation
S1sHPAWgz_27,"so in revision I would like authors to clarify what precisely they mean by a prior about “ the world is composed of objects ” and how this particular experiment differentiates “ object ” from a more general prior about “ video games have clearly defined goals , there are 4 clearly defined boxes here , let me try touching them . ”",request
S1sHPAWgz_28,This is important,evaluation
S1sHPAWgz_29,because a clear definition will give us an idea for how to actually build this prior into AI systems .,evaluation
S1sHPAWgz_30,Issue 2 : Are the results here really about “ high level ” priors ?,request
S1sHPAWgz_31,There are two ways to interpret the authors ’ main claim :,fact
S1sHPAWgz_32,the strong version would maintain that semantic priors are n’t important at all .,evaluation
S1sHPAWgz_33,There is no real evidence here for the strong version of the claim .,evaluation
S1sHPAWgz_34,A real test would be to reverse some of the expected game semantics and see if people perform just as well as in the “ masked semantics ” condition .,evaluation
S1sHPAWgz_35,"For example , suppose we had exactly the same game and N different types of objects in various places of the game where N-1 of them caused death but 1 of them opened the door ( but it was n’t the object that looked like a key ) .",evaluation
S1sHPAWgz_36,My hypothesis would be that performance would fall drastically as semantic priors would quickly lead people in that direction .,evaluation
S1sHPAWgz_37,"Thus , we could consider a weaker version of the claim : semantic priors are important but even in the absence of explicit semantic cues ( note , this is different from having the wrong semantic cues as above ) people can do a good job on the game .",evaluation
S1sHPAWgz_38,"This is much more supported by the data ,",evaluation
S1sHPAWgz_39,but still I think very particular to this situation .,evaluation
S1sHPAWgz_40,"Imagine a slight twist on the game : There is a sword ( with a lock on it ) , a key , a slime and the door ( and maybe some spikes ) .",evaluation
S1sHPAWgz_41,"The player must do things in exactly this order : first the player must get the key , then they must touch the sword , then they must kill the slime , then they go to the door .",evaluation
S1sHPAWgz_42,Here without semantic priors I would hypothesize that human performance would fall quite far ( whereas with semantics people would be able to figure it out quite well ) .,evaluation
S1sHPAWgz_43,"Thus , I think the authors ’ claim needs to be qualified quite a bit .",request
S1sHPAWgz_44,"It ’s also important to take into account how much work general priors about video game playing ( games have goals , up jumps , there is basic physics ) are doing here",evaluation
S1sHPAWgz_45,( the authors do this when they discuss versions of the game with different physics ) .,fact
SkwCEXalM_0,"To speed up RL algorithms , the authors propose a simple method based on utilizing expert demonstrations .",fact
SkwCEXalM_1,The proposed method consists in explicitly learning a prediction function that maps each time-step into a state .,fact
SkwCEXalM_2,This function is learned from expert demonstrations .,fact
SkwCEXalM_3,The cost of visiting a state is then defined as the distance between that state and the predicted state according to the learned function .,fact
SkwCEXalM_4,This reward is then used in standard RL algorithms to learn to stick close to the expert 's demonstrations .,fact
SkwCEXalM_5,"An on-loop variante of this method consists of learning a function that maps each state into a next state according to the expert , instead of the off-loop function that maps time-steps into states .",fact
SkwCEXalM_6,"While the experiments clearly show the advantage of this method ,",evaluation
SkwCEXalM_7,this is hardly surprising or novel .,evaluation
SkwCEXalM_8,The concept of encoding the demonstration explicitly in the form of a reward has been around for over a decade .,fact
SkwCEXalM_9,This is the most basic form of teaching by demonstration .,evaluation
SkwCEXalM_10,"Previous works had used other models for generalizing demonstrations ( GMMs , GPs , Kernel methods , neural nets etc. . ) .",fact
SkwCEXalM_11,"This paper uses a three layered fully connected auto-encoder ( which is not that deep of a model , btw ) for the same purpose .",fact
SkwCEXalM_12,The idea of using this model as a reward instead of directly cloning the demonstrations is pretty straightforward .,evaluation
SkwCEXalM_13,Other comments : - Most IRL methods would work just fine by defining rewards on states only and ignoring actions all together .,evaluation
SkwCEXalM_14,"If you know the transition function , you can choose actions that lead to highly rewarding states ,",evaluation
SkwCEXalM_15,so you do n't need to know the expert 's executed actions .,fact
SkwCEXalM_16,"- "" We assume that maximizing likelihood of next step prediction in equation 1 will be globally optimized in RL "" .",quote
SkwCEXalM_17,Could you elaborate more on this assumption ?,request
SkwCEXalM_18,"Your model finds rewards based on local state features , where a greedy ( one-step planning ) policy would reproduce the expert 's demonstrations ( if the system is deterministic ) .",fact
SkwCEXalM_19,It does not compare the global performance of the expert to alternative policies ( as is typically done in IRL ) .,fact
SkwCEXalM_20,- Related to the previous point : a reward function that makes every step of the expert optimal may not be always exist .,fact
SkwCEXalM_21,The expert may choose to go to terrible states with the hope of getting to a highly rewarding state in the future .,fact
SkwCEXalM_22,"Therefore , the objective functions set in this paper may not be the right ones , unless your state description contains features related to future states so that you can incorporate future rewards in the current state ( like in the reacher task , where a single image contains all the information about the problem ) .",evaluation
SkwCEXalM_23,What you need is actually features that can capture the value function ( like in DQN ) and not just the immediate reward ( as is done in IRL methods ) .,fact
SkwCEXalM_24,"- What if in two different trajectories , the expert chooses opposite actions for the same state appearing in both trajectories ?",request
SkwCEXalM_25,"For example , there are two shortest paths to a goal , one starts with going left and another starts with going right .",fact
SkwCEXalM_26,"If you try to generate a state that minimizes the sum of distances to the two states ( left and right ones ) , then you may choose to remain in the middle , which is suboptimal .",fact
SkwCEXalM_27,"You would n't have this issue with regular IRL techniques ,",fact
SkwCEXalM_28,because you can explain both behaviors with future rewards instead of trying to explain every action of the expert using only local state description .,fact
By-4qhFeM_0,This paper proposes another entropic regularization term for deep neural nets .,fact
By-4qhFeM_1,"The key idea can be stated as follows : Let X denote the observed input , C the hidden class label taking values in a finite set , and Y the representation computed by a neural net .",fact
By-4qhFeM_2,Then <EQN> is a Markov chain .,fact
By-4qhFeM_3,"Moreover , assuming that the mapping <EQN> is deterministic ( as is the case with neural nets or any other deterministic representations ) , we can write down the mutual information between X and Y as",fact
By-4qhFeM_4,<EQN> .,fact
By-4qhFeM_5,A simple manipulation shows that <EQN> .,fact
By-4qhFeM_6,"The authors interpret the first term , <VAR> , as a data fit term that quantifies the statistical correlations between the class label C and the representation Y ,",fact
By-4qhFeM_7,"whereas the second term , <VAR> , is the amount by which the representation Y can be compressed knowing the class label C.",fact
By-4qhFeM_8,The authors then propose to ' explicitly decouple ' the data-fit term <VAR> from the regularization penalty and focus on minimizing <VAR> .,fact
By-4qhFeM_9,"In fact , they replace this term by the sum of conditional entropies of the form <VAR> , where <VAR> is the activation of the ith neuron in the kth layer of the neural net .",fact
By-4qhFeM_10,"The final step is to recognize that the conditional entropy may not admit a scalable and differentiable estimator ,",fact
By-4qhFeM_11,so they use the relation between a quantity called entropy power and second moments to replace the entropic penalty with the conditional variance penalty <VAR> .,fact
By-4qhFeM_12,"Since the class-conditional distributions are unknown ,",fact
By-4qhFeM_13,a surrogate model <VAR> is used .,fact
By-4qhFeM_14,The authors present some experimental results as well .,fact
By-4qhFeM_15,"However , this approach has a number of serious flaws .",evaluation
By-4qhFeM_16,"First of all , if the distribution of X is nonatomic and the mapping <EQN> Y is continuous ( in the case of neural nets , it is even Lipschitz ) , then the mutual information <VAR> is infinite .",fact
By-4qhFeM_17,"In that case , the representation of <VAR> in terms of entropies is not valid",fact
By-4qhFeM_18,"-- indeed , one can write the mutual information between two jointly distributed random variables X and Y in terms of differential entropies as <EQN> ,",fact
By-4qhFeM_19,but this is possible only if both terms on the right-hand side exist .,fact
By-4qhFeM_20,"This is not the case here ,",fact
By-4qhFeM_21,"so , in particular , one can not relate <VAR> to <VAR> . <VAR>",evaluation
By-4qhFeM_22,"Ironically , is finite ,",fact
By-4qhFeM_23,"because C takes values in a finite set ,",fact
By-4qhFeM_24,so <VAR> is at most the log cardinality of the set of labels .,fact
By-4qhFeM_25,"One can start , then , simply with <VAR> and express it as <VAR> .",fact
By-4qhFeM_26,"Both terms are well-defined Shannon entropies , where the first one does not depend on the representation ,",fact
By-4qhFeM_27,whereas the second one involves the representation .,fact
By-4qhFeM_28,"But then , if the goal is to _ minimize _ the mutual information between <VAR> , it makes sense to _ maximize _ the conditional entropy <VAR> .",evaluation
By-4qhFeM_29,"In short , the line of reasoning that leads to minimizing <VAR> is not convincing .",evaluation
By-4qhFeM_30,"Moreover , why is it a good idea to _ minimize _ <VAR> in the first place ?",evaluation
By-4qhFeM_31,"Should n't one aim to maximize it subject to structural constraints on the representation , along the lines of InfoMax ?",evaluation
By-4qhFeM_32,The next issue is the chain of reasoning that leads to replacing <VAR> with <VAR> .,evaluation
By-4qhFeM_33,"One could start with that instead without changing the essence of the approach , but then the magic words "" Shannon decay "" would have to disappear altogether , and the proposed method would lose all of its appeal .",evaluation
S1skfxRxM_0,SUMMARY The paper considers the problem of using cycle GANs to decipher text encrypted with historical ciphers .,fact
S1skfxRxM_1,Also it presents some theory to address the problem that discriminating between the discrete data and continuous prediction is too simple .,fact
S1skfxRxM_2,The model proposed is a variant of the cycle GAN in which in addition embeddings helping the Generator are learned for all the values of the discrete variables .,fact
S1skfxRxM_3,The log loss of the GAN is replaced by a quadratic loss and a regularization of the Jacobian of the discriminator .,fact
S1skfxRxM_4,Experiments show that the method is very effective .,evaluation
S1skfxRxM_5,REVIEW The paper considers an interesting and fairly original problem,evaluation
S1skfxRxM_6,and the overall discussion of ciphers is quite nice .,evaluation
S1skfxRxM_7,"Unfortunately , my understanding is that the theory proposed in section 2 does not correspond to the scheme used in the experiments",evaluation
S1skfxRxM_8,"( contrarily to what the conclusion suggest and contrarily to what the discussion of the end of section 3 , which says that using embedding is assumed to have an equivalent effect to using the methodology considered in the theoretical part ) .",fact
S1skfxRxM_9,Another important concern is with the proof : there seems to be an unmotivated additional assumption that appears in the middle of the proof of Proposition 1,evaluation
S1skfxRxM_10,+ some steps need to be clarified ( see comment 16 below ) .,request
S1skfxRxM_11,"The experiments do not have any simple baseline , which is somewhat unfortunate .",evaluation
S1skfxRxM_12,DETAILED COMMENTS : 1 - The paper makes a few bold and debatable statements :,evaluation
S1skfxRxM_13,"line 9 of section 1 "" Such hand-crafted features have fallen out of favor ( Goodfellow et al. , 2016 ) as a result of their demonstrated inferiority to features learned directly from data in end-to-end learning frameworks such as neural networks """,quote
S1skfxRxM_14,This is certainly an overstatement,evaluation
S1skfxRxM_15,"and although it might be true for specific types of inputs it is not universally true ,",evaluation
S1skfxRxM_16,most deep architectures rely on a human-in-the-loop,fact
S1skfxRxM_17,"and there are number of areas where human crafted feature are arguably still relevant , if only to specify what is the input of a deep network :",evaluation
S1skfxRxM_18,"there are many domains where the notion of raw data does not make sense , and , when it does , it is usually associated with a sensing device that has been designed by a human and which implicitly imposes what the data is based on human expertise .",evaluation
S1skfxRxM_19,"2 - In the last paragraph of the introduction , the paper says that previous work has only worked on vocabularies of 26 characters while the current paper tackles word level ciphers with 200 words .",fact
S1skfxRxM_20,"But , is n't this just a matter of scalability and only possible with very large amounts of text ?",fact
S1skfxRxM_21,Is it really because of an intrinsic limitation or lack of scalability of previous approaches or just because the authors of the corresponding papers did not care to present larger scale experiments ?,evaluation
S1skfxRxM_22,3 - The discussion at the top of page 5 is difficult to follow .,evaluation
S1skfxRxM_23,"What do you mean when you say "" this motivates the benefits of having strong curvature globally , as opposed to linearly between etc """,request
S1skfxRxM_24,Which curvature are we talking about ?,request
S1skfxRxM_25,"and what how does the "" as opposed to linearly "" mean ?",request
S1skfxRxM_26,"Should we understand "" as opposed to having curvature linearly interpolated between etc "" or "" as opposed to having a linear function "" ?",request
S1skfxRxM_27,Please clarify .,request
S1skfxRxM_28,"4 - In the same paragraph : what does "" a region that has not seen the Jacobian norm applied to it "" mean ?",request
S1skfxRxM_29,How is a norm applied to a region ?,request
S1skfxRxM_30,I guess that what you mean is that the generator G might creates samples in a part of the space where the function F has not yet been learned and is essentially close to 0 .,evaluation
S1skfxRxM_31,Is this what you mean ?,request
S1skfxRxM_32,5 - I do not understand why the paper introduces WGAN,evaluation
S1skfxRxM_33,"since in the end it does not use them but uses a quadratic loss , introduced in the first display of section 4.3 .",fact
S1skfxRxM_34,6 - The paper makes a theoretical contribution which supports replacing the sample y by a sample drawn from a region around y.,fact
S1skfxRxM_35,But it seems that this is not used in the experiment,fact
S1skfxRxM_36,and that the authors consider that the introduction of the embedding is a substitution for this .,fact
S1skfxRxM_37,"Indeed , in the last paragraph of section 3.1 , the paper says "" we make the assumption that the training of the embedding vectors approximates random sampling similar to what is described in Proposition 1 "" .",quote
S1skfxRxM_38,This does not make any sense to me,evaluation
S1skfxRxM_39,"because the embedding vectors map each y deterministically to a single point ,",fact
S1skfxRxM_40,and so the distribution on the corresponding vectors is still a fixed discrete distribution .,fact
S1skfxRxM_41,This gives me this impression that the proposed theory does not match what is used in the experiments .,evaluation
S1skfxRxM_42,"( The last sentence of section 3.1 , which is commenting on this and could perhaps clarify the situation is ill formed with two verbs . )",evaluation
S1skfxRxM_43,"7 - In the definitions : "" A discriminator is said to perform uninformative discrimination "" etc .",quote
S1skfxRxM_44,- > It seems that the choice of the word uninformative would be misleading :,evaluation
S1skfxRxM_45,"an uninformative discrimination would be a discrimination that completely fails , while what the condition is saying it that it can not perform perfect discrimination .",fact
S1skfxRxM_46,"I would thus suggest to call this "" imperfect discrimination "" .",request
S1skfxRxM_47,8 - It seems that the same embedding is used in X space and in Y space ( from equations 6 and 7 ) .,fact
S1skfxRxM_48,Is there any reason for that ?,request
S1skfxRxM_49,I would seem more natural to me to introduce two different embeddings,evaluation
S1skfxRxM_50,since the objects are a priori different ...,fact
S1skfxRxM_51,Actually I do n't understand how the embeddings can be the same in the Vignere code case,evaluation
S1skfxRxM_52,since time taken into account one one side .,fact
S1skfxRxM_53,"9 - On the 5th line after equation ( 7 ) , the paper says "" the embeddings ... are trained to minimize L_GAN and L_cyc , meaning ... and are easy to discriminate """,quote
S1skfxRxM_54,- > This last part of the sentence seems wrong to me .,evaluation
S1skfxRxM_55,The discriminator is trying to maximize L_GAN,fact
S1skfxRxM_56,and so minimizing w.r.t. to the embedding is precisely trying to prevent to the discriminator to tell apart too easily the true elements from the estimated ones .,fact
S1skfxRxM_57,In fact the regularization of the Jacobian that will be preventing the discriminator to vary too quickly in space is more likely to explain the fact that the discrimination is not too easy to do between the true and mapped embeddings .,fact
S1skfxRxM_58,This might be connected to the discussion at the top of page 5 .,evaluation
S1skfxRxM_59,"Since there are no experiments with alpha different than the default value = 10 ,",fact
S1skfxRxM_60,this is difficult to assess .,evaluation
S1skfxRxM_61,10-The Vigenere cipher is explained again at the end of section 4.2 when it has already been presented in section 1.1,fact
S1skfxRxM_62,"11 - Concerning results in Table 2 : I do not see why it would not be possible to compare the performance of the method with classical frequency analysis , at least for the character case .",request
S1skfxRxM_63,"12 - At the beginning of section 4.3 , the text says that the log loss was replaced with the quadratic loss , but without giving any reason .",fact
S1skfxRxM_64,Could you explain why .,request
S1skfxRxM_65,"13 - The only comparison of results with and without embeddings is presented in the curves of figure 3 , for Brown-W with a vocabulary of 200 words .",fact
S1skfxRxM_66,In that case it helps .,evaluation
S1skfxRxM_67,Could the authors report systematically results about all cases ?,request
S1skfxRxM_68,( I guess this might however be the only hard case ... ),evaluation
S1skfxRxM_69,14 - It would be useful to have a brief reminder of the architecture of the neural network,request
S1skfxRxM_70,"( right now the reader is just refered to Zhu et al. , 2017 ) :",fact
S1skfxRxM_71,"how many layers , how many convolution layers etc .",request
S1skfxRxM_72,The same comment applies for the way the position of the letter/word in the text appear is in encoded in a feature that is provided as input to the neural network : it would be nice if the paper could provide a few details here and be more self contained .,request
S1skfxRxM_73,"( The fact that the engineering of the time feature can "" dramatically "" improve the performance of the network should be an argument to convince the authors that hand-crafted feature have not fallen out of favor completely yet ... )",evaluation
S1skfxRxM_74,"15 - I disagree with the statement made in the conclusion that the proposed work "" empirically confirms [ ... ] that the use of continuous relaxation of discrete variable facilitates [ ... ] and prevents [ ... ] """,evaluation
S1skfxRxM_75,"because for me the proposed implementation does not use at all the theoretical idea of continuous relaxation proposed in the paper , unless there is a major point that I am missing .",evaluation
S1skfxRxM_76,16 - I have two issues with the proof in the appendix,evaluation
S1skfxRxM_77,"a ) after the first display of the last page the paper makes an additional assumption which is not announced in the statement of the theorem , which is that two specific inequality hold ...",fact
S1skfxRxM_78,Unless I am mistaken this assumption is never proven ( later or earlier ) .,fact
S1skfxRxM_79,"Given that this inequality is just "" the right inequality to get the proof go through """,evaluation
S1skfxRxM_80,"and given that there are no explanation for why this assumption is reasonable , to me this invalidates the proof .",evaluation
S1skfxRxM_81,The step of going from <VAR> to <VAR> seems delicate ...,evaluation
S1skfxRxM_82,"b ) If we accept these inequalities , the determinant of the Jacobian ( the notation is not defined ) of F at ( x_bar ) disappears from the equations , as if it could be assumed to be greater than one .",fact
S1skfxRxM_83,"If this is indeed the case , please provide a justification of this step .",request
S1skfxRxM_84,"17 - A way to address the issue of trivial discrimination in GANs with discrete data has been proposed in Luc , P. , Couprie , C. , Chintala , S. , & Verbeek , J. ( 2016 ) . Semantic segmentation using adversarial networks . arXiv preprint arXiv : 1611.08408 .",fact
S1skfxRxM_85,The authors should probably reference this paper .,request
S1skfxRxM_86,"18 - Clarification of the Jacobian regularization : in equation ( 3 ) , the Jacobian computed seems to be w.r.t D composed with F",fact
S1skfxRxM_87,while in equation ( 8 ) it is only the Jacobian of D.,fact
S1skfxRxM_88,Which equation is the correct one ?,request
S1skfxRxM_89,TYPOS : Proposition 1 : the if-then statement is broken into two sentences separated by a full point and a carriage return .,fact
S1skfxRxM_90,sec. 4.3 line 10 we use a cycle loss * with a regularization coefficient * <EQN> ( a piece of the sentence is missing ),fact
S1skfxRxM_91,"sec. 4.3 lines 12-13 the learning rates given are the same at startup and after "" warming up "" ...",fact
S1skfxRxM_92,"In the appendix : 3rd line of proof of prop 1 : I don ' understand "" countably infinite finite sequences of vectors lying in the vertices of the simplex """,evaluation
S1skfxRxM_93,- > what is countable infinite here ?,request
S1skfxRxM_94,The vertices ?,request
BJovaI9gf_0,"This is an interesting paper that builds a parameterized network to select actions for a robot in a simulated environment , with the objective of quickly reaching an internal belief state that is predictive of the true state .",fact
BJovaI9gf_1,This is an interesting idea,evaluation
BJovaI9gf_2,and it works much better than I would have expected .,evaluation
BJovaI9gf_3,"In more careful examination it is clear that the authors have done a good job of designing a network that is partly pre-specified and partly free , in a way that makes the learning effective .",evaluation
BJovaI9gf_4,In particular - the transition model is known and fixed ( in the way it is used in the belief update process ),fact
BJovaI9gf_5,- the belief state representation is known and fixed ( in the way it is used to decide whether the agent should be rewarded ),fact
BJovaI9gf_6,- the reward function is known and fixed ( as above ),fact
BJovaI9gf_7,- the mechanics of belief update,fact
BJovaI9gf_8,But we learn - the observation model,fact
BJovaI9gf_9,- the control policy,fact
BJovaI9gf_10,I 'm not sure that global localization is still an open problem with known models .,fact
BJovaI9gf_11,"Or , at least , it 's not one of our worst .",fact
BJovaI9gf_12,"Early work by Cassandra , Kurien , et al used POMDP models and solvers for active localization with known transition and observation models .",fact
BJovaI9gf_13,It was computationally slow but effective .,fact
BJovaI9gf_14,"Similarly , although the online speed of your learned method is much better than for active Markov localization , the offline training cost is dramatically higher ;",evaluation
BJovaI9gf_15,it 's important to remember to be clear on this point .,evaluation
BJovaI9gf_16,It is not obvious to me that it is sensible to take the cosine similarity between the feature representation of the observation and the feature representation of the state to get the entry in the likelihood map .,evaluation
BJovaI9gf_17,It would be good to make it clear this is the right measure .,request
BJovaI9gf_18,How is exploration done during the RL phase ?,request
BJovaI9gf_19,These domains are still not huge .,evaluation
BJovaI9gf_20,Please explain in more detail what the memory images are doing .,request
BJovaI9gf_21,"In general , the experiments seem to be well designed and well carried out , with several interesting extensions .",evaluation
BJovaI9gf_22,I have one more major concern : it is not the job of a localizer to arrive at a belief state with high probability mass on the true state,fact
BJovaI9gf_23,--- it is the job of a localizer to have an accurate approximation of the true posterior under the prior and observations .,fact
BJovaI9gf_24,"There are situations ( in which , for example , the robot has gotten an unusual string of observations ) in which it is correct for the robot to have more probability mass on a "" wrong "" state .",fact
BJovaI9gf_25,"Or , it seems that this model may earn rewards for learning to make its beliefs overconfident .",fact
BJovaI9gf_26,It would be very interesting to see if you could find an objective that would actually cause the model to learn to compute the appropriate posterior .,request
BJovaI9gf_27,"In the end , I have trouble making a recommendation :",evaluation
BJovaI9gf_28,Con : I 'm not convinced that an end-to-end approach to this problem is the best one,evaluation
BJovaI9gf_29,Pro : It 's actually a nice idea that seems to have worked out well,evaluation
BJovaI9gf_30,Con : I remain concerned that the objective is not the right one,evaluation
BJovaI9gf_31,My rating would really be something like 6.5 if that were possible .,non-arg
SkFdaJtgf_0,The authors present an estimator for the mutual information ( MI ) based on the Donsker-Varadhan representation for the KL divergence and its generalization to arbitrary f-divergences by Ruderman et al .,fact
SkFdaJtgf_1,"While that last work introduced an estimator based on optimization over the unit ball in an RKHS ,",fact
SkFdaJtgf_2,the current work propose to use a parametric function class given by a neural network,fact
SkFdaJtgf_3,"( I 'd suggest that the authors make this point more explicit ,",request
SkFdaJtgf_4,as currently it 's not totally clear what their actual contribution is and how their work compares to the prior art they cite ) .,evaluation
SkFdaJtgf_5,The authors show that such an estimator can be used to train models with less mode-dropping in adversarial models .,fact
SkFdaJtgf_6,"The work is quite straightforward ,",evaluation
SkFdaJtgf_7,but improves over similar work in the GAN space by Nowozin et al. by using Ruderman 's tighter variational representation instead of Nguyen 's one .,fact
SkFdaJtgf_8,The paper contains many typos and grammatical errors,fact
SkFdaJtgf_9,and the authors should do an exhaustive proof-reading .,request
SkFdaJtgf_10,"More problematic is that , right after eq . 10 , the authors mention "" We show in the Appendix that OMIE has the desirable strong consistency and convergence properties "" .",evaluation
SkFdaJtgf_11,"However , the appendix does n't contain such a proof .",fact
SkFdaJtgf_12,Is it missing from the submitted version ?,non-arg
SkFdaJtgf_13,"I do n't think that such a consistency proof is strictly necessary for a paper like this ,",evaluation
SkFdaJtgf_14,but for the review to be accurate I need to see the proof .,evaluation
SkFdaJtgf_15,"Since I ca n't find it ,",evaluation
SkFdaJtgf_16,I assume it does not exist .,evaluation
SkFdaJtgf_17,"In that case , the authors should give less emphasis to the MI estimator itself and more to the empirical properties and applications .",request
SkFdaJtgf_18,The authors present some experiments comparing different estimators of MI applied to synthetic data .,fact
SkFdaJtgf_19,"Figure 1 is hard to read ,",evaluation
SkFdaJtgf_20,I suggest the authors try to come up with a more legible plot .,request
SkFdaJtgf_21,"Figure 2 is also a bit surprising ,",evaluation
SkFdaJtgf_22,why show error for 50 dimensions but estimates for 2 dimensions ?,request
SkFdaJtgf_23,"Since these experiments are quick to run ,",evaluation
SkFdaJtgf_24,it would be helpful to get more information on how the gap between the methods change as the dimensionality increases ( e.g. a surface plot with d and # of iterations on the x and y axes ) .,request
SkFdaJtgf_25,"Also it would be highly beneficial to compare with the method in Ruderman at al. ,",request
SkFdaJtgf_26,so that people interested in MI estimation but who do n't plan on using the estimator as part of a neural net architecture can get some idea on how the inductive bias of NNs compare to RKHS .,evaluation
SkFdaJtgf_27,"In the caption to Fig. 3 the authors state "" The OMIEGAN generator learns a distribution with a high amount of structured noise "" , which I find hard to understand .",evaluation
SkFdaJtgf_28,"Probably the authors can be a bit more precise than saying "" structured noise "" .",request
SkFdaJtgf_29,I would recommend dropping the Information Bottleneck section to focus on showing more convincingly the impact of OMIE in GANs .,request
SkFdaJtgf_30,The experiments section currently looks rushed and lacking in depth .,evaluation
SkFdaJtgf_31,"In summary , this work provides value by introducing a ( previously known ) superior f-divergence variational representation to the GAN community .",fact
SkFdaJtgf_32,The mode-collapse prevention via MI maximisation is also interesting and deserves more experimental attention to make the paper stronger .,evaluation
r14eWGtez_0,"The authors propose a technique to compress LSTMs in RNNs by using a group Lasso regularizer which results in structured sparsity , by eliminating individual hidden layer inputs at a particular layer .",fact
r14eWGtez_1,The authors present experiments on unidirectional and bidirectional LSTM models which demonstrate the effectiveness of this method .,fact
r14eWGtez_2,"The proposed techniques are evaluated on two models : a fairly large LSTM with ~ 66.0 M parameters , as well as a more compact LSTM with ~ 2.7 M parameters , which can be sped up significantly through compression .",fact
r14eWGtez_3,"Overall this is a clearly written paper that is easy to follow , with experiments that are well motivated .",evaluation
r14eWGtez_4,"To the best of my knowledge most previous papers in the area of RNN compression focus on pruning or compression of the node outputs/connections , but do not focus as much on reducing the computation/parameters within an RNN cell .",evaluation
r14eWGtez_5,I only have a few minor comments/suggestions which are listed below :,evaluation
r14eWGtez_6,"1 . It is interesting that the model structure where the number of parameters is reduced to the number of ISSs chosen from the proposed procedure does not attain the same performance as when training with a larger number of nodes , with the group lasso regularizer .",evaluation
r14eWGtez_7,"It would be interesting to conduct experiments for a range of \ lambda values : i.e. , to allow for different degrees of compression , and then examine whether the model trained from scratch with the “ optimal ” structure achieves performance closer to the ISS-based strategy , for example , for smaller amounts of compression , this might be the case ?",request
r14eWGtez_8,"2 . In the experiment , the authors use a weaker dropout when training with ISS .",fact
r14eWGtez_9,Could the authors also report performance for the baseline model if trained with the same dropout ( but without the group LASSO regularizer ) ?,request
r14eWGtez_10,3 . The colors in the figures : especially the blue vs. green contrast is really hard to see .,evaluation
r14eWGtez_11,"It might be nicer to use lighter colors , which are more distinct .",request
r14eWGtez_12,4 . The authors mention that the thresholding operation to zero-out weights based on the hyperparameter \ tau is applied “ after each iteration ” .,fact
r14eWGtez_13,What is an iteration in this context ?,request
r14eWGtez_14,"An epoch , a few mini-batch updates , per mini-batch ?",request
r14eWGtez_15,Could the authors please clarify .,request
r14eWGtez_16,5 . Clarification about the hyperparameter \ tau used for sparsification : Is \ tau determined purely based on the converged weight values in the model when trained without the group LASSO constraint ?,request
r14eWGtez_17,"It would be interesting to plot a histogram of weight values in the baseline model , and perhaps also after the group LASSO regularized training .",request
r14eWGtez_18,6 . Is the same value of \ lambda used for all groups in the model ?,request
r14eWGtez_19,"It would be interesting to consider the effect of using stronger sparsification in the earlier layers , for example .",request
r14eWGtez_20,"7 . Section 4.2 : Please explain what the exact match ( EM ) and F1 metrics used to measure performance of the BIDAF model are , in the text .",request
r14eWGtez_21,Minor Typographical/Grammatical errors : - Sec 1 : “ ... in LSTMs meanwhile maintains the dimension consistency . ” → “ ... in LSTMs while maintaining the dimension consistency . ”,request
r14eWGtez_22,- Sec 1 : “ ... is public available ” → “ is publically available ”,request
r14eWGtez_23,"- Sec 2 : Please rephrase : “ After learning those structures , compact LSTM units remain original structural schematic but have the sizes reduced . ”",request
r14eWGtez_24,- Sec 4.1 : “ The exactly same training scheme of the baseline ... ” → “ The same training scheme as the baseline ... ”,request
Hk1CjNZyM_0,This paper investigates the effect of replacing identity skip connections with trainable convolutional skip connections in ResNet .,fact
Hk1CjNZyM_1,"The authors find that in their experiments , performance improves .",fact
Hk1CjNZyM_2,"Therefore , the power of skip connections is due to their linearity rather than due to the fact that they represent the identity .",fact
Hk1CjNZyM_3,"Overall , the paper has a clear and simple message and is very readable .",evaluation
Hk1CjNZyM_4,"The paper contains a good amount of experiments ,",evaluation
Hk1CjNZyM_5,but in my opinion not quite enough to conclude that identity skip connections are inherently worse .,evaluation
Hk1CjNZyM_6,The question is then : how non-trivial is it that tandem networks work ?,evaluation
Hk1CjNZyM_7,"For someone who understands and has worked with ResNet and similar architectures , this is not a surprise .",evaluation
Hk1CjNZyM_8,"Therefore , the paper is somewhat marginal but , I think , still worth accepting .",evaluation
Hk1CjNZyM_9,Why did you choose a single learning rate for all architectures and datasets instead of choosing the optimal one for each archtitecture and dataset ?,non-arg
Hk1CjNZyM_10,Was it a question of computational resources ?,non-arg
Hk1CjNZyM_11,Using custom step sizes would strenghten your experimental results significantly .,request
Hk1CjNZyM_12,"In the absence of this , I would still ask that you create an appendix where you specify exactly how hyperparameters were chosen .",request
Hk1CjNZyM_13,"Other comments : - "" and that it ’s easier for a layer to learn from a starting point of keeping things the same ( the identity map ) than from the zero map "" I do n't understand this comment .",evaluation
Hk1CjNZyM_14,"Networks without skip connections are not initialized to the zero map but have nonzero , usually Gaussian , weights .",fact
Hk1CjNZyM_15,"- in section 2 , reason ( ii ) , you seem to imply that it is a good thing if a network behaves as an ensemble of shallower networks .",fact
Hk1CjNZyM_16,"In general , this is a bad thing .",evaluation
Hk1CjNZyM_17,"Therefore , the fact that ResNet with tandom networks is an ensemble of shallower networks is a reason for why it might perform badly , not well .",fact
Hk1CjNZyM_18,I would suggest removing reason ( ii ) .,request
Hk1CjNZyM_19,"- in section 3 , reason ( iii ) , you state that removing nonlinearities from the skip path can improve performance .",fact
Hk1CjNZyM_20,"However , using tandom blocks instead of identity skip connections does not change the number of nonlinearity layers .",fact
Hk1CjNZyM_21,"Therefore , I do not see how reason ( iii ) applies to tandem networks .",evaluation
Hk1CjNZyM_22,"- "" The best blocks in each challenge were competitive with the best published results for their numbers of parameters ; see Table 2 for the breakdown . """,quote
Hk1CjNZyM_23,What are the best published results ?,request
Hk1CjNZyM_24,I do not see them in table 2 .,fact
r11STCqxG_0,This paper presents a generic unbiased low-rank stochastic approximation to full rank matrices that makes it possible to do online RNN training without the <VAR> overhead of real-time recurrent learning ( RTRL ) .,fact
r11STCqxG_1,This is an important and long-sought-after goal of connectionist learning,evaluation
r11STCqxG_2,"and this paper presents a clear and concise description of why their method is a natural way of achieving that goal , along with experiments on classic toy RNN tasks with medium-range time dependencies for which other low-memory-overhead RNN training heuristics fail .",evaluation
r11STCqxG_3,"My only major complaint with the paper is that it does not extend the method to large-scale problems on real data , for instance work from the last decade on sequence generation , speech recognition or any of the other RNN success stories that have led to their wide adoption",evaluation
r11STCqxG_4,"( eg Graves 2013 , Sutskever , Martens and Hinton 2011 or Graves , Mohamed and Hinton 2013 ) .",reference
r11STCqxG_5,"However , if the paper does achieve what it claims to achieve , I am sure that many people will soon try out UORO to see if the results are in any way comparable .",evaluation
Bk2K_F9lz_0,The paper presents an off-policy actor-critic method for learning a stochastic policy with entropy regularization .,fact
Bk2K_F9lz_1,"It is a direct extension of maximum entropy reinforcement learning for Q-learning ( recently called soft-Q learning ) , and named soft actor-critic ( SAC ) .",fact
Bk2K_F9lz_2,"Empirically SAC is shown to outperform DDPG significantly in terms of stability and sample efficiency , and can solve relatively difficult tasks that previously only on-policy ( or hybrid on-policy/off-policy ) method such as TRPO/PPO can solve stably .",fact
Bk2K_F9lz_3,"Besides entropy regularization , it also introduces multi-modal policy parameterization through mixture of Gaussians that enables diverse , on-policy exploration .",fact
Bk2K_F9lz_4,The main appeal of the paper is the strong empirical performance of this new off-policy method in continuous action benchmarks .,fact
Bk2K_F9lz_5,"Several design choices could be the key ,",fact
Bk2K_F9lz_6,"so it is encouraged to provide more ablation studies on these , which would be highly valuable for the community .",request
Bk2K_F9lz_7,"In particular , - Amortization of Q and \ pi through fitting state value function",request
Bk2K_F9lz_8,- On-policy exploration vs OU process based off-policy exploration,request
Bk2K_F9lz_9,- Mixture vs non-mixture-based stochastic policy,request
Bk2K_F9lz_10,- SAC vs soft Q-learning,request
Bk2K_F9lz_11,Another valuable discussion to be had is the stability of off-policy algorithm comparing Q-learning versus actor-critic method .,request
Bk2K_F9lz_12,Pros : - Simple off-policy algorithm that achieves significantly better performance than existing off-policy baseline algorithms,evaluation
Bk2K_F9lz_13,"- It allows on-policy exploration in off-policy learning , partially thanks to entropy regularization that prevents variance from shrinking to 0 .",fact
Bk2K_F9lz_14,It could be considered a major success of off-policy algorithm that removes heuristic exploration noise .,evaluation
Bk2K_F9lz_15,Cons : - Method is relatively simple extension from existing work in maximum entropy reinforcement learning .,evaluation
Bk2K_F9lz_16,It is unclear what aspects lead to significant improvements in performance due to insufficient ablation studies .,evaluation
Bk2K_F9lz_17,Other question : - Above Eq . 7 it discusses that fitting a state value function wrt Q and \ pi is shown to improve the stability significantly .,fact
Bk2K_F9lz_18,Is this comparison with directly estimating state value using finite samples ?,request
Bk2K_F9lz_19,"If so , is the primary instability due to variance of the estimate , which can be avoided by drawing a lot of samples or do full integration ( still reasonably tractable for finite mixture model ) ?",request
Bk2K_F9lz_20,"Or , is the instability from elsewhere ?",request
Bk2K_F9lz_21,"By having SGD-based fitting of state value function , it appears to simulate slowly changing target values ( similar role as target networks ) .",evaluation
Bk2K_F9lz_22,"If so , could a similar technique be used with DDPG and get more stable performance ?",request
BkWlPOFlM_0,"The authors present confidence-based autodidactic returns , a Deep learning RL method to adjust the weights of an eligibility vector in TD ( lambda ) - like value estimation to favour more stable estimates of the state .",fact
BkWlPOFlM_1,The key to being able to learn these confidence values is to not allow the error of the confidence estimates propagate back though the deep learning architecture .,evaluation
BkWlPOFlM_2,"However , the method by which these confidence estimates are refined could be better described .",request
BkWlPOFlM_3,"The authors describe these confidences variously as : "" some notion of confidence that the agent has in the value function estimate "" and "" weighing the returns based on a notion of confidence has been explored earlier ( White & White , 2016 ; Thomas et al. , 2015 ) "" .",fact
BkWlPOFlM_4,But the exact method is difficult to piece together from what is written .,evaluation
BkWlPOFlM_5,I believe that the confidence estimates are considered to be part of the critic,evaluation
BkWlPOFlM_6,and the w vector to be part of the theta_c parameters .,evaluation
BkWlPOFlM_7,This would then be captured by the critic gradient for the CAR method that appears towards the end of page 5 .,fact
BkWlPOFlM_8,"If so , this should be stated explicitly .",request
BkWlPOFlM_9,There is another theoretical point that could be clearer .,evaluation
BkWlPOFlM_10,"The variation in an autodidactic update of a value function ( Equation ( 4 ) ) depends on a few things ,",fact
BkWlPOFlM_11,the in variation future value function estimates themselves being just one factor .,fact
BkWlPOFlM_12,"Another two sources of variation are : the uncertainty over how likely each path is to be taken , and the uncertainty in immediate rewards accumulated as part of some n-step return .",fact
BkWlPOFlM_13,"In my opinion , the quality of the paper would be much improved by a brief discussion of this ,",request
BkWlPOFlM_14,and some reflection on what aspects of these variation contribute to the confidence vectors and what is n't captured .,request
BkWlPOFlM_15,"Nonetheless , I believe that the paper represents an interesting and worthy submission to the conference .",evaluation
BkWlPOFlM_16,I would strongly urge the authors to improve the method description in the camera read version though .,request
BkWlPOFlM_17,A few additional comments are as follows : • The plot in Figure 3 is the leading collection of results to demonstrate the dominance of the authors ' adaptive weight approach ( CAR ) over the A3C ( TD ( 0 ) estimates ) and LRA3C ( truncated TD ( lambda ) estimates ) approaches .,evaluation
BkWlPOFlM_18,"However , the way the results are presented/plotted , namely the linear plot of the ( shifted ) relative performance of CAR ( and LRA3C ) versus A3C , visually inflates the importance of tasks on which CAR ( and LRA3C ) perform better than A3C , and diminishes the importance of those tasks on which A3C performs better .",evaluation
BkWlPOFlM_19,It would be better kept as a relative value and plotted on a log-scale so that positive and negative improvements can be viewed on an equal setting .,request
BkWlPOFlM_20,"• On page 3 , when Gt is first mentioned , Gt should really be described first , before the reader is told what it is often replaced with .",request
BkWlPOFlM_21,"• On page 3 , where delta_t is defined ( the j step return TD error , I think the middle term should be <VAR>",request
BkWlPOFlM_22,"• On page 4 and 5 , when describing the gradient for the actor and critic , it would be better if these were given their own terminology , but if not , then use of the word respectively in each case would help .",request
Syi9ojdgf_0,This paper proposed a new parametrization scheme for weight matrices in neural network based on the Householder reflectors to solve the gradient vanishing and exploding problems in training .,fact
Syi9ojdgf_1,The proposed method improved two previous papers :,fact
Syi9ojdgf_2,"1 ) stronger expressive power than Mahammedi et al. ( 2017 ) ,",reference
Syi9ojdgf_3,2 ) faster gradient update than Vorontsov et al. ( 2017 ) .,reference
Syi9ojdgf_4,The proposed parametrization scheme is natrual from numerical linear algebra point of view and authors did a good job in Section 3 in explaining the corresponding expressive power .,evaluation
Syi9ojdgf_5,The experimental results also look promising .,evaluation
Syi9ojdgf_6,It would be nice if the authors can analyze the spectral properties of the saddle points in linear RNN ( nonlinear is better but it 's too difficult I believe ) .,request
Syi9ojdgf_7,"If the authors can show the strict saddle properties then as a corollary , ( stochastic ) gradient descent finds a global minimum .",request
Syi9ojdgf_8,Overall this is a strong paper,evaluation
Syi9ojdgf_9,and I recommend to accept .,evaluation
r1gHCrFlM_0,This paper studies off-policy learning in the bandit setting .,fact
r1gHCrFlM_1,It develops a new learning objective where the empirical risk is regularized by the squared Chi-2 divergence between the new and old policy .,fact
r1gHCrFlM_2,"This objective is motivated by a bound on the empirical risk , where this divergence appears .",fact
r1gHCrFlM_3,The authors propose to solve this objective by using generative adversarial networks for variational divergence minimization ( f-GAN ) .,fact
r1gHCrFlM_4,The algorithm is then evaluated on settings derived from supervised learning tasks and compared to other algorithms .,fact
r1gHCrFlM_5,I find the paper well written and clear .,evaluation
r1gHCrFlM_6,I like that the proposed method is both supported by theory and empirical results .,evaluation
r1gHCrFlM_7,Minor point : I do not really agree with the discussion on the impact of the stochasticity of the logging policy in section 5.6 .,evaluation
r1gHCrFlM_8,"Based on Figure 5 a and b , it seems that the learned policy is performing equally well no matter how stochastic the logging policy is .",fact
r1gHCrFlM_9,So I find it a bit misleading to suggest that the learned policy are not being improved when the logging policy is more deterministic .,evaluation
r1gHCrFlM_10,"Rather , the gap reduces between the two policies",fact
r1gHCrFlM_11,because the logging policy gets better .,evaluation
r1gHCrFlM_12,"In order to better showcase this mechanism , perhaps you could try using a logging policy that does not favor the best action .",request
r1gHCrFlM_13,quality and clarity : + + code made available,fact
r1gHCrFlM_14,+ well written and clear,evaluation
r1gHCrFlM_15,- The proof of theorem 2 is not in the paper nor appendix,fact
r1gHCrFlM_16,( the authors say it is similar to another work ) .,fact
r1gHCrFlM_17,originality + good extension of the work by Swaminathan & Joachims ( 2015a ) : derivation of an alternative objective and use of a deep networks .,evaluation
r1gHCrFlM_18,This paper leverages a set of diverse results,evaluation
r1gHCrFlM_19,significance - The proposed method can only be applied if propensity scores were recorded when the data was generated .,fact
r1gHCrFlM_20,- no test on a real setting,fact
r1gHCrFlM_21,+ + The proposed method is supported both by theoretical insights and empirical experiments .,evaluation
r1gHCrFlM_22,+ empirical improvement with respect to previous methods,fact
r1gHCrFlM_23,"details/typos : 3.1 , p3 : <VAR> has an indexed parenthesis",fact
r1gHCrFlM_24,5.2 ; and we more details,fact
r1gHCrFlM_25,5.3 : so that results more comparable,fact
SJppHuogG_0,"re . Introduction , page 2 : Briefly explain here how SAB is different from regular Attention ?",request
SJppHuogG_1,Good paper .,evaluation
SJppHuogG_2,"There 's not that much discussion of the proposed SAB compared to regular Attention ,",evaluation
SJppHuogG_3,perhaps that could be expanded .,request
SJppHuogG_4,"Also , I suggest summarizing the experimental findings in the Conclusion .",request
SJBZut5gM_0,This paper examines ways of producing word embeddings for rare words on demand .,fact
SJBZut5gM_1,"The key real-world use case is for domain specific terms ,",evaluation
SJBZut5gM_2,but here the techniques are demonstrated on rarer words in standard data sets .,fact
SJBZut5gM_3,"The strength of this paper is that it both gives a more systematic framework for and builds on existing ideas ( character-based models , using dictionary definitions ) to implement them as part of a model trained on the end task .",evaluation
SJBZut5gM_4,The contribution is clear but not huge .,evaluation
SJBZut5gM_5,"In general , for the scope of the paper , it seems like what is here could fairly easily have been made into a short paper for other conferences that have that category .",evaluation
SJBZut5gM_6,"The basic method easily fits within 3 pages ,",evaluation
SJBZut5gM_7,"and while the presentation of the experiments would need to be much briefer ,",evaluation
SJBZut5gM_8,this seems quite possible .,evaluation
SJBZut5gM_9,More things could have been considered .,evaluation
SJBZut5gM_10,"Some appear in the paper ,",evaluation
SJBZut5gM_11,and there are some fairly natural other ones such as mining some use contexts of a word ( such as just from Google snippets ) rather than only using textual definitions from wordnet .,evaluation
SJBZut5gM_12,"The contributions are showing that existing work using character-level models and definitions can be improved by optimizing representation learning in the context of the final task , and the idea of adding a learned linear transformation matrix inside the mean pooling model ( p. 3 ) .",evaluation
SJBZut5gM_13,"However , it is not made very clear why this matrix is needed or what the qualitative effect of its addition is .",evaluation
SJBZut5gM_14,The paper is clearly written .,evaluation
SJBZut5gM_15,A paper that should be referred to is the ( short ) paper of,request
SJBZut5gM_16,Dhingra et al. ( 2017 ) : A Comparative Study of Word Embeddings for Reading Comprehension <URL> .,reference
SJBZut5gM_17,While it in no way covers the same ground as this paper,evaluation
SJBZut5gM_18,it is relevant as follows :,evaluation
SJBZut5gM_19,This paper assumes a baseline that is also described in that paper of using a fixed vocab and mapping other words to UNK .,fact
SJBZut5gM_20,"However , they point out that at least for matching tasks like QA and NLI that one can do better by assigning random vectors on the fly to unknown words .",fact
SJBZut5gM_21,That method could also be considered as a possible approach to compare against here .,request
SJBZut5gM_22,"Other comments : - The paper suggests a couple of times including at the end of the 2nd Intro paragraph that you ca n't really expect spelling models to perform well in representing the semantics of arbitrary words ( which are not morphological derivations , etc. ) .",fact
SJBZut5gM_23,"While this argument has intuitive appeal ,",evaluation
SJBZut5gM_24,"it seems to fly in the face of the fact that actually spelling models , including in this paper , seem to do surprisingly well at learning such arbitrary semantics .",evaluation
SJBZut5gM_25,- p. 2 : You use pretrained GloVe vectors that you do not update .,fact
SJBZut5gM_26,"My impression is that people have had mixed results , sometimes better , sometimes worse with updating pretrained vectors or not .",evaluation
SJBZut5gM_27,Did you try it both ways ?,non-arg
SJBZut5gM_28,"- fn . 1 : Perhaps slightly exaggerates the point being made ,",evaluation
SJBZut5gM_29,"since people usually also get good results with the GloVe or word2vec model trained on "" only "" 6 billion words – 2 orders of magnitude less data .",evaluation
SJBZut5gM_30,"- p. 4 . When no definition is available , is making e_d ( w ) a zero vector worse than or about the same as using a trained UNK vector ?",request
SJBZut5gM_31,- Table 1 : The baseline seems reasonable ( near enough to the quality of the original Salesforce model from 2016 ( 66 F1 ) but well below current best single models of around 76-78 F1 .,evaluation
SJBZut5gM_32,The difference between D1 and D3 does well illustrate that better definition learning is done with backprop from end objective .,evaluation
SJBZut5gM_33,This model shows the rather strong performance of spelling models – at least on this task,evaluation
SJBZut5gM_34,– which he again benefit from training in the context of the end objective .,evaluation
SJBZut5gM_35,"- Fig 2 : It 's weird that only the + dict ( left ) model learns to connect "" In "" and "" where "" .",evaluation
SJBZut5gM_36,"The point made in the text between "" Where "" and "" overseas "" is perfectly reasonable ,",evaluation
SJBZut5gM_37,"but it is a mystery why the base model on the right does n't learn to associate the common words "" where "" and "" in "" both commonly expressing a location .",evaluation
SJBZut5gM_38,- Table 2 : These results are interestingly different .,evaluation
SJBZut5gM_39,Dict is much more useful than spelling here .,evaluation
SJBZut5gM_40,"I guess that is because of the nature of NLI ,",evaluation
SJBZut5gM_41,but it is n't 100 % clear why NLI benefits so much more than QA from definitional knowledge .,evaluation
SJBZut5gM_42,- p. 7 : I was slightly surprised by how small vocabs ( 3k and 5k words ) are said to be optimal for NLI ( and similar remarks hold for SQuAD ) .,evaluation
SJBZut5gM_43,"My impression is that most papers on NLI use much larger vocabs , no ?",evaluation
SJBZut5gM_44,- Fig 3 : This could really be drawn considerably better :,evaluation
SJBZut5gM_45,make the dots bigger and their colors more distinct .,request
SJBZut5gM_46,"- Table 3 : The differences here are quite small and perhaps the least compelling , but the same trends hold .",evaluation
rkvVN7kGf_0,This paper describes how to use a set of sentences in the source language mapped to another set of sentences in the target sentences instead of using single sentence to sentence samples .,fact
rkvVN7kGf_1,The paper claims superior results using the described method .,fact
rkvVN7kGf_2,"Overall , there are a few problems with the paper .",evaluation
rkvVN7kGf_3,1 ) The arguments for using clusters instead of single sentences are questionable .,evaluation
rkvVN7kGf_4,The paper claims several times that MLE training for NMT faces over-training ( or data sparsity ),fact
rkvVN7kGf_5,"-- while that can be true depending on the corpus and model used , there are well-known remedies for that , for example regularization via dropout ( almost everybody uses that ) .",fact
rkvVN7kGf_6,It is not clear why that is not used or at least compared to the method presented .,evaluation
rkvVN7kGf_7,"2 ) The writing of the paper is often unclear ( and sometimes grammatically wrong , typos etc. but that aside ) ,",evaluation
rkvVN7kGf_8,"there are some made up words/concepts ( What is ' Golden Centroid Augmentation "" or "" Model Centroid Augmentation "" ?",fact
rkvVN7kGf_9,"The reason for attention is not to better memorize input information ,",fact
rkvVN7kGf_10,it is to be able to attend to certain regions in the input .,fact
rkvVN7kGf_11,The reason to use RL is to focus on optimizing directly for BLEU score or other metrics instead of likelihood,fact
rkvVN7kGf_12,but not for improving on the train/test loss discrepancy .,fact
rkvVN7kGf_13,There are lots more examples of unclear statements in this paper,evaluation
rkvVN7kGf_14,-- it should be heavily improved .,request
rkvVN7kGf_15,"3 ) Section 3 and 4 are very hard/impossible to understand ,",evaluation
rkvVN7kGf_16,it is not clear how the formulas help the reader to better understand the concept in any way .,evaluation
rkvVN7kGf_17,5 ) The results presented in this paper given the complexity of the method are just not great,evaluation
rkvVN7kGf_18,"-- for example , WMT en-de is 21.3 BLEU reported by you while much older papers report for example 24.67 BLEU ( Google 's Neural Machine Translation System )",fact
rkvVN7kGf_19,-- why not first try to get to state-of-the-art with already published methods and then try to improve on top of that ? .,request
rkvVN7kGf_20,"6 ) Finally , what is missing most is simply why a much simpler method ( just generate some data using a trained system and use that as additional training data , with details on how much etc. ) -- is not directly compared to this very complicated looking method .",request
SJ13MSaxf_0,The authors demonstrate experimentally a problem with the way common latent space operations such as linear interpolation are performed for GANs and VAEs .,fact
SJ13MSaxf_1,They propose a solution based on matching distributions using optimal transport .,fact
SJ13MSaxf_2,"Quite heavy machinery to solve a fairly simple problem ,",evaluation
SJ13MSaxf_3,but their approach is practical and effective experimentally,evaluation
SJ13MSaxf_4,( though the gain over the simple SLERP heuristic is often marginal ) .,evaluation
SJ13MSaxf_5,The problem they describe ( and so the solution ) deserves to be more widely known .,evaluation
SJ13MSaxf_6,"Major comments : The paper is quite verbose , probably unnecessarily so .",evaluation
SJ13MSaxf_7,"Firstly , the authors devote over 2 pages to examples that distribution mismatches can arise in synthetic cases ( section 2 ) .",fact
SJ13MSaxf_8,This point is well made by a single example ( e.g. section 2.2 ),evaluation
SJ13MSaxf_9,and the interesting part is that this is also an issue in practice ( experimental section ) .,evaluation
SJ13MSaxf_10,"Secondly , the authors spend a lot of space on the precise derivation of the optimal transport map for the uniform distribution .",evaluation
SJ13MSaxf_11,"The fact that the optimal transport computation decomposes across dimensions for pointwise operations is very relevant , and the matching of CDFs ,",evaluation
SJ13MSaxf_12,"but I think a lot of the mathematical detail could be relegated to an appendix , especially the detailed derivation of the particular CDFs .",request
SJ13MSaxf_13,"Minor comments : It seems worth highlighting that in practice , for the common case of a Gaussian , the proposed method for linear interpolation is just a very simple procedure that might be called "" projected linear interpolation "" , where the generated vector is multiplied by a constant .",request
SJ13MSaxf_14,"All the optimal transport theory is nice ,",evaluation
SJ13MSaxf_15,but it 's helpful to know that this is simple to apply in practice .,evaluation
SJ13MSaxf_16,Might I suggest a very simple approach to fixing the distribution mismatch issue ?,evaluation
SJ13MSaxf_17,Train with a spherical uniform prior .,request
SJ13MSaxf_18,"When interpolating , project the linear interpolation back to the sphere .",request
SJ13MSaxf_19,"This matches distribution , and has the attractive property that the entire geodesic between two points lies in a region with typical probability density .",fact
SJ13MSaxf_20,This would also work for vicinity sampling .,fact
SJ13MSaxf_21,"In section 1 , overfitting concerns seem like a strange way to motivate the desire for smoothness .",evaluation
SJ13MSaxf_22,"Overfitting is relatively easy to compensate for ,",evaluation
SJ13MSaxf_23,and investigating the latent space is interesting regardless .,evaluation
SJ13MSaxf_24,"When discussing sampling from VAEs as opposed to GANs , it would be good to mention that one has to sample from <VAR> not just <VAR> .",request
SJ13MSaxf_25,"Lots of math typos such as <VAR> should be <VAR> in ( 2 ) , "" V times a times r "" instead of "" Var "" in ( 3 ) and "" s times i times n "" instead of "" sin "" , etc , <VAR> instead of <VAR> , inconsistent bolding of vectors .",request
SJ13MSaxf_26,Also strange use of blackboard bold Z to mean a vector of random variables instead of the integers .,evaluation
SJ13MSaxf_27,"Could cite an existing source for the fact that most mass for a Gaussian is concentrated on a thin shell ( section 2.2 ) ,",request
SJ13MSaxf_28,"e.g. David MacKay Information Theory , Inference and Learning Algorithms .",reference
SJ13MSaxf_29,"At the end of section 2.4 , a plot of the final 1D-to-1D optimal transport function ( for a few different values of t ) for the uniform case would be incredibly helpful .",request
SJ13MSaxf_30,Section 3 should be a subsection of section 2 .,request
SJ13MSaxf_31,"For both SLERP and the proposed method , there 's quite a sudden change around the midpoint of the interpolation in Figure 2 .",evaluation
SJ13MSaxf_32,It would be interesting to plot more points around the midpoint to see the transition in more detail .,request
SJ13MSaxf_33,"( A small inkling that samples from the proposed approach might change fastest qualitatively near the midpoint of the interpolation perhaps maybe be seen in Figure 1 ,",evaluation
SJ13MSaxf_34,since the angle is changing fastest there ?? ),evaluation
H15HuyMlz_0,"This paper proposes an importance-weighted estimator of the MMD , in order to estimate the MMD between distributions based on samples biased according to a known scheme .",fact
H15HuyMlz_1,"It then discusses how to estimate the scheme when it is unknown , and further proposes using it in either the MMD-based generative models of Y. Li et al. ( 2015 ) / Dziugaite et al. ( 2015 ) , or in the MMD GAN of C.-L . Li et al. ( 2017 ) .",fact
H15HuyMlz_2,"The estimator itself is natural ( and relatively obvious ) ,",evaluation
H15HuyMlz_3,though it has some drawbacks that are n't fully discussed ( below ) .,evaluation
H15HuyMlz_4,"The application to GAN-type learning is reasonable , and topical .",evaluation
H15HuyMlz_5,"The first , univariate , experiment shows that the scheme is at least plausible .",evaluation
H15HuyMlz_6,"But the second experiment , involving a simple T ratio based on whether an MNIST digit is a 0 or a 1 , does n't even really work !",evaluation
H15HuyMlz_7,"( The best model only gets the underrepresented class from 20 % up to less than 40 % , rather than the desired 50 % , and the "" more realistic "" setting only to 33 % . )",fact
H15HuyMlz_8,"It would be helpful to debug whether this is due to the classifier being incorrect , estimator inaccuracies , or what .",request
H15HuyMlz_9,"In particular , I would try using T based on a pretrained convnet independent of the autoencoder representation in the MMD GAN , to help diagnose where the failure mode comes from .",request
H15HuyMlz_10,"Without at least a working should-be-easy example like this , and with the rest of the paper 's technical contribution so small , I just do n't think this paper is ready for ICLR .",evaluation
H15HuyMlz_11,It 's also worth noting that the equivalent algorithm for either vanilla GANs or Wasserstein GANs would be equally obvious .,evaluation
H15HuyMlz_12,Estimator : In the discussion about ( 2 ) : where does the 1/m bias come from ?,request
H15HuyMlz_13,"This does n't seem to be in Robert and Casella section 3.3.2 , which is the part of the book that I assume you 're referring to",fact
H15HuyMlz_14,"( incidentally , you should specify that rather than just citing a 600-page textbook ) .",request
H15HuyMlz_15,"Moreover , it is worth noting that Robert and Cassela emphasize that if <VAR> is infinite , the importance sampling estimator can be quite bad ( for example , the estimator may have infinite variance ) .",evaluation
H15HuyMlz_16,"This happens when <VAR> puts mass in a neighborhood around 0 , i.e. when the thinned distribution does n't have support at any place that P does .",fact
H15HuyMlz_17,"In the biased-observations case , this is in some sense unsurprising :",evaluation
H15HuyMlz_18,"if we do n't see * any * data in a particular class of inputs , then our estimates can be quite bad",evaluation
H15HuyMlz_19,( since we know nothing about a group of inputs that might strongly affect the results ) .,fact
H15HuyMlz_20,"In the modulating case , the equivalent situation is when <VAR> lacks a mean ,",fact
H15HuyMlz_21,which seems less likely .,evaluation
H15HuyMlz_22,"Thus although this is probably not a huge problem for your case ,",evaluation
H15HuyMlz_23,it 's worth at least mentioning .,request
H15HuyMlz_24,( See also the following relevant blog posts :,evaluation
H15HuyMlz_25,<URL>,reference
H15HuyMlz_26,and <URL> . ),reference
H15HuyMlz_27,"The paper might be improved by stating ( and proving ) a theorem with expressions for the rate of convergence of the estimator , and how they depend on T.",request
H15HuyMlz_28,"Minor : Another piece of somewhat-related work is Xiong and Schneider , Learning from Point Sets with Observational Bias , UAI 2014 .",evaluation
H15HuyMlz_29,"Sutherland et al. 2016 and 2017 , often referenced in the same block of citations , are the same paper .",fact
H15HuyMlz_30,"On page 3 , above ( 1 ) : "" Since we have projected the distributons into an infinite-dimensional space , the distance between the two distributions is zero if and only if all their moments are the same . """,quote
H15HuyMlz_31,An infinite-dimensional space is n't enough ;,fact
H15HuyMlz_32,"the kernel must further be characteristic , as you mention .",fact
H15HuyMlz_33,See e.g. Sriperumbuder et al. ( AISTATS 2010 ) for more details .,reference
H15HuyMlz_34,"Figure 1 ( b ) seems to be plotting only the first term of <VAR> , without the + 0.5 .",evaluation
HyZBE0IlM_0,This paper studies how the variance of the discriminator affect the gradient signal provided to the generator and therefore how it might limit its ability to learn the true data distribution .,fact
HyZBE0IlM_1,The approach suggested in this paper models the output of the discriminator using a mixture of two Gaussians ( one for “ fake ” and the other for “ not fake ” ) .,fact
HyZBE0IlM_2,This seems like a rather crude approximation as the distribution of each “ class ” is likely to be multimodal .,evaluation
HyZBE0IlM_3,Can the authors comment on this ?,non-arg
HyZBE0IlM_4,Could they extend their approach to use a mixture of multimodal distributions ?,request
HyZBE0IlM_5,The paper mentions that fixing the means of the distribution can be “ problematic during optimization as the discriminator ’s goal is to maximize the difference between these two means . “ .,fact
HyZBE0IlM_6,This relates to my previous comment where the distribution might not be unimodal .,fact
HyZBE0IlM_7,"In this case , shifting the mean does n’t seem to be a good solution and might just yield to oscillations between different modes .",evaluation
HyZBE0IlM_8,Can you please comment on this ?,non-arg
HyZBE0IlM_9,Mode collapse : Can you comment on the behavior of your approach w.r.t. to mode collapse ?,request
HyZBE0IlM_10,Implementation details : How is the mean of the two Gaussians initialized ?,request
HyZBE0IlM_11,"Relation to instance noise and regularization techniques : Instance noise is a common trick being used to train GANs ,",fact
HyZBE0IlM_12,see e.g. <URL>,reference
HyZBE0IlM_13,"This also relates to some regularization techniques , e.g. Roth et al. , 2017 that provides a regularizer that amounts to convolving the densities with white Gaussian noise .",evaluation
HyZBE0IlM_14,Can you please elaborate on the potential advantages of the proposed solution over these existing techniques ?,request
HyZBE0IlM_15,"Comparison to existing baselines : Given that the paper addresses the stability problem , I would expect some empirical comparison to at least one or two of the stability methods cited in the introduction , e.g. Gulrajani et al. , 2017 or Roth et al. , 2017 .",request
HyZBE0IlM_16,Relation to Kernel MMD : Can the authors elaborate on how their method relates to approaches that replace the discriminator with MMD nets . e.g.,request
HyZBE0IlM_17,"- Training generative neural networks via Maximum Mean Discrepancy optimization , Dziugaite et al",reference
HyZBE0IlM_18,"- Generative models and model criticism via optimized maximum mean discrepancy , Sutherland et al",reference
HyZBE0IlM_19,"More explicitly , the variance in these methods can be controlled via the bandwidth of the kernel",fact
HyZBE0IlM_20,and I therefore wonder what would one use a simple mixture of Gaussians instead ?,evaluation
H1g6cQsxM_0,This paper tackles the overfitting problem when training neural networks based on regularization technique .,fact
H1g6cQsxM_1,"More precisely , the authors propose new regularization terms that are related to the underlying virtual geometrical transformations ( shift , rotation and scale ) of the input data ( signal , image and video ) .",fact
H1g6cQsxM_2,"By formalizing the geometrical transformation process of a given image , the authors deduce constraints on the objective function which depend on the magnitude of the applied transformation .",fact
H1g6cQsxM_3,The proposed method is compared to three methods : one baseline and two methods of the literature ( AT and VAT ) .,fact
H1g6cQsxM_4,"The comparison is done on three datasets ( synthetic data , MNIST and CIFAR10 ) in terms of test errors ( for classification problems ) and running time .",fact
H1g6cQsxM_5,The paper is well formalized,evaluation
H1g6cQsxM_6,and the idea is interesting .,evaluation
H1g6cQsxM_7,The regularization approach is novel compared to the methods of the literature .,fact
H1g6cQsxM_8,Main concerns : 1 ) The experimental validation of the proposed approach is not consistent :,evaluation
H1g6cQsxM_9,The description of the baseline method is not detailed in the paper .,evaluation
H1g6cQsxM_10,"A priori , the baseline should naturally be the method without your regularization terms .",evaluation
H1g6cQsxM_11,"But , this seems to be contrary with what you displayed in Figure 3 .",fact
H1g6cQsxM_12,"Indeed , in Figure 3 , there is three different graphs for the baseline method ( i.e. , one for each regularization term ) .",fact
H1g6cQsxM_13,"It seems that the baseline method depends on the different kinds of regularization term ,",fact
H1g6cQsxM_14,why ?,request
H1g6cQsxM_15,Same question for AT and VAT methods .,request
H1g6cQsxM_16,"In practice , what is the magnitude of the perturbations ?",request
H1g6cQsxM_17,"Please , explain the axis of all the figures .",request
H1g6cQsxM_18,"Please , explain how do you mix your different regularization terms in your method that you call VMT-all ?",request
H1g6cQsxM_19,All the following points are related to the experiment for which you presented the results in Table 2 :,fact
H1g6cQsxM_20,"Please , provide the results of all your methods on the synthetic dataset ( only VMT-shift is provided ) .",request
H1g6cQsxM_21,What is VMF ?,request
H1g6cQsxM_22,Do you mean VMT ?,request
H1g6cQsxM_23,"For the evaluations , it would be more rigorous to re-implement also the state-of-the-art methods for which you only give the results that they report in their paper .",request
H1g6cQsxM_24,"Especially , because you re-implemented AT with L-2 constraint ,",fact
H1g6cQsxM_25,"so , it seems straightforward to re-implement also AT with L-infinite constraint .",request
H1g6cQsxM_26,"Same remark for the dropout regularization technique ,",request
H1g6cQsxM_27,"which is easy to re-implement on the dense layers of your neural networks , within the Tensorflow framework .",evaluation
H1g6cQsxM_28,"As you mentioned , your main contribution is related to running time ,",fact
H1g6cQsxM_29,"thus , you should give the running time in all experiments .",request
H1g6cQsxM_30,2 ) The method seems to be a tradeoff between accuracy and running time :,evaluation
H1g6cQsxM_31,The VAT method performs better than all your methods in all the datasets .,fact
H1g6cQsxM_32,The baseline method is faster than all the methods ( Table 3 ) .,fact
H1g6cQsxM_33,"This being said , the proposed method should be clearly presented in the paper as a tradeoff between accuracy and running time .",request
H1g6cQsxM_34,3 ) The positioning of the proposed approach is not so clear :,evaluation
H1g6cQsxM_35,"As mentioned above , your method is a tradeoff between accuracy and running time .",evaluation
H1g6cQsxM_36,But you also mentioned ( top of page 2 ) that the contribution of your paper is also related to the interpretability in terms of ‘’ Human perception ’’ .,fact
H1g6cQsxM_37,"Indeed , you clearly mentioned that the methods of the literature lacks interpretability .",fact
H1g6cQsxM_38,You also mentioned that your method is more ‘’ geometrically ’’ interpretable than methods of the literature .,fact
H1g6cQsxM_39,The link between interpretability in terms of “ human perception ” and “ geometry ” is not obvious .,evaluation
H1g6cQsxM_40,"Anyway , the interpretability point is not sufficiently demonstrated , or at least , discussed in the paper .",evaluation
H1g6cQsxM_41,4 ) Many typos in the paper :,evaluation
H1g6cQsxM_42,Section 1 : “ farward-backward ”,request
H1g6cQsxM_43,Section 2.1 : “ we define the movement field V of as a n +1 … ”,request
H1g6cQsxM_44,Section 2.2 : “ lable ” - “ the another ” - “ of how it are generated ”,request
H1g6cQsxM_45,– Sentence “ Since V is normalized . ” seems incomplete …,request
H1g6cQsxM_46,- <VAR> not defined,request
H1g6cQsxM_47,"- Please , precise the simplifications like <VAR> to <VAR>",request
H1g6cQsxM_48,Section 3 : “ DISCUSSTION ”,request
H1g6cQsxM_49,Section 4.1 : “ negtive ”,request
H1g6cQsxM_50,Figure 2 : “ negetive ”,request
H1g6cQsxM_51,Table 2 : “ VMF ”,request
H1g6cQsxM_52,Section 4.2 : “ Tab 2.3 ” does not exist,fact
H1g6cQsxM_53,Section 4.3 : “ consists 9 convolutional ” – “ nerual networks ” …,request
H1g6cQsxM_54,"Please , always use the \ eqref latex command to refer to equations .",request
H1g6cQsxM_55,"There is many others typos in the paper ,",evaluation
H1g6cQsxM_56,"so , please proofread the paper …",request
BJGq_QclM_0,"This paper leverages how deep Bayesian NNs , in the limit of infinite width , are Gaussian processes ( GPs ) .",fact
BJGq_QclM_1,"After characterizing the kernel function , this allows us to use the GP framework for prediction , model selection , uncertainty estimation , etc .",fact
BJGq_QclM_2,- Pros of this work The paper provides a specific method to efficiently compute the covariance matrix of the equivalent GP and shows experimentally on CIFAR and MNIST the benefits of using the this GP as opposed to a finite-width non-Bayesian NN .,fact
BJGq_QclM_3,The provided phase analysis and its relation to the depth of the network is also very interesting .,evaluation
BJGq_QclM_4,Both are useful contributions as long as deep wide Bayesian NNs are concerned .,evaluation
BJGq_QclM_5,A different question is whether that regime is actually useful .,evaluation
BJGq_QclM_6,"- Cons of this work Although this work introduces a new GP covariance function inspired by deep wide NNs ,",fact
BJGq_QclM_7,I am unconvinced of the usefulness of this regime for the cases in which deep learning is useful .,evaluation
BJGq_QclM_8,"For instance , looking at the experiments , we can see that on MNIST-50k ( the one with most data , and therefore , the one that best informs about the "" true "" underlying NN structure ) the inferred depth is 1 for the GP and 2 for the NN , i.e. , not deep .",fact
BJGq_QclM_9,"Similarly for CIFAR , where only up to depth 3 is used .",fact
BJGq_QclM_10,None of these results beat state-of-the-art deep NNs .,fact
BJGq_QclM_11,"Also , the results about the phase structure show how increased depth makes the parameter regime in which these networks work more and more constrained .",fact
BJGq_QclM_12,"In [ 1 ] , it is argued that kernel machines with fixed kernels do not learn a hierarchical representation .",fact
BJGq_QclM_13,And such representation is generally regarded as essential for the success of deep learning .,fact
BJGq_QclM_14,My impression is that the present line of work will not be relevant for deep learning and will not beat state-of-the-art results,evaluation
BJGq_QclM_15,because of the lack of a structured prior .,fact
BJGq_QclM_16,"In that sense , to me this work is more of a negative result informing that to be successful , deep Bayesian NNs should not be wide and should have more structure to avoid reaching the GP regime .",evaluation
BJGq_QclM_17,"- Other comments : In Fig. 5 , use a consistent naming for the axes ( bias and variances ) .",request
BJGq_QclM_18,"In Fig. 1 , I did n't find the meaning of the acronym NN with no specified width .",request
BJGq_QclM_19,Does the unit norm normalization used to construct the covariance disallow ARD input selection ?,request
BJGq_QclM_20,[1] <CIT>,reference
H1EMeWfgz_0,"This paper is in some sense a "" position paper , """,evaluation
H1EMeWfgz_1,giving a framework for thinking about the loss functions implicitly used by the generator of GAN-type models .,fact
H1EMeWfgz_2,It advocates thinking about the loss in a way similar to how it is considered in structured prediction .,evaluation
H1EMeWfgz_3,"It also proposes that approximating the dual formulation of various divergences with functions from a parametric class , as is typically done in GAN-type setups , is not only more tractable ( computationally and in sample complexity ) than the full nonparametric estimation , but also gives a better actual loss .",fact
H1EMeWfgz_4,"Overall , I like the argument here , and think that it is a useful framework for thinking about these things .",evaluation
H1EMeWfgz_5,My main concern is that the practical contribution on top of Liu et al. ( 2017 ) might be somewhat limited .,evaluation
H1EMeWfgz_6,"A few small points : - f-divergences can actually be nonparametrically estimated purely from samples , e.g. with the k-nearest neighbor estimator of <URL> , or ( for certain f-divergences ) the kernel density based estimator of <URL> .",fact
H1EMeWfgz_7,"These are unlikely to lead to a practical learning algorithm ,",evaluation
H1EMeWfgz_8,but could be mentioned in Table 1 .,request
H1EMeWfgz_9,- The discussion of MMD in the end of section 3.1 is a little off .,evaluation
H1EMeWfgz_10,MMD is fundamentally defined by the kernel choice ;,fact
H1EMeWfgz_11,"Dziugaite et al. ( 2015 ) only demonstrated that the Gaussian RBF kernel is a poor choice for MNIST modeling ,",evaluation
H1EMeWfgz_12,while the samples of Li et al. ( 2015 ) simply by using a mixture of Gaussian kernels were much better .,evaluation
H1EMeWfgz_13,"No reasonable fixed kernel is likely to yield good results on a harder image modeling problem ,",evaluation
H1EMeWfgz_14,but that is a slightly different message than the one this paragraph conveys .,evaluation
H1EMeWfgz_15,- It would be interesting to replicate the analysis of Danihelka et al. ( 2017 ) on the Thin-8 dataset .,request
H1EMeWfgz_16,"This might help clarify which of the undesirable effects observed in the VAE model here are due to likelihood , and which due to other aspects of VAEs ( like the use of the lower bound ) .",evaluation
SJNicmVeM_0,The paper presents Erdos-Selfridge-Spencer games as environments for investigating deep reinforcement learning algorithms .,fact
SJNicmVeM_1,"The proposed games are interesting and clearly challenging ,",evaluation
SJNicmVeM_2,but I am not sure what they tell us about the algorithms chosen to test them .,evaluation
SJNicmVeM_3,There are some clarity issues with the justification and evaluation which undermine the message the authors are trying to make .,evaluation
SJNicmVeM_4,"In particular , I have the following concerns : • these games have optimal policies that are expressible as a linear model , meaning that if the architecture or updating of the learning algorithm is such that there is a bias towards exploring these parts of policy space , then they will perform better than more general algorithms .",evaluation
SJNicmVeM_5,What does this tell us about the relative merits of each approach ?,request
SJNicmVeM_6,"The authors could do more to formally motivate these games as "" difficult "" for any deep learning architecture if possible .",request
SJNicmVeM_7,"• the authors compare linear models with non-linear models at some point for attacker policies ,",fact
SJNicmVeM_8,but it is unclear whether these linear models are able to express the optimal policy .,evaluation
SJNicmVeM_9,"In fact , there is a level of non-determinism in how the attacker policies are encoded which means that an optimal policy can not be ( even up to soft-max ) expressed by the agent",fact
SJNicmVeM_10,( as I read things the number of pieces chosen in level l is always chosen uniformly randomly ) .,fact
SJNicmVeM_11,"• As the authors state , this paper is an empirical evaluation ,",fact
SJNicmVeM_12,and the theorems presented are derived from earlier work .,fact
SJNicmVeM_13,There is possibly too much focus on the proofs of these theorems .,evaluation
SJNicmVeM_14,• There are a number of ambiguities and errors which places difficulties on the interpretation ( and potential replication ) of the experiments .,evaluation
SJNicmVeM_15,"As this is an empirical study ,",fact
SJNicmVeM_16,this is the yardstick by which the paper should be judged .,evaluation
SJNicmVeM_17,"In particular , this relates to : ◦ The architecture of each of the tested Deep RL methods .",evaluation
SJNicmVeM_18,"◦ What is done to select appropriate tuning parameters of the tested Deep RL methods , if anything .",request
SJNicmVeM_19,"◦ It is unclear whether ' incorrect actions ' in the supervised learning evaluations , refer to non-optimal actions , or simply actions that do not preserve the dominance of the defender , e.g. both partitions may have potential > 0.5",evaluation
SJNicmVeM_20,"◦ Fig 4 . right looks like a reward signal ,",evaluation
SJNicmVeM_21,but is labelled Proportion correct .,fact
SJNicmVeM_22,The text is not clear enough to be sure which it is .,evaluation
SJNicmVeM_23,"◦ Fig 4 . left and right has 4 methods : rl rewards , rl correct actions , sup rewards , and sup correct actions .",fact
SJNicmVeM_24,The specifics of how these methods are constructed is unclear from the paper .,evaluation
SJNicmVeM_25,◦ What parts of the evaluation explores how well these methods are able to represent the states ( feature/representation learning ),request
SJNicmVeM_26,and what parts are evaluating the propagation of sparse rewards ( the reinforcment learning core ) ?,request
SJNicmVeM_27,The authors could be clearer and more targetted with respect to this question .,request
SJNicmVeM_28,"There is value in this work ,",evaluation
SJNicmVeM_29,but in its current state I do not think it is ready for publicaiton .,evaluation
SJNicmVeM_30,"# Detailed notes [ p4 , end of sec 3 ] The authors say that the difficulty of the games can be varied with "" continuous changes in potential "" ,",fact
SJNicmVeM_31,"but the potential is derived from the discrete initial game state ,",fact
SJNicmVeM_32,so these values are not continuously varying ( even though it is possible to adjust them by non-integer amounts ) .,fact
SJNicmVeM_33,"[ p4 , sec 4.1 ] "" strategy unevenly partitions the occupied levels ... with the proportional difference between the two sets being sampled randomly """,quote
SJNicmVeM_34,What is meant by this ?,request
SJNicmVeM_35,"The proportional difference between the two sets is discussed as if it is a continuous property ,",evaluation
SJNicmVeM_36,but must be chosen from the discrete set of all available partitions .,fact
SJNicmVeM_37,"If one partition one is chosen uniformly randomly from all possibly sets A , B ( and the potential proportion calculated ) then I do n't know why it would be written in this way .",evaluation
SJNicmVeM_38,"That suggests that proportions that are closer to 1:1 are chosen more often than "" extreme "" partitions ,",evaluation
SJNicmVeM_39,but how ?,request
SJNicmVeM_40,This feels a little under-justified .,evaluation
SJNicmVeM_41," very different states A , B ( uneven potential , disjoint occupied levels ) ",quote
SJNicmVeM_42,"Are these states really "" very different "" , or at least for the reasons indicated .",evaluation
SJNicmVeM_43,Later on ( Theorem 3 ) we see how an optimal partition is generated .,fact
SJNicmVeM_44,"This chooses a partition where one part contains all pieces in layer ( l +1 ) and above and one part with all pieces in layer ( l-1 ) and below , with layer l being distributed between the two parts .",fact
SJNicmVeM_45,The first part will typically have a slightly lower potential than the other,evaluation
SJNicmVeM_46,and all layers other than layer l will be disjoint .,fact
SJNicmVeM_47,"[ p6 , Fig 4 ] The right plot y-limits vary between -1 and 1",fact
SJNicmVeM_48,so it can not represent a proportion of correct actions .,fact
SJNicmVeM_49,"Also , in the text the authors say :",fact
SJNicmVeM_50,">> The results , shown in Figure 4 are surprising . Reinforcement learning >> is better at playing the game , but does worse at predicting optimal moves .",quote
SJNicmVeM_51,I am not sure which plot shows the playing of the game .,evaluation
SJNicmVeM_52,Is this the right hand plot ?,non-arg
SJNicmVeM_53,In which case are we looking at rewards ?,request
SJNicmVeM_54,"In fact , I am a little confused as to what is being shown here .",evaluation
SJNicmVeM_55,"Is "" sup rewards "" a supervised learning method trained on rewards , or evaluated on rewards , or both ?",request
SJNicmVeM_56,And how is this done .,request
SJNicmVeM_57,The text is just not clear enough .,evaluation
SJNicmVeM_58,[ p7 Fig 6 and text ] Here the authors are comparing how well agents select the optimal actions as compared to how close they are to the end of the game .,fact
SJNicmVeM_59,"This relates to the "" surprising "" fact that "" Reinforcement learning is better at playing the game , but does worse at predicting optimal moves . "" .",evaluation
SJNicmVeM_60,I think an important point here is how many training/test examples there are in each bin .,evaluation
SJNicmVeM_61,"If there are more in the range 3-7 moves from the end of the game , than there are outside this range , then the supervised learner will",fact
SJNicmVeM_62,"[ p8 proof of theorem 3 ] "" <EQN> and <EQN> . """,quote
SJNicmVeM_63,Is it true that both these inequalities are strict ?,request
SJNicmVeM_64, Since A l only contains pieces from levels K to <VAR> ,quote
SJNicmVeM_65,In fact this should read from levels K to l.,request
SJNicmVeM_66, we can move <EQN> pieces from A <VAR> to A l ,quote
SJNicmVeM_67,"Do the authors mean that we can define a partition A , B where <EQN> plus some ( but not all ) elements in level <VAR> ?",request
SJNicmVeM_68, ... such that the potential of the new set equals 0.5 ,quote
SJNicmVeM_69,"It will equal exactly 0.5 as suggested ,",fact
SJNicmVeM_70,but the authors could make it more precise as to why ( there is a value <EQN> ( maybe < = l ) such that <EQN> ( guaranteed ) .,request
SJNicmVeM_71,They should also indicate why this then justifies their proof ( namely that <EQN> ) .,request
SJNicmVeM_72,[ p8 paramterising action space ] A comment : this does n't give as much control as the authors suggest .,evaluation
SJNicmVeM_73,Perhaps the agent should also chose the proportion of elements in layer l to set A.,request
SJNicmVeM_74,"For instance , if there are a large number of elements in l , and or <VAR> is very close to 0.5 ( or <VAR> is very close to 0.5 ) then this does n't give the attacker the opportunity to fine tune the policy to select very good partitions .",evaluation
SJNicmVeM_75,It is unclear expected level of control that agents have under various conditions ( K and starting states ) .,evaluation
SJNicmVeM_76,"[ p9 Fig 8 ] As the defender 's score is functionally determined by the attackers score ,",fact
SJNicmVeM_77,it does n't help to include this on the plot .,request
SJNicmVeM_78,It just distracts from the signal .,evaluation
rynGrnpeM_0,This paper addresses multiple issues arising from the fact that commonly reported best model performance numbers are a single sample from a performance distribution .,fact
rynGrnpeM_1,"These problems are very real ,",evaluation
rynGrnpeM_2,and they deserve significant attention from the ML community .,evaluation
rynGrnpeM_3,"However , I feel that the proposed solution may actually compound the issues highlighted .",evaluation
rynGrnpeM_4,"Firstly , the proposed metric requires calculation of multiple test set experiments for every evaluation .",fact
rynGrnpeM_5,In the paper up to 100 experiments were used .,fact
rynGrnpeM_6,"This may be reasonable in scenarios where the test set is hidden , and individual test numbers are never revealed .",evaluation
rynGrnpeM_7,It also may be reasonable if we cynically assume that researchers are already running many test-set evaluations .,evaluation
rynGrnpeM_8,"But I am very opposed to any suggestion that we should relax the maxim that the test set should be used only once , or as close to once as is possible .",evaluation
rynGrnpeM_9,Even the idea of researchers knowing their test set variance makes me very uneasy .,evaluation
rynGrnpeM_10,"Secondly , this paper tries to account for variation in results due to different degrees of hyper-parameter tuning .",fact
rynGrnpeM_11,"This is certainly an admirable aim ,",evaluation
rynGrnpeM_12,since different research groups have access to very different types of resources .,evaluation
rynGrnpeM_13,"However , the suggested approach relies on randomly picking hyper-parameters from "" a range that we previously found to work reasonably well "" .",fact
rynGrnpeM_14,This randomization does not account for the many experiments that were required to find this range .,fact
rynGrnpeM_15,And the randomization is also not extended to parameters controlling the model architecture,fact
rynGrnpeM_16,( I suspect that a number of experiments went into picking the 32 layers in the ResNet used by this paper ) .,evaluation
rynGrnpeM_17,"Without a solid and consistent basis for these hyper-parameter perturbations , I worry that this approach will fail to normalize the effect of experiment numbers while also giving researchers an excuse to avoid reporting their experimental process .",evaluation
rynGrnpeM_18,I think this is a nice idea,evaluation
rynGrnpeM_19,and the metric does merge the stability and low variance of mean score with the aspirations of best score .,evaluation
rynGrnpeM_20,The metric may be very useful at development time in helping researchers build a reasonable expectation of test time performance in cases where the dev and test sets are strongly correlated .,evaluation
rynGrnpeM_21,"However , for the reasons outlined above , I do n't think the proposed approach solves the problems that it addresses .",evaluation
rynGrnpeM_22,"Ultimately , the decision about this paper is a subjective one .",evaluation
rynGrnpeM_23,Are we willing to increase the risk of inadvertent hyper-parameter tuning on the test set for the sake of a more stable metric ?,evaluation
r1k_ETYlM_0,"This article aims at understanding the role played by the different words in a sentence , taking into account their order in the sentence .",fact
r1k_ETYlM_1,"In sentiment analysis for instance , this capacity is critical to model properly negation .",evaluation
r1k_ETYlM_2,"As state-of-the-art approaches rely on LSTM ,",fact
r1k_ETYlM_3,the authors want to understand which information comes from which gate .,fact
r1k_ETYlM_4,"After a short remainder regarding LSTM , the authors propose a framework to disambiguate interactions between gates .",fact
r1k_ETYlM_5,"In order to obtain an analytic formulation of the decomposition , the authors propose to linearize activation functions in the network .",fact
r1k_ETYlM_6,"In the experiment section , authors compare themselves to a standard logistic regression ( based on a bag of words representation ) .",fact
r1k_ETYlM_7,They also check the unigram sentiment scores ( without context ) .,fact
r1k_ETYlM_8,The main issue consists in modeling the dynamics inside a sentence ( when a negation or a ' used to be ' reverses the sentiment ) .,fact
r1k_ETYlM_9,The proposed approach works fine on selected samples .,evaluation
r1k_ETYlM_10,The related work section is entirely focused on deep learning,evaluation
r1k_ETYlM_11,while the experiment section is dedicated to sentiment analysis .,evaluation
r1k_ETYlM_12,This section should be rebalanced .,request
r1k_ETYlM_13,"Even if the authors claim that their approach is general ,",fact
r1k_ETYlM_14,they also show that it fits well the sentiment analysis task in particular .,fact
r1k_ETYlM_15,"On top of that , a lot of fine-grained sentiment analysis tools has been developed outside deep-learning :",evaluation
r1k_ETYlM_16,the authors should refer to those works .,request
r1k_ETYlM_17,"Finally , authors should provide some quantitative analysis on sentiment classification :",request
r1k_ETYlM_18,a lot of standard benchmarks are widely use in the literature,evaluation
r1k_ETYlM_19,and we need to see how the proposed method performs with respect to the state-of-the-art .,request
r1k_ETYlM_20,"Given the chosen tasks , this work should be compared to the beermind system :",request
r1k_ETYlM_21,<URL>,reference
r1k_ETYlM_22,and the associated publication,non-arg
r1k_ETYlM_23,<URL>,reference
S1_Zyk9xG_0,"Neal ( 1994 ) showed that a one hidden layer Bayesian neural network , under certain conditions , converges to a Gaussian process as the number of hidden units approaches infinity .",fact
S1_Zyk9xG_1,Neal ( 1994 ) and Williams ( 1997 ) derive the resulting kernel functions for such Gaussian processes when the neural networks have certain transfer functions .,fact
S1_Zyk9xG_2,"Similarly , the authors show an analogous result for deep neural networks with multiple hidden layers and an infinite number of hidden units per layer , and show the form of the resulting kernel functions .",fact
S1_Zyk9xG_3,"For certain transfer functions , the authors perform a numerical integration to compute the resulting kernels .",fact
S1_Zyk9xG_4,"They perform experiments on MNIST and CIFAR-10 , doing classification by scaled regression .",fact
S1_Zyk9xG_5,"Overall , the work is an interesting read , and a nice follow-up to Neal ’s earlier observations about 1 hidden layer neural networks .",evaluation
S1_Zyk9xG_6,It combines several insights into a nice narrative about infinite Bayesian deep networks .,evaluation
S1_Zyk9xG_7,"However , the practical utility , significance , and novelty of this work -- in its current form -- are questionable ,",evaluation
S1_Zyk9xG_8,"and the related work sections , analysis , and experiments should be significantly extended .",request
S1_Zyk9xG_9,"In detail : ( 1 ) This paper misses some obvious connections and references , such as",evaluation
S1_Zyk9xG_10,* Krauth et . al ( 2017 ) : “ Exploring the capabilities and limitations of Gaussian process models ” for recursive kernels with GPs .,reference
S1_Zyk9xG_11,* Hazzan & Jakkola ( 2015 ) : “ Steps Toward Deep Kernel Methods from Infinite Neural Networks ” for GPs corresponding to NNs with more than one hidden layer .,reference
S1_Zyk9xG_12,"* The growing body of work on deep kernel learning , which “ combines the inductive biases and representation learning abilities of deep neural networks with the non-parametric flexibility of Gaussian processes ” .",reference
S1_Zyk9xG_13,E.g. : ( i ) “ Deep Kernel Learning ” ( AISTATS 2016 ) ;,reference
S1_Zyk9xG_14,( ii ) “ Stochastic Variational Deep Kernel Learning ” ( NIPS 2016 ) ;,reference
S1_Zyk9xG_15,( iii ) “ Learning Scalable Deep Kernels with Recurrent Structure ” ( JMLR 2017 ) .,reference
S1_Zyk9xG_16,These works should be discussed in the text .,request
S1_Zyk9xG_17,"( 2 ) Moreover , as the authors rightly point out , covariance functions of the form used in ( 4 ) have already been proposed .",fact
S1_Zyk9xG_18,"It seems the novelty here is mainly the empirical exploration ( will return to this later ) , and numerical integration for various activation functions .",evaluation
S1_Zyk9xG_19,That is perfectly fine,evaluation
S1_Zyk9xG_20,-- and this work is still valuable .,evaluation
S1_Zyk9xG_21,"However , the statement “ recently , kernel functions for multi-layer random neural networks have been developed , but only outside of a Bayesian framework ” is incorrect .",fact
S1_Zyk9xG_22,"For example , Hazzan & Jakkola ( 2015 ) in “ Steps Toward Deep Kernel Methods from Infinite Neural Networks ” consider GP constructions with more than one hidden layer .",fact
S1_Zyk9xG_23,Thus the novelty of this aspect of the paper is overstated .,evaluation
S1_Zyk9xG_24,See also comment [ * ] later on the presentation .,non-arg
S1_Zyk9xG_25,"In any case , the derivation for computing the covariance function ( 4 ) of a multi-layer network is a very simple reapplication of the procedure in Neal ( 1994 ) .",evaluation
S1_Zyk9xG_26,"What is less trivial is estimating ( 4 ) for various activations ,",evaluation
S1_Zyk9xG_27,and that seems to the major methodological contribution .,evaluation
S1_Zyk9xG_28,Also note that multidimensional CLT here is glossed over .,evaluation
S1_Zyk9xG_29,It ’s actually really unclear whether the final limit will converge to a multidimensional Gaussian with that kernel without stronger conditions .,evaluation
S1_Zyk9xG_30,This derivation should be treated more thoroughly and carefully .,request
S1_Zyk9xG_31,"( 3 ) Most importantly , in this derivation , we see that the kernels lose the interesting representations that come from depth in deep neural networks .",fact
S1_Zyk9xG_32,"Indeed , Neal himself says that in the multi-output settings , all the outputs become uncorrelated .",fact
S1_Zyk9xG_33,Multi-layer representations are mostly interesting,evaluation
S1_Zyk9xG_34,because each layer shares hidden basis functions .,fact
S1_Zyk9xG_35,"Here , the sharing is essentially meaningless ,",evaluation
S1_Zyk9xG_36,because the variance of the weights in this derivation shrinks to zero .,fact
S1_Zyk9xG_37,"In Neal ’s case , the method was explored for single output regression ,",fact
S1_Zyk9xG_38,where the fact that we lose this sharing of basis functions may not be so restrictive .,evaluation
S1_Zyk9xG_39,"However , these assumptions are very constraining for multi-output classification and also interesting multi-output regressions .",evaluation
S1_Zyk9xG_40,"[ * ] : Generally , in reading the abstract and introduction , we get the impression that this work somehow allows us to use really deep and infinitely wide neural networks as Gaussian processes , and even without the pain of training these networks .",evaluation
S1_Zyk9xG_41,“ Deep neural networks without training deep networks ” .,quote
S1_Zyk9xG_42,This is not an accurate portrayal .,evaluation
S1_Zyk9xG_43,"The very title “ Deep neural networks as Gaussian processes ” is misleading ,",evaluation
S1_Zyk9xG_44,since it ’s not really the deep neural networks that we know and love .,fact
S1_Zyk9xG_45,"In fact , you lose valuable structure when you take these limits ,",fact
S1_Zyk9xG_46,and what you get is very different than a standard deep neural network .,evaluation
S1_Zyk9xG_47,"In this sense , the presentation should be re-worked .",request
S1_Zyk9xG_48,"( 4 ) Moreover , neural networks are mostly interesting because they learn the representation .",evaluation
S1_Zyk9xG_49,"To do something similar with GPs , we would need to learn the kernel .",evaluation
S1_Zyk9xG_50,"But here , essentially no kernel learning is happening .",fact
S1_Zyk9xG_51,The kernel is fixed .,fact
S1_Zyk9xG_52,"( 5 ) Given the above considerations , there is great importance in understanding the practical utility of the proposed approach through a detailed empirical evaluation .",evaluation
S1_Zyk9xG_53,"In other words , how structured is this prior and does it really give us some of the interesting properties of deep neural networks , or is it mostly a cute mathematical trick ?",request
S1_Zyk9xG_54,"Unfortunately , the empirical evaluation is very preliminary , and provides no reassurance that this approach will have any practical relevance :",evaluation
S1_Zyk9xG_55,( i ) Directly performing regression on classification problems is very heuristic and unnecessary .,evaluation
S1_Zyk9xG_56,"( ii ) Given the loss of dependence between neurons in this approach , it makes sense to first explore this method on single output regression , where we will likely get the best idea of its useful properties and advantages .",request
S1_Zyk9xG_57,( iii ) The results on CIFAR10 are very poor .,evaluation
S1_Zyk9xG_58,"We do n’t need to see SOTA performance to get some useful insights in comparing for example parametric vs non-parametric ,",evaluation
S1_Zyk9xG_59,but 40 % more error than SOTA makes it very hard to say whether any of the observed patterns hold weight for more competitive architectural choices .,evaluation
S1_Zyk9xG_60,A few more minor comments : ( i ) How are you training a GP exactly on 50k training points ?,evaluation
S1_Zyk9xG_61,Even storing a 50k x 50k matrix requires about 20 GB of RAM .,fact
S1_Zyk9xG_62,"Even with the best hardware , computing the marginal likelihood dozens of times to learn hyperparameters would be near impossible .",fact
S1_Zyk9xG_63,What are the runtimes ?,request
S1_Zyk9xG_64,"( ii ) "" One benefit in using the GP is due to its Bayesian nature , so that predictions have uncertainty estimates ( Equation ( 9 ) ) . ”",quote
S1_Zyk9xG_65,"The main benefit of the GP is not the uncertainty in the predictions , but the marginal likelihood which is useful for kernel learning .",fact
SysTGDdxf_0,"This paper is based on the theory of group equivariant CNNs ( G-CNNs ) , proposed by Cohen and Welling ICML ' 16 .",fact
SysTGDdxf_1,"Regular convolutions are translation-equivariant , meaning that if an image is translated , its convolution by any filter is also translated .",fact
SysTGDdxf_2,They are however not rotation-invariant for example .,fact
SysTGDdxf_3,"G-CNN introduces G-convolutions , which are equivariant to a given transformation group G.",fact
SysTGDdxf_4,"This paper proposes an efficient implementation of G-convolutions for 6-fold rotations ( rotations of multiple of 60 degrees ) , using a hexagonal lattice .",evaluation
SysTGDdxf_5,"The approach is evaluated on CIFAR-10 and AID , a dataset of aerial scene classification .",fact
SysTGDdxf_6,"The approach outperforms G-convolutions implemented on a squared lattice , which allows only 4-fold rotations on AID by a short margin .",fact
SysTGDdxf_7,"On CIFAR-10 , the difference does not seem significative ( according to Tables 1 and 2 ) .",evaluation
SysTGDdxf_8,"I guess this can be explained by the fact that rotation equivariance makes sense for aerial images , where the scene is mostly fronto-parallel , but less for CIFAR ( especially in the upper layers ) , which exhibits 3D objects .",fact
SysTGDdxf_9,I like the general approach of explicitly putting desired equivariance in the convolutional networks .,evaluation
SysTGDdxf_10,"Using a hexagonal lattice is elegant , even if it is not new in computer vision",evaluation
SysTGDdxf_11,( as written in the paper ) .,fact
SysTGDdxf_12,"However , as the transformation group is limited to rotations ,",fact
SysTGDdxf_13,"this is interesting in practice mostly for fronto-parallel scenes , as the experiences seem to show .",evaluation
SysTGDdxf_14,It is not clear how the method can be extended to other groups than 2D rotations .,evaluation
SysTGDdxf_15,"Moreover , I feel like the paper sometimes tries to mask the fact that the proposed method is limited to rotations .",evaluation
SysTGDdxf_16,"It is admittedly clearly stated in the abstract and introduction , but much less in the rest of the paper .",evaluation
SysTGDdxf_17,The second paragraph of Section 5.1 is difficult to keep in a paper .,evaluation
SysTGDdxf_18,"It says that "" From a qualitative inspection of these hexagonal interpolations we conclude that no information is lost during the sampling procedure . """,quote
SysTGDdxf_19," No information is lost  is a strong statement from a qualitative inspection , especially of a hexagonal image .",evaluation
SysTGDdxf_20,This statement should probably be removed .,request
SysTGDdxf_21,One way to evaluate the information lost could be to iterate interpolation between hexagonal and squared lattices to see if the image starts degrading at some point .,request
SyU_UK2lf_0,This paper is about modifications to the skip-thought framework for learning sentence embeddings .,fact
SyU_UK2lf_1,The results show performance comparable to or better than skip-thought while decreasing training time .,fact
SyU_UK2lf_2,I think the overall approach makes sense :,evaluation
SyU_UK2lf_3,use an RNN encoder,fact
SyU_UK2lf_4,"because we know it works well ,",evaluation
SyU_UK2lf_5,but improve training efficiency by changing the decoder to a combination of feed-forward and convolutional layers .,fact
SyU_UK2lf_6,I think it may be the case that this works well because the decoder is not auto-regressive but merely predicts each word independently .,fact
SyU_UK2lf_7,This is possible because the decoder will not be used after training .,fact
SyU_UK2lf_8,So all the words can be predicted all at once with a fixed maximum sentence length .,fact
SyU_UK2lf_9,"In typical encoder-decoder applications , the decoder is used at test time to get predictions ,",fact
SyU_UK2lf_10,so it is natural to make it auto-regressive .,evaluation
SyU_UK2lf_11,"But in this case , the decoder is thrown away after training ,",fact
SyU_UK2lf_12,so it makes more sense to make the decoder non-auto-regressive .,evaluation
SyU_UK2lf_13,I think this point should be made in the paper .,request
SyU_UK2lf_14,"Also , I think it 's worth noting that an RNN decoder could be used in a non-auto-regressive architecture as well .",fact
SyU_UK2lf_15,"That is , the sentence encoding could be mapped to a sequence of length 30 as is done with the CNN decoder currently ;",fact
SyU_UK2lf_16,"then a ( multi-layer ) BiLSTM could be run over that sequence ,",fact
SyU_UK2lf_17,and then a softmax classifier can be attached to each hidden vector to predict the word at that position .,fact
SyU_UK2lf_18,It would be interesting to compare that BiLSTM decoder with the proposed CNN decoder and also to compare it to a skip-thought-style auto-regressive RNN decoder .,request
SyU_UK2lf_19,This would let us understand whether the benefit is coming more from the non-auto-regressive nature of the decoder or from the CNN vs RNN differences .,evaluation
SyU_UK2lf_20,"That is , it would make sense to factor the decision of decoder design along multiple axes .",evaluation
SyU_UK2lf_21,One axis could be auto-regressive vs predict-all-words .,request
SyU_UK2lf_22,Another axis could be using a CNN over the sequence of word positions or an RNN over the sequence of word positions .,request
SyU_UK2lf_23,"For auto-regressive models , another axis could be train using previous ground-truth word vs train using previous predicted word .",request
SyU_UK2lf_24,Skip-thought corresponds to an auto-regressive RNN ( using the previous ground-truth word IIRC ) .,request
SyU_UK2lf_25,The proposed decoder is a predict-all-words CNN .,request
SyU_UK2lf_26,It would be natural to also experiment with an auto-regressive CNN and a predict-all-words RNN ( like what I described in the paragraph above ) .,request
SyU_UK2lf_27,"The paper is choosing a single point in the space and referring to it as a "" CNN decoder """,fact
SyU_UK2lf_28,whereas there are many possible architectures that can be described this way,evaluation
SyU_UK2lf_29,and I think it would strengthen the paper to increase the precision in discussing the architecture and possible alternatives .,request
SyU_UK2lf_30,"Overall , I think the architectural choices and results are strong enough to merit publication .",evaluation
SyU_UK2lf_31,Adding any of the above empirical comparisons would further strengthen the paper .,request
SyU_UK2lf_32,"However , I did have quibbles with some of the exposition and some of the claims made throughout the paper .",evaluation
SyU_UK2lf_33,"They are detailed below : Sec. 2 : In the "" Decoder "" paragraph : please add more details about how the words are predicted .",request
SyU_UK2lf_34,Are there final softmax layers that provide distributions over output words ?,request
SyU_UK2lf_35,I could n't find this detail in the paper .,fact
SyU_UK2lf_36,What loss is minimized during training ?,request
SyU_UK2lf_37,Is it the sum of log losses over all words being predicted ?,request
SyU_UK2lf_38,Sec. 3 : Section 3 does not add much to the paper .,evaluation
SyU_UK2lf_39,The motivations there are mostly suggestive rather than evidence-based .,evaluation
SyU_UK2lf_40,Section 3 could be condensed by about 80 % or so without losing much information .,evaluation
SyU_UK2lf_41,"Overall , the paper has more than 10 pages of content ,",fact
SyU_UK2lf_42,and the use of 2 extra pages beyond the desired submission length of 8 should be better justified .,request
SyU_UK2lf_43,I would recommend adding a few more details to Section 2 and removing most of Section 3 .,request
SyU_UK2lf_44,I 'll mention below some problematic passages in Section 3 that should be removed .,non-arg
SyU_UK2lf_45,"Sec. 3.2 : "" ... this same constraint ( if using RNN as the decoder ) could be an inappropriate constraint in the decoding process . """,quote
SyU_UK2lf_46,What is the justification or evidence for this claim ?,fact
SyU_UK2lf_47,I think the claim should be supported by an argument or some evidence or else should be removed .,request
SyU_UK2lf_48,"If the authors intend the subsequent paragraphs to justify the claim , then see my next comments .",non-arg
SyU_UK2lf_49,"Sec. 3.2 : "" The existence of the ground-truth current word embedding potentially decreases the tendency for the decoder to exploit other information from the sentence representation . """,quote
SyU_UK2lf_50,But this is not necessarily an inherent limitation of RNN decoders,evaluation
SyU_UK2lf_51,since it could be addressed by using the embedding of the previously-predicted word rather than the ground-truth word .,fact
SyU_UK2lf_52,"This is a standard technique in sequence-to-sequence learning ; cf. scheduled sampling ( Bengio et al. , 2015 ) .",evaluation
SyU_UK2lf_53,"Sec. 3.2 : "" Although the word order information is implicitly encoded in the CNN decoder , it is not emphasized as it is explicitly in the RNN decoder . The CNN decoder cares about the quality of generated sequences globally instead of the quality of the next generated word . Relaxing the emphasis on the next word , may help the CNN decoder model to explore the contribution of context in a larger space . """,quote
SyU_UK2lf_54,"Again , I do n't see any evidence or justification for these arguments .",fact
SyU_UK2lf_55,Also see my discussion above about decoder variations ;,non-arg
SyU_UK2lf_56,these are not properties of RNNs vs CNNs but rather properties of auto-regressive vs predict-all-words decoders .,fact
SyU_UK2lf_57,"Sec. 5.2-5.3 : There are a few high-level decisions being tuned on the test sets for some of the tasks , e.g. , the length of target sequences in Section 5.2 and the number of layers and channel size in Section 5.3 .",fact
SyU_UK2lf_58,"Sec. 5.4 : When trying to explain why an RNN encoder works better than a CNN encoder , the paper includes the following :",fact
SyU_UK2lf_59," We stated above that , in our belief , explicit usage of the word order information will augment the transferability of the encoder , and constrain the search space of the parameters in the encoder . The results match our belief . ",quote
SyU_UK2lf_60,I do n't think these beliefs are concrete enough to be upheld or contradicted .,evaluation
SyU_UK2lf_61,Both encoders explicitly use word order information .,fact
SyU_UK2lf_62,Can you provide some formal or theoretical statement about how the two encoders treat word order differently ?,request
SyU_UK2lf_63,"I fear that it 's only impressions and suppositions that lead to this difference , rather than necessarily something formal .",evaluation
SyU_UK2lf_64,"Sec. 5.4 : In Table 1 , it is unclear why the "" future predictor "" model is the one selected to be reported from Gan et al ( 2017 ) .",evaluation
SyU_UK2lf_65,"Gan et al has many settings and the "" future predictor "" setting is the worst .",fact
SyU_UK2lf_66,An explanation is needed for this choice .,request
SyU_UK2lf_67,"Sec. 6.1 : In the "" BYTE m-LSTM "" paragraph : "" Our large RNN-CNN model trained on Amazon Book Review ( the largest subset of Amazon Review ) performs on par with BYTE m-LSTM model , and ours works better than theirs on semantic relatedness and entailment tasks . """,quote
SyU_UK2lf_68,"I 'm not sure this "" on par "" assessment is warranted by the results in Table 2 .",evaluation
SyU_UK2lf_69,BYTE m-LSTM is better on MR by 1.6 points and better on CR by 4.7 points .,fact
SyU_UK2lf_70,The authors ' method is better on SUBJ by 0.7 and better on MPQA by 0.5 .,fact
SyU_UK2lf_71,"So on sentiment tasks , BYTE m-LSTM is clearly better ,",fact
SyU_UK2lf_72,"and on the other tasks the RNN-CNN is typically better , especially on SICK-r .",fact
SyU_UK2lf_73,"More minor things are below : Sec. 1 : The paper contains this : "" The idea of learning from the context information was first successfully applied to vector representation learning for words in Mikolov et al. ( 2013b ) """,fact
SyU_UK2lf_74,I do n't think this is accurate .,evaluation
SyU_UK2lf_75,"When restricting attention to neural network methods , it would be more correct to give credit to Collobert et al. ( 2011 ) .",request
SyU_UK2lf_76,"But moving beyond neural methods , there were decades of previous work in using context information ( counts of context words ) to produce vector representations of words .",fact
SyU_UK2lf_77,"typo : "" which d reduces "" -- > "" which reduces """,request
SyU_UK2lf_78,Sec. 2 : The notation in the text does n't match that in Figure 1 : w_i ^ 1 vs. w_1 and h_i ^ 1 vs h_1 .,fact
SyU_UK2lf_79,"Instead of writing "" non-parametric composition function "" , describe it as "" parameter-free "" .",request
SyU_UK2lf_80," Non-parametric  means that the number of parameters grows with the data , not that there are no parameters .",fact
SyU_UK2lf_81,"In the "" Representation "" paragraph : how do you compute a max over vectors ?",request
SyU_UK2lf_82,Is it a separate max for each dimension ?,request
SyU_UK2lf_83,This is not clear from the notation used .,evaluation
SyU_UK2lf_84,"Sec. 3.1 : inappropriate word choice : the use of "" great "" in "" a great and efficient encoding model """,evaluation
SyU_UK2lf_85,"Sec. 3.2 : inappropriate word choice : the use of "" unveiled "" in "" is still to be unveiled """,evaluation
SyU_UK2lf_86,Sec. 3.4 : Tying input and output embeddings can be justified with a single sentence and the relevant citations ( which are present here ) .,evaluation
SyU_UK2lf_87,"There is no need for speculation about what may be going on , e.g. : "" the model learns to explore the non-linear compositionality of the input words and the uncertain contribution of the target words in the same space "" .",evaluation
SyU_UK2lf_88,Sec. 4 : I think STS14 should be defined and cited where the other tasks are described .,request
SyU_UK2lf_89,"Sec. 5.3 : typo in Figure 2 caption : "" and and """,fact
SyU_UK2lf_90,"Sec. 6.1 : In the "" Skip-thought "" paragraph : inappropriate word choice : "" kindly """,evaluation
SyU_UK2lf_91,"The description that says "" we cut off a branch for decoding "" is not clear to me .",evaluation
SyU_UK2lf_92,"What is a "" branch for decoding "" in this context ?",request
SyU_UK2lf_93,Please modify it to make it more clear .,request
SyU_UK2lf_94,"References : Bengio S , Vinyals O , Jaitly N , Shazeer N. Scheduled sampling for sequence prediction with recurrent neural networks . NIPS 2015 .",reference
SyU_UK2lf_95,"Collobert R , Weston J , Bottou L , Karlen M , Kavukcuoglu K , Kuksa P. Natural language processing ( almost ) from scratch . Journal of Machine Learning Research 2011 .",reference
BJBU32dgz_0,The paper ‘ Deep learning for Physical Process : incorporating prior physical knowledge ’ proposes to question the use of data-intensive strategies such as deep learning in solving physical inverse problems that are traditionally solved through assimilation strategies .,fact
BJBU32dgz_1,They notably show how physical priors on a given phenomenon can be incorporated in the learning process and propose an application on the problem of estimating sea surface temperature directly from a given collection of satellite images .,fact
BJBU32dgz_2,All in all the paper is very clear and interesting .,evaluation
BJBU32dgz_3,"The results obtained on the considered problem are clearly of great interest , especially when compared to state-of-the-art assimilation strategies such as the one of Béréziat .",evaluation
BJBU32dgz_4,"While the learning architecture is not original in itself ,",evaluation
BJBU32dgz_5,it is shown that a proper physical regularization greatly improves the performance .,evaluation
BJBU32dgz_6,For these reasons I believe the paper has sufficient merits to be published at ICLR .,evaluation
BJBU32dgz_7,"That being said , I believe that some discussions could strengthen the paper :",evaluation
BJBU32dgz_8,"- Most classical variational assimilation schemes are stochastic in nature , notably by incorporating uncertainties in the observation or physical evolution models .",fact
BJBU32dgz_9,It is still unclear how those uncertainties can be integrated in the model ;,evaluation
BJBU32dgz_10,- Assimilation methods are usually independent of the type of data at hand .,fact
BJBU32dgz_11,It is not clear how the model learnt on one particular type of data transpose to other data sequences .,evaluation
BJBU32dgz_12,"Notably , the question of transfer and generalization is of high relevance here .",evaluation
BJBU32dgz_13,"Does the learnt model performs well on other dataset ( for instance , acquired on a different region or at a distant time ) .",request
BJBU32dgz_14,I believe this type of issue has to be examinated for this type of approach to be widely use in inverse physical problems .,request
SJ3tICFlz_0,The authors combine an ensemble of DNNs as model for the dynamics with TRPO .,fact
SJ3tICFlz_1,The ensemble is used in two steps : First to collect imaginary roll-outs for TRPO and secondly to estimate convergence of the algorithm .,fact
SJ3tICFlz_2,The experiments indicate superior performance over the baselines .,evaluation
SJ3tICFlz_3,The paper is well-written,evaluation
SJ3tICFlz_4,and the experiments indicate good results .,evaluation
SJ3tICFlz_5,"However , idea of using ensembles in the context of ( model-based ) RL is not novel ,",evaluation
SJ3tICFlz_6,and it comes at the cost of time complexity .,fact
SJ3tICFlz_7,"Therefore , the method should utilize the advantage an ensemble provides to its full extent .",evaluation
SJ3tICFlz_8,"The main strength of an ensemble is to provide lower test error , but also some from of uncertainty estimate given by the spread of the predictions .",evaluation
SJ3tICFlz_9,"The authors mainly utilize the first , but to a lesser extent the second advantage ( the imaginary roll-outs will utilize the spread to generate possible outcomes ) .",evaluation
SJ3tICFlz_10,Ideally the exploration should also be guided by the uncertainty ( such as VIME ) .,request
SJ3tICFlz_11,"Related , what where the arguments in favor of an ensemble compared to Bayesian neural networks ( possibly even as simple as using MH-dropout ) ?",request
SJ3tICFlz_12,BNNs provide a stronger theoretical justification that the predictive uncertainty is meaningful .,evaluation
SJ3tICFlz_13,Can the authors comment on the time-complexity of the proposed methods compared to the baselines ?,request
SJ3tICFlz_14,In Fig. 2 the x-axis is the time step of the real data .,fact
SJ3tICFlz_15,But I assume it took a different amount of time for each method to reach step t.,evaluation
SJ3tICFlz_16,The same argument can be made for Fig. 4 .,fact
SJ3tICFlz_17,"It seems here that in snake the larger ensembles reach convergence the quickest ,",fact
SJ3tICFlz_18,but I expect this effect to be reversed when considering actual training time .,evaluation
SJ3tICFlz_19,In total I think this paper can provide a useful addition to the literature .,evaluation
SJ3tICFlz_20,"However , the proposed approach does not have strong novelty",evaluation
SJ3tICFlz_21,and I am not fully convinced if the additional burden on time complexity outweighs the improved performance .,evaluation
SJ3tICFlz_22,"Minor : In Sec. 2 : "" Both of these approaches assume a fixed dataset of samples which are collected before the algorithm starts operating . """,quote
SJ3tICFlz_23,"This is incorrect ,",fact
SJ3tICFlz_24,"while these methods consider the domain of fixed datasets ,",fact
SJ3tICFlz_25,the algorithms themselves are not limited to this context .,fact
SkxVw6t7z_0,This paper presents a new method for detecting hypernymy by extending the distributional inclusion hypothesis to low-rank embeddings .,fact
SkxVw6t7z_1,"The first half of the paper is written superbly , providing a sober account of the state-of-the-art in hypernymy detection via distributional semantics , and a clear , well-motivated explanation of the main algorithm ( DIVE ) .",evaluation
SkxVw6t7z_2,The reformulation of PMI is interesting ;,evaluation
SkxVw6t7z_3,the authors essentially replace <VAR> with the uniform distribution,fact
SkxVw6t7z_4,"( this should be stated explicitly in the paper , BTW ) .",request
SkxVw6t7z_5,They then augment SGNS to reflect this change by dynamically selecting the number of negatively-sampled contexts according to the target word ( w ) .,fact
SkxVw6t7z_6,This is a clever trick .,evaluation
SkxVw6t7z_7,I was also very happy to see that the authors evaluated on virtually every lexical inference dataset available .,evaluation
SkxVw6t7z_8,"However , the remainder of the paper describing the experiments and their results was extremely difficult to parse .",evaluation
SkxVw6t7z_9,"First of all , the line between the authors ' contribution and prior art is blurry ,",evaluation
SkxVw6t7z_10,because they seem to be introducing new metrics for measuring hypernymy as part of the experiments .,fact
SkxVw6t7z_11,"There are also too many moving parts : datasets , evaluation metrics , detection metrics , embedding methods , hyperparameters , etc .",evaluation
SkxVw6t7z_12,"These need to be organized , controlled for , and clearly explained .",request
SkxVw6t7z_13,"I have two major concerns regarding the experiments , beyond intelligibility .",evaluation
SkxVw6t7z_14,"First , the authors used a specific corpus ( with specific preprocessing ) to train their vectors , but compared their results to those reported in other papers , which are not based on the same data .",fact
SkxVw6t7z_15,This invalidates these comparisons .,evaluation
SkxVw6t7z_16,"Instead , the other methods should be replicated and rerun on exactly the same corpus .",request
SkxVw6t7z_17,"I would also recommend using a much larger corpus ,",request
SkxVw6t7z_18,since 50M tokens is considered quite small when training word embeddings .,fact
SkxVw6t7z_19,My second concern is that summation ( dS ) is doing all the heavy lifting .,evaluation
SkxVw6t7z_20,"In Table 2 , we can see that the difference is only 0.1 between dS and W * dS ( where W is also not trained using DIVE ) .",fact
SkxVw6t7z_21,"Since dS is basically a proxy for difference in word frequency ,",fact
SkxVw6t7z_22,could it be that the proposed method is just computing which word is more general ?,evaluation
SkxVw6t7z_23,This looks awfully familiar to Levy et al 's prototypical hypernym result .,evaluation
SkxVw6t7z_24,Miscellaneous Comments : - There 's a very recent paper on using vector norms for detecting hypernymy that might be worth contrasting with :,request
SkxVw6t7z_25,<URL>,reference
SkxVw6t7z_26,"- Micro-averaging these datasets is problematic ,",evaluation
SkxVw6t7z_27,"because some of the datasets based on WordNet are much larger than the hand-annotated ones , and will likely drown them out .",evaluation
SkxVw6t7z_28,"Because these datasets are so different ,",evaluation
SkxVw6t7z_29,I think it is critical to look at the details and not only at the averages .,request
SkxVw6t7z_30,- PMI filtering needs to be controlled/ablated in the experiments .,request
SkxVw6t7z_31,"- While explaining equation 6 , the authors say that the gradients for x and y are similar ;",fact
SkxVw6t7z_32,"this is not true ,",fact
SkxVw6t7z_33,"because k is a function of x ( or y ) , and if one appears more often in general ( not necessarily with c ) , then the gradients will be different as well .",fact
ryv9d98lf_0,"This should be the first work which introduces in the causal structure into the GAN , to solve the label dependency problem .",fact
ryv9d98lf_1,The idea is interesting and insightful .,evaluation
ryv9d98lf_2,The proposed method is theoretically analyzed and experimentally tested .,fact
ryv9d98lf_3,Two minor concerns are 1 ) what is the relationship between the anti-labeler and and discriminator ?,request
ryv9d98lf_4,2 ) how the tune related weight of the different objective functions .,request
Byk2NS9xf_0,This paper proposes a method to generate adversarial examples for text classification problems .,fact
Byk2NS9xf_1,They do this by iteratively replacing words in a sentence with words that are close in its embedding space and which cause a change in the predicted class of the text .,fact
Byk2NS9xf_2,"To preserve correct grammar , they only change words that do n't significantly change the probability of the sentence under a language model .",fact
Byk2NS9xf_3,The approach seems incremental and very similar to existing work such as Papernot et . al .,evaluation
Byk2NS9xf_4,"The paper also states in the discussion in section 5.1 that they generate adversarial examples in state-of-the-art models ,",fact
Byk2NS9xf_5,"however , they ignore some state of the art models entirely such as Miyato et . al .",fact
Byk2NS9xf_6,The experiments are solely missing comparisons to existing text adversarial generation approaches such as Papernot et . al and a comparison to adversarial training for text classification in Miyato et . al which might already mitigate this attack .,fact
Byk2NS9xf_7,"The experimental section also fails to describe what kind of language model is used ,",fact
Byk2NS9xf_8,( what kind of trigram LM is used ?,request
Byk2NS9xf_9,A traditional ( non-neural ) LM ?,request
Byk2NS9xf_10,Does it use backoff ? ) .,request
Byk2NS9xf_11,"Finally , algorithm 1 does not seem to enforce the semantic constraints in Eq . 4 despite it being mentioned in the text .",fact
Byk2NS9xf_12,This can be seen in section 4.5 where the algorithm is described as choosing words that were far in word vector space .,fact
Byk2NS9xf_13,The last sentence in section 6 is also unfounded .,fact
Byk2NS9xf_14,"Nicolas Papernot , Patrick McDaniel , Ian Goodfellow , Somesh Jha , Z.Berkay Celik , and Ananthram Swami Practical Black-Box Attacks against Machine Learning . Proceedings of the 2017 ACM Asia Conference on Computer and Communications Security",reference
Byk2NS9xf_15,"Takeru Miyato , Andrew M. Dai and Ian Goodfellow Adversarial Training Methods for Semi-Supervised Text Classification . International Conference on Learning Representation ( ICLR ) , 2017",reference
ryCv5QFgz_0,"Motivated via Talor approximation of the Residual network on a local minima , this paper proposed a warp operator that can replace a block of a consecutive number of residual layers .",fact
ryCv5QFgz_1,"While having the same number of parameters as the original residual network , the new operator has the property that the computation can be parallelized .",fact
ryCv5QFgz_2,"As demonstrated in the paper , this improves the training time with multi-GPU parallelization , while maintaining similar performance on CIFAR-10 and CIFAR-100 .",fact
ryCv5QFgz_3,One thing that is currently not very clear to me is about the rotational symmetry .,evaluation
ryCv5QFgz_4,"The paper mentioned rotated filters ,",fact
ryCv5QFgz_5,but continue to talk about the rotation in the sense of an orthogonal matrix applying to the weight matrix of a convolution layer .,fact
ryCv5QFgz_6,"The rotation of the filters ( as 2D images or images with depth ) seem to be quite different from "" rotating "" a general N-dim vectors in an abstract Euclidean space .",evaluation
ryCv5QFgz_7,It would be helpful to make the description here more explicit and clear .,request
HJ_BtypgM_0,"This paper offers a very promising approach to the processing of the type of sequences we find in dialogues , somewhat in between RNNs which have problem modeling memory , and memory networks whose explicit modeling of the memory is too rigid .",evaluation
HJ_BtypgM_1,"To achieve that , the starting point seems to be a strength GRU that has the ability to dynamically add memory banks to the original dialogue and question sentence representations , thanks to the use of imperative DNN programming .",fact
HJ_BtypgM_2,"The use of the reparametrization trick to enable global differentiability is reminiscent of an ICLR '17 paper "" Learning graphical state transitions "" .",evaluation
HJ_BtypgM_3,"Compared to the latter , the current paper seems to offer a more tractable architecture and optimization problem that does not require strong supervision and should be much faster to train .",evaluation
HJ_BtypgM_4,"Unfortunately , this is the best understanding I got from this paper ,",evaluation
HJ_BtypgM_5,as it seems to be in such a preliminary stage that the exact operations of the SGRU are not parsable .,evaluation
HJ_BtypgM_6,Maybe the authors have been taken off guard by the new review process where one can no longer improve the manuscript during this 2017 review,evaluation
HJ_BtypgM_7,( something that had enabled a few paper to pass the 2016 review ) .,fact
HJ_BtypgM_8,"After a nice introduction ,",evaluation
HJ_BtypgM_9,"everything seems to fall apart in section 4 , as if the authors did not have time to finish their write-up .",evaluation
HJ_BtypgM_10,"- N is both the number of sentences and number of word per sentence ,",fact
HJ_BtypgM_11,which does not make sense .,evaluation
HJ_BtypgM_12,- i iterates over both the sentences and the words .,fact
HJ_BtypgM_13,The critical SGRU algorithm is impossible to parse,evaluation
HJ_BtypgM_14,"- The hidden vector sigma , which is usually noted h in the GRU notation , is not even defined",fact
HJ_BtypgM_15,"- The critical reset gate operation in Eq . ( 6 ) is not even explained , and modified in a way I do not understand compared to standard GRU .",fact
HJ_BtypgM_16,- What is t ?,request
HJ_BtypgM_17,"From algorithm 1 in Appendix A , it seems to correspond to looping over both sentences and words .",evaluation
HJ_BtypgM_18,"- The most novel and critical operation of this SGRU , to process the entities of the memory bank , is not even explained .",evaluation
HJ_BtypgM_19,"All we get at the end of section 4.2 is "" After these steps are finished , all entities are passed through the strength modified GRU ( 4.1 ) to recompute question relevance . """,fact
HJ_BtypgM_20,The algorithm in Appendix A does not help much .,evaluation
HJ_BtypgM_21,"With PyTorch being so readable , I wish some source code had been made available .",request
HJ_BtypgM_22,Experiments reporting also contains unacceptable omissions and errors :,evaluation
HJ_BtypgM_23,"- The definition of ' failed task ' , essential for understanding , is not stated ( more than 5 % error )",fact
HJ_BtypgM_24,- Reported numbers of failed tasks are erroneous :,fact
HJ_BtypgM_25,it should be 1 for DMN + and 3 for MemN2N .,request
HJ_BtypgM_26,Page 3 : dynanet - > dynet,request
S1uz175xf_0,"The authors study the problem of distributed routing in a network , where the goal is to minimize the maximal load ( i.e. the load of the link with the highest utilization ) .",fact
S1uz175xf_1,The authors advocate to use multi-agent reinforcement learning .,fact
S1uz175xf_2,"The main idea put forward by the authors is that by designing artificial rewards ( to guide the agents ) , one can achieve faster exploration , in order to reduce convergence time .",fact
S1uz175xf_3,"While the authors put forward several interesting ideas ,",evaluation
S1uz175xf_4,"there are some shortcomings to the present version of the paper , including :",evaluation
S1uz175xf_5,- The design objective seems flawed from the networking point of view :,evaluation
S1uz175xf_6,while minimizing the maximal load of a link is certainly a good starting point ( to avoid instable queues ),evaluation
S1uz175xf_7,one typically wants to minimize delay ( or maximize flow throughput ) .,evaluation
S1uz175xf_8,"Indeed , it is possible to have a larger maximal load while reducing delay in many cases .",evaluation
S1uz175xf_9,"- Furthermore , the authors do not provide a baseline to which the outcome of the learning algorithms they propose :",fact
S1uz175xf_10,"for instance how does their approach compare to simple policies ( those are commonplace in networking ) such as MaxWeight , Backpressure and so on ?",request
S1uz175xf_11,- The authors argue that using multi-agent learning is more desirable than single agent ( i.e. with a single reward signal which is common to all agents ) .,fact
S1uz175xf_12,"However , is multi-agent guaranteed to converge in such a setting ?",request
S1uz175xf_13,"If some versions of the problem ( for some particular reward signal ) are not guaranteed to converge , it is difficult to understand whether "" convergence "" is slow due to an inefficient exploration , or simply because convergence can not occur in the first place .",evaluation
S1uz175xf_14,- The learning algorithms used are not clearly explained :,evaluation
S1uz175xf_15,"the authors simply state that they use "" ACCNet "" ( from some unpublished prior work ) ,",fact
S1uz175xf_16,"but to readers unfamiliar with this algorithm , it is difficult to judge the contents of the paper .",evaluation
S1uz175xf_17,"- In the numerical experiments , what is the "" convergence rate "" ?",request
S1uz175xf_18,is it the ratio between the mean reward of the learnt policy and that of the optimal ?,request
S1uz175xf_19,For how many time steps are the learning algorithm run before evaluating their outcome ?,request
S1uz175xf_20,"What are the meaning of the various input parameter of ACCnet ,",request
S1uz175xf_21,and is the performance sensitive to those parameters ?,request
HyJlQlQWf_0,The paper considers multi-task setting of machine learning .,fact
HyJlQlQWf_1,The first contribution of the paper is a novel PAC-Bayesian risk bound .,evaluation
HyJlQlQWf_2,This risk bound serves as an objective function for multi-task machine learning .,fact
HyJlQlQWf_3,"A second contribution is an algorithm , called LAP , for minimizing a simplified version of this objective function .",evaluation
HyJlQlQWf_4,LAP algorithm uses several training tasks to learn a prior distribution P over hypothesis space .,fact
HyJlQlQWf_5,This prior distribution P is then used to find a posterior distribution Q that minimizes the same objective function over the test task .,fact
HyJlQlQWf_6,The third contribution is an empirical evaluation of LAP over toy dataset of two clusters and over MNIST .,evaluation
HyJlQlQWf_7,"While the paper has the title of "" life-long learning "" ,",fact
HyJlQlQWf_8,"the authors admit that all experiments are in multi-task setting , where the training is done over all tasks simultaneously .",fact
HyJlQlQWf_9,"The novel risk bound and LAP algorithm can definitely be applied to life-long setting , where training tasks are available sequentially .",fact
HyJlQlQWf_10,"But since there is no empirical evaluation in this setting ,",fact
HyJlQlQWf_11,I suggest to adjust the title of the paper .,request
HyJlQlQWf_12,"The novel risk bound of the paper is an extension of the bound from [ Pentina & Lampert , ICML 2014 ] .",evaluation
HyJlQlQWf_13,The extension seems to be quite significant .,evaluation
HyJlQlQWf_14,"Unlike the bound of [ Pentina & Lampert , ICML 2014 ] , the new bound allows to re-use many different PAC-Bayesian complexity terms that were published previously .",fact
HyJlQlQWf_15,I liked risk bound and optimization sections of the paper .,evaluation
HyJlQlQWf_16,But I was less convinced by the empirical experiments .,evaluation
HyJlQlQWf_17,"Since the paper improves the risk bound of [ Pentina & Lampert , ICML 2014 ] ,",fact
HyJlQlQWf_18,I expected to see an empirical comparison of LAP and optimization algorithm from the latter paper .,evaluation
HyJlQlQWf_19,"To make such comparison fair , both optimization algorithms should use the same base algorithm , e.g. ridge regression , as in [ Pentina & Lampert , ICML 2014 ] .",request
HyJlQlQWf_20,Also I suggest to use the datasets from the latter paper .,request
HyJlQlQWf_21,"The experiment with multi-task learning over MNIST dataset looks interesting ,",evaluation
HyJlQlQWf_22,but it is still a toy experiment .,evaluation
HyJlQlQWf_23,"This experiment will be more convincing with more sophisticated datasets ( CIFAR-10 , ImageNet ) and architectures ( e.g. Inception-V4 , ResNet ) .",request
HyJlQlQWf_24,"Minor remarks : Section 6 , line 4 : "" Combing "" - > "" Combining """,request
HyJlQlQWf_25,"Page 14 , first equation : There should be "" = "" before the second expectation .",request
SkpfobogG_0,The authors tackle the problem of estimating risk in a survival analysis setting with competing risks .,fact
SkpfobogG_1,They propose directly optimizing the time-dependent discrimination index using a siamese survival network .,fact
SkpfobogG_2,Experiments on several real-world dataset reveal modest gains in comparison with the state of the art .,evaluation
SkpfobogG_3,- The authors should clearly highlight what is their main technical contribution .,request
SkpfobogG_4,"For example , Eqs . 1-6 appear to be background material",fact
SkpfobogG_5,"since the time-dependent discrimination index is taken from the literature , as the authors point out earlier .",fact
SkpfobogG_6,"However , this is unclear from the writing .",evaluation
SkpfobogG_7,- One of the main motivations of the authors is to propose a model that is specially design to avoid the nonidentifiability issue in an scenario with competing risks .,fact
SkpfobogG_8,"It is unclear why the authors solution is able to solve such an issue , specially given the modest reported gains in comparison with several competitive baselines .",evaluation
SkpfobogG_9,"In other words , the authors oversell their own work , specially in comparison with the state of the art .",evaluation
SkpfobogG_10,- The authors use off-the-shelf siamese networks for their settting,fact
SkpfobogG_11,and thus it is questionable there is any novelty there .,evaluation
SkpfobogG_12,"The application/setting may be novel ,",evaluation
SkpfobogG_13,but not the architecture of choice .,evaluation
SkpfobogG_14,"- From Eq . 4 to Eq . 5 , the authors argue that the denominator does not depend on the model parameters and can be ignored .",fact
SkpfobogG_15,"However , afterwards the objective does combine time-dependent discrimination indices of several competing risks , with different denominator values .",fact
SkpfobogG_16,This could be problematic if the risks are unbalanced .,evaluation
SkpfobogG_17,- The competitive gain of the authors method in comparison with other competing methods is minor .,evaluation
SkpfobogG_18,"- The authors introduce F ( t , D | x ) as cumulative incidence function ( CDF ) at the beginning of section 2 ,",fact
SkpfobogG_19,"however , afterwards they use R ^ m ( t , x ) , which they define as risk of the subject experiencing event m before t.",fact
SkpfobogG_20,Is the latter a proxy for the former ?,request
SkpfobogG_21,How are they related ?,request
rJFaZDqgz_0,This is a well-written paper with sound experiments .,evaluation
rJFaZDqgz_1,"However , the research outcome is not very surprising .",evaluation
rJFaZDqgz_2,- Only macro-average F-scores are reported .,fact
rJFaZDqgz_3,Please present micro-average scores as well .,request
rJFaZDqgz_4,- The detailed procedure of relation extraction should be described .,request
rJFaZDqgz_5,How do you use entity type information ?,request
rJFaZDqgz_6,"( Probably , you did not use entity types . )",evaluation
rJFaZDqgz_7,- Table 3 : The SotA score of EU-ATR target-disease ( i.e. 84.6 ) should be in bold face .,request
rJFaZDqgz_8,- Section 5.3 : Your system scorers in Table 3 are not consistent with Table 2 scores .,evaluation
rJFaZDqgz_9,"- Page 8 . "" Our approach outperforms ... """,quote
rJFaZDqgz_10,The improvement is clear only for SNPPhenA and EU-ADR durg-disease .,fact
rJFaZDqgz_11,Minor comments : - TreeLSTM -- > Tree-LSTM,request
rJFaZDqgz_12,- Page 7 . connexion -- > connection,request
rJFaZDqgz_13,- Page 8 . four EU-ADR subtasks -- > three ...,request
rJFaZDqgz_14,- I suggest to conduct transfer learning studies in the similar settings .,request
rJwXkeOgM_0,"The paper considers the problem of training neural networks in mixed precision ( MP ) , using both 16-bit floating point ( FP16 ) and 32-bit floating point ( FP32 ) .",fact
rJwXkeOgM_1,"The paper proposes three techniques for training networks in mixed precision : first , keep a master copy of network parameters in FP32 ; second , use loss scaling to ensure that gradients are representable using the limited range of FP16 ; third , compute dot products and reductions with FP32 accumulation .",fact
rJwXkeOgM_2,Using these techniques allows the authors to match the results of traditional FP32 training on a wide variety of tasks without modifying any training hyperparameters .,fact
rJwXkeOgM_3,"The authors show results on ImageNet classification ( with AlexNet , VGG , GoogLeNet , Inception-v1 , Inception-v3 , and ResNet-50 ) , VOC object detection ( with Faster R-CNN and Multibox SSD ) , speech recognition in English and Mandarin ( with CNN+GRU ) , English to French machine translation ( with multilayer LSTMs ) , language modeling on the 1 Billion Words dataset ( with a bigLSTM ) , and generative adversarial networks on CelebFaces ( with DCGAN ) .",fact
rJwXkeOgM_4,Pros : - Three simple techniques to use for mixed-precision training,evaluation
rJwXkeOgM_5,- Matches performance of traditional FP32 training without modifying any hyperparameters,fact
rJwXkeOgM_6,- Very extensive experiments on a wide variety of tasks,evaluation
rJwXkeOgM_7,Cons : - Experiments do not validate the necessity of FP32 accumulation,evaluation
rJwXkeOgM_8,- No comparison of training time speedup from mixed precision,fact
rJwXkeOgM_9,"With new hardware ( such as NVIDIA ’s Volta architecture ) providing large computational speedups for MP computation , I expect that MP training will become standard practice in deep learning in the near future .",evaluation
rJwXkeOgM_10,Naively porting FP32 training recipes can fail due to the reduced numeric range of FP16 arithmetic ;,fact
rJwXkeOgM_11,"however by adopting the techniques of this paper , practitioners will be able to migrate their existing FP32 training pipelines to MP without modifying any hyperparameters .",fact
rJwXkeOgM_12,I expect these techniques to be hugely impactful as more people begin migrating to new MP hardware .,evaluation
rJwXkeOgM_13,"The experiments in this paper are very exhaustive , covering nearly every major application of deep learning .",evaluation
rJwXkeOgM_14,Matching state-of-the-art results on so many tasks increases my confidence that I will be able to apply these techniques to my own tasks and architectures to achieve stable MP training .,evaluation
rJwXkeOgM_15,My first concern with the paper is that there are no experiments to demonstrate the necessity of FP32 accumulation .,fact
rJwXkeOgM_16,"With an FP32 master copy of the weights and loss scaling , can all arithmetic be performed solely in FP16 , or are there some tasks where training will still diverge ?",request
rJwXkeOgM_17,My second concern is that there is no comparison of training-time speedup using MP .,fact
rJwXkeOgM_18,The main reason that MP is interesting is because new hardware promises to accelerate it .,evaluation
rJwXkeOgM_19,"If people are willing to endure the extra engineering overhead of implementing the techniques from this paper , what kind of practical speedups can they expect to see from their workloads ?",request
rJwXkeOgM_20,NVIDIA ’s marketing material claims that the Tensor Cores in the V100 offer an 8x speedup over its general-purpose CUDA cores,fact
rJwXkeOgM_21,( <URL> ) .,reference
rJwXkeOgM_22,"Since in this paper some operations are performed in FP32 ( weight updates , batch normalization ) and other operations are bound by memory and not compute bandwidth ,",fact
rJwXkeOgM_23,what kinds of speedups do you see in practice when moving from FP32 to MP on V100 ?,request
rJwXkeOgM_24,My other concerns are minor .,evaluation
rJwXkeOgM_25,Mandarin speech recognition results are reported on “ our internal test set ” .,fact
rJwXkeOgM_26,"Is there any previously published work on this dataset , or any publicly available test set for this task ?",request
rJwXkeOgM_27,The notation around the Inception architectures should be clarified .,request
rJwXkeOgM_28,"According to [ 3 ] and [ 4 ] , “ Inception-v1 ” and “ GoogLeNet ” both refer to the architecture used in [ 1 ] .",fact
rJwXkeOgM_29,The architecture used in [ 2 ] is referred to as “ BN-Inception ” by [ 3 ] and “ Inception-v2 ” by [ 4 ] .,fact
rJwXkeOgM_30,"“ Inception-v3 ” is the architecture from [ 3 ] , which is not currently cited .",fact
rJwXkeOgM_31,"To improve clarity in Table 1 , I suggest renaming “ GoogLeNet ” to “ Inception-v1 ” , changing “ Inception-v1 ” to “ Inception-v2 ” , and adding explicit citations to all rows of the table .",request
rJwXkeOgM_32,In Section 4.3 the authors note that “ half-precision storage format may act as a regularizer during training ” .,fact
rJwXkeOgM_33,"Though the effect is most obvious from the speech recognition experiments in Section 4.3 ,",evaluation
rJwXkeOgM_34,MP also achieves slightly higher performance than baseline for all ImageNet models but Inception-v1 and for both object detection models ;,fact
rJwXkeOgM_35,these results add support to the idea of FP16 as a regularizer .,evaluation
rJwXkeOgM_36,"Minor typos : Section 3.3 , Paragraph 3 : “ either FP16 or FP16 math ” - > “ either FP16 or FP32 math ”",request
rJwXkeOgM_37,"Section 4.1 , Paragraph 4 : “ pre-ativation ” - > “ pre-activation ”",request
rJwXkeOgM_38,"Overall this is a strong paper ,",evaluation
rJwXkeOgM_39,and I believe that it will be impactful as MP hardware becomes more widely used .,evaluation
rJwXkeOgM_40,References [1] <CIT>,reference
rJwXkeOgM_41,[2] <CIT>,reference
rJwXkeOgM_42,[3] <CIT>,reference
rJwXkeOgM_43,[4] <CIT>,reference
B1chwjFlz_0,"The authors propose a scheme to generate questions based on some answer sentences , topics and question types .",fact
B1chwjFlz_1,Topics are extracted from questions using similar words in question-answer pairs .,fact
B1chwjFlz_2,It is similar to what we find in some Q&A systems ( like lexical answer types in Watson ) .,evaluation
B1chwjFlz_3,A sequence classifier is also used to tag the presence of topic words .,fact
B1chwjFlz_4,Question types correspond mostly to salient questions words .,fact
B1chwjFlz_5,LSTMs are used to encode the various inputs and generate the questions .,fact
B1chwjFlz_6,The paper is well written and easy to follow .,evaluation
B1chwjFlz_7,I would expect more explanations why sentence classification and labeling results presented in Table 2 are so low .,request
B1chwjFlz_8,Experimental results on question generation are convincing and clearly indicate that the approach is effective to generate relevant and well-structured short questions .,evaluation
B1chwjFlz_9,The main weakness of the paper is the selected set of question types that seems to be a fuzzy combination of answer types and question types ( for ex . yes/no ) .,evaluation
B1chwjFlz_10,Some questions type can be highly ambiguous ;,evaluation
B1chwjFlz_11,"for instance “ What ” might lead to a definition , a quantity , some named entities ...",evaluation
B1chwjFlz_12,Hence I suggest you revise your qt set .,request
B1chwjFlz_13,"I would also suggest , for your next experiments , that you try to generate questions leading to answers with list of values .",request
SycLo2FxG_0,Summary The paper presents an interesting view on the recently proposed MAML formulation of meta-learning ( Finn et al ) .,evaluation
SycLo2FxG_1,"The main contribution is a ) insight into the connection between the MAML procedure and MAP estimation in an equivalent linear hierarchical Bayes model with explicit priors ,",evaluation
SycLo2FxG_2,"b ) insight into the connection between MAML and MAP estimation in non-linear HB models with implicit priors ,",fact
SycLo2FxG_3,"c ) based on these insights , the paper proposes a variant of MALM using a Laplace approximation ( with additional approximations for the covariance matrix .",fact
SycLo2FxG_4,The paper finally provides an evaluation on the mini ImageNet problem without significantly improving on the MAML results on the same task .,fact
SycLo2FxG_5,Pro : - The topic is timely and of relevance to the ICLR community continuing a current trend in building meta-learning system for few-shot learning .,evaluation
SycLo2FxG_6,- Provides valuable insight into the MAML objective and its relation to probabilistic models,evaluation
SycLo2FxG_7,Con : - The paper is generally well-written,evaluation
SycLo2FxG_8,but I find ( as a non-meta-learner expert ) that certain fundamental aspects could have been explained better or in more detail ( see below for details ) .,evaluation
SycLo2FxG_9,- The toy example is quite difficult to interpret the first time around,evaluation
SycLo2FxG_10,and does not provide any empirical insight into the converge of the proposed method ( compared to e.g. MAML ),fact
SycLo2FxG_11,- I do not think the empirical results provide enough evidence that it is a useful/robust method .,evaluation
SycLo2FxG_12,"Especially it does not provide insight into which types of problems ( small/large , linear / non-linear ) the method is applicable to .",evaluation
SycLo2FxG_13,Detailed comments/questions : - The use of Laplace approximation is ( in the paper ) motivated from a probabilistic/Bayes and uncertainty point-of-view .,evaluation
SycLo2FxG_14,"It would , however , seem that the truncated iterations do not result in the approximation being very accurate during optimization",evaluation
SycLo2FxG_15,as the truncation does not result in the approximation being created at a mode .,fact
SycLo2FxG_16,"Could the authors perhaps comment on : a ) whether it is even meaningful to talk about the approximations as probabilistic distribution during the optimization ( given the psd approximation to the Hessian ) , or does it only make sense after convergence ?",request
SycLo2FxG_17,b ) the consequence of the approximation errors on the general convergence of the proposed method ( consistency and rate ),request
SycLo2FxG_18,"- Sec 4.1 , p5 : Last equation : Perhaps useful to explain the term <VAR> and why it is not in subroutine 4 .",request
SycLo2FxG_19,Should <VAR> be <VAR> ?,request
SycLo2FxG_20,- Sec 4.2 : “ A straightforward … ” : I think it would improve readability to refer back to the to the previous equation ( i.e. H ) such that it is clear what is meant by “ straightforward ” .,request
SycLo2FxG_21,- Sec 4.2 : Several ideas are being discussed in Sec 4.2,fact
SycLo2FxG_22,and it is not entirely clear to me what has actually been adopted here ;,evaluation
SycLo2FxG_23,perhaps consider formalizing the actual computations in Subroutine 4 – and provide a clearer argument ( preferably proof ) that this leads to consistent and robust estimator of \ theta .,request
SycLo2FxG_24,- It is not clear from the text or experiment how the learning parameters are set .,evaluation
SycLo2FxG_25,- Sec 5.1 : It took some effort to understand exactly what was going on in the example and particular figure 5.1 ;,evaluation
SycLo2FxG_26,"e.g. , in the model definition in the body text there is no mention of the NN mentioned/used in figure 5 ,",fact
SycLo2FxG_27,"the blue points are not defined in the caption ,",fact
SycLo2FxG_28,the terminology e.g. “ pre-update density ” is new at this point .,fact
SycLo2FxG_29,I think it would benefit the readability to provide the reader with a bit more guidance .,request
SycLo2FxG_30,"- Sec 5.1 : While the qualitative example is useful ( with a bit more text ) ,",evaluation
SycLo2FxG_31,I believe it would have been more convincing with a quantitative example to demonstrate e.g. the convergence of the proposal compared to std MAML and possibly compare to a std Bayesian inference method from the HB formulation of the problem ( in the linear case ),request
SycLo2FxG_32,- Sec 5.2 : The abstract clams increased performance over MAML,fact
SycLo2FxG_33,but the empirical results do not seem to be significantly better than MAML ?,evaluation
SycLo2FxG_34,I find it quite difficult to support the specific claim in the abstract from the results without adding a comment about the significance .,evaluation
SycLo2FxG_35,- Sec 5.2 : The authors have left out “ Mishral et al ” from the comparison due to the model being significantly larger than others .,fact
SycLo2FxG_36,Could the authors provide insight into why they did not use the ResNet structure from the tcml paper in their L-MLMA scheme ?,request
SycLo2FxG_37,- Sec 6 +7 : The paper clearly states that it is not the aim to ( generally ) formulate the MAML as a HB .,fact
SycLo2FxG_38,"Given the advancement in gradient based inference for HB the last couple of years ( e.g. variational , nested laplace , expectation propagation etc ) for explicit models ,",fact
SycLo2FxG_39,could the authors perhaps indicate why they believe their approach of looking directly to the MAML objective is more scalable/useful than trying to formulate the same or similar objective in an explicit HB model and using established inference methods from that area ?,request
SycLo2FxG_40,Minor : - Sec 4.1 “ … each integral in the sum in ( 2 ) … ” eq 2 is a product,request
BJfU-qDeG_0,The authors provide an algorithm-agnostic active learning algorithm for multi-class classification .,fact
BJfU-qDeG_1,The core technique is to construct a coreset of points whose labels inform the labels of other points .,evaluation
BJfU-qDeG_2,The coreset construction requires one to construct a set of points which can cover the entire dataset .,fact
BJfU-qDeG_3,"While this is NP-hard problem in general ,",fact
BJfU-qDeG_4,the greedy algorithm is 2-approximate .,fact
BJfU-qDeG_5,The authors use a variant of the greedy algorithm along with bisection search to solve a series of feasibility problems to obtain a good cover of the dataset each time .,fact
BJfU-qDeG_6,This cover tells us which points are to be queried .,fact
BJfU-qDeG_7,The reason why choosing the cover is a good idea is,evaluation
BJfU-qDeG_8,because under suitable Lipschitz continuity assumption the generalization error can be controlled via an appropriate value of the covering radius in the data space .,fact
BJfU-qDeG_9,The authors use the coreset construction with a CNN to demonstrate an active learning algorithm for multi-class classification .,fact
BJfU-qDeG_10,The experimental results are convincing enough to show that it outperforms other active learning algorithms .,evaluation
BJfU-qDeG_11,"However , I have a few major and minor comments .",evaluation
BJfU-qDeG_12,Major comments : 1 . The proof of Lemma 1 is incomplete .,evaluation
BJfU-qDeG_13,We need the Lipschitz constant of the loss function .,evaluation
BJfU-qDeG_14,The loss function is a function of the CNN function and the true label .,fact
BJfU-qDeG_15,The proof of lemma 1 only establishes the Lipschitz constant of the CNN function .,fact
BJfU-qDeG_16,Some more extra work is needed to derive the lipschitz constant of the loss function from the CNN function .,request
BJfU-qDeG_17,2 . The statement of Prop 1 seems a bit confusing to me .,evaluation
BJfU-qDeG_18,the hypothsis says that the loss on the coreset = 0 .,fact
BJfU-qDeG_19,But the equation in proposition 1 also includes the loss on the coreset .,fact
BJfU-qDeG_20,Why is this term included .,request
BJfU-qDeG_21,Is this term not equal to 0 ?,request
BJfU-qDeG_22,3 . Some important works are missing .,evaluation
BJfU-qDeG_23,"Especially works related to pool based active learning , and landmark results on labell complexity of agnostic active learning .",evaluation
BJfU-qDeG_24,UPAL : Unbiased Pool based active learning by Ganti & Gray . <URL>,reference
BJfU-qDeG_25,Efficient active learning of half-spaces by Gonen et al. <URL>,reference
BJfU-qDeG_26,A bound on the label complexity of agnostic active learning . <URL>,reference
BJfU-qDeG_27,4 . The authors use L_2 loss as their objective function .,fact
BJfU-qDeG_28,This is a bit of a weird choice,evaluation
BJfU-qDeG_29,"given that they are dealing with multi-class classification and the output layer is a sigmoid layer , making it a natural fit to work with something like a cross-entropy loss function .",fact
BJfU-qDeG_30,"I guess the theoretical results do not extend to cross-entropy loss ,",evaluation
BJfU-qDeG_31,but the authors do not mention these points anywhere in the paper .,fact
BJfU-qDeG_32,"For example , the ladder network , which is one of the networks used by the authors is a network that uses cross-entropy for training .",fact
BJfU-qDeG_33,Minor-comment : 1 . The feasibility program in ( 6 ) is an MILP .,fact
BJfU-qDeG_34,"However , the way it is written it does not look like an MILP .",evaluation
BJfU-qDeG_35,It would have been great had the authors mentioned that <EQN> .,request
BJfU-qDeG_36,"2 . The authors write on page 4 , "" Moreover , zero training error can be enforced by converting average loss into maximal loss "" .",quote
BJfU-qDeG_37,It is not clear to me what the authors mean here .,evaluation
BJfU-qDeG_38,"For example , can I replace the average error in proposition 1 , by maximal loss ?",request
BJfU-qDeG_39,Why can I do that ?,request
BJfU-qDeG_40,Why would that result in zero training error ?,request
BJfU-qDeG_41,On the whole this is interesting work and the results are very nice .,evaluation
BJfU-qDeG_42,"But , the proof for Lemma 1 seems incomplete to me ,",evaluation
BJfU-qDeG_43,and some choices ( such as choice of loss function ) are unjustified .,evaluation
BJfU-qDeG_44,"Also , important references in active learning literature are missing .",evaluation
HkxWKlkgM_0,"Although GAN recently has attracted so many attentions ,",evaluation
HkxWKlkgM_1,the theory of GAN is very poor .,evaluation
HkxWKlkgM_2,This paper tried to make a new insight of GAN from theories,evaluation
HkxWKlkgM_3,and I think their approach is a good first step to build theories for GAN .,evaluation
HkxWKlkgM_4,"However , I believe this paper is not enough to be accepted .",evaluation
HkxWKlkgM_5,The main reason is that the main theorem ( Theorem 4.1 ) is too restrictive .,evaluation
HkxWKlkgM_6,1 . There is no theoretical result for failed conditions .,fact
HkxWKlkgM_7,"2 . To obtain the theorem , they assume the optimal discriminator .",fact
HkxWKlkgM_8,"However , most of failed scenarios come from the discriminator dynamics as in Figure 2 .",fact
HkxWKlkgM_9,3 . The authors could make more interesting results using the current ingredients .,request
HkxWKlkgM_10,"For instance , I would like to check the conditions on eta and T to guarantee <EQN> when <EQN> and <EQN> .",request
HkxWKlkgM_11,"In Theorem 4.1 , the authors use the same delta for delta_1 , delta_2 , delta_3 .",fact
HkxWKlkgM_12,"So , it is not clear which initial condition or target performance makes the eta and T.",evaluation
Hk8Nwx9xf_0,"Strengths : * Very simple approach , amounting to coupled training of "" e "" identical copies of a chosen net architecture , whose predictions are fused during training .",evaluation
Hk8Nwx9xf_1,This forces the different model instances to become more complementary .,fact
Hk8Nwx9xf_2,"* Perhaps counterintuitively , experiments also show that coupled ensembling leads to individual nets that perform better than those produced by separate training .",fact
Hk8Nwx9xf_3,"* The practical advantages of the proposed approach are twofold : 1 . Given a fixed parameter budget , coupled ensembling leads to better accuracy than a single net or an ensemble of disjointly-trained nets .",fact
Hk8Nwx9xf_4,"2 . For the same accuracy , coupled ensembling yields significant parameter savings .",evaluation
Hk8Nwx9xf_5,"Weaknesses : * Although results are very strong ,",evaluation
Hk8Nwx9xf_6,"the proposed models do not outperform the state-of-the-art , except for the models reported in Table 4 ,",evaluation
Hk8Nwx9xf_7,which however were obtained by * traditional * ensembling of coupled ensembles .,fact
Hk8Nwx9xf_8,* Coupled ensembling requires joint training of all nets in the ensemble,fact
Hk8Nwx9xf_9,and thus is limited by the size of the model that can be fit in memory .,fact
Hk8Nwx9xf_10,"Conversely , traditional ensembling involves separate training of the different instances",fact
Hk8Nwx9xf_11,and this enables the learning of an arbitrary number of individual nets .,fact
Hk8Nwx9xf_12,"* I am surprised by the results in Table 2 ,",evaluation
Hk8Nwx9xf_13,which suggest that the optimal number of nets in the ensemble is remarkably low ( only 3 ! ) .,fact
Hk8Nwx9xf_14,It 'd be valuable to understand whether this kind of result holds for other network architectures or whether it is specific to this choice of net .,request
Hk8Nwx9xf_15,"* Strictly speaking it is correct to refer to the individual nets in the ensembles as "" branches "" and "" basic blocks . """,fact
Hk8Nwx9xf_16,"Nevertheless , I find the use of these terms confusing in the context of the proposed approach ,",evaluation
Hk8Nwx9xf_17,since they are commonly used to denote concepts different from those represented here .,evaluation
Hk8Nwx9xf_18,I would recommend refraining from using these terms here .,request
Hk8Nwx9xf_19,"Overall , the paper provides limited technical novelty .",evaluation
Hk8Nwx9xf_20,"Yet , it reveals some interesting empirical findings about the benefits of coordinated training of models in an ensemble .",evaluation
rJ96Jgclf_0,I quite liked the revival of the dual memory system ideas and the cognitive ( neuro ) science inspiration .,evaluation
rJ96Jgclf_1,"The paper is overall well written and tackles serious modern datasets , which was impressive ,",evaluation
rJ96Jgclf_2,"even though it relies on a pre-trained , fixed ResNet ( see point below ) .",fact
rJ96Jgclf_3,My only complaint is that I felt I could n’t understand why the model worked so well .,evaluation
rJ96Jgclf_4,A better motivation for some of the modelling decisions would be helpful .,request
rJ96Jgclf_5,"For instance , how much the existence ( and training ) of a BLA network really help",request
rJ96Jgclf_6,"— which is a central new part of the paper , and was n’t in my view well motivated .",evaluation
rJ96Jgclf_7,"It would be nice to compare with a simpler baseline , such as a HC classifier network with reject option .",request
rJ96Jgclf_8,I also do n’t really understand why the proposed pseudorehearsal works so well .,evaluation
rJ96Jgclf_9,"Some formal reasoning , even if approximate , would be appreciated .",request
rJ96Jgclf_10,"Some additional comments below : - Although the paper is in general well written ,",evaluation
rJ96Jgclf_11,it falls on the lengthy side,evaluation
rJ96Jgclf_12,and I found it difficult at first to understand the flow of the algorithm .,evaluation
rJ96Jgclf_13,I think it would be helpful to have a high-level pseudocode presentation of the main steps .,request
rJ96Jgclf_14,"- It was somewhat buried in the details that the model actually starts with a fixed , advanced feature pre-processing stage ( the ResNet , trained on a distinct dataset , as it should ) .",fact
rJ96Jgclf_15,"I ’m fine with that ,",evaluation
rJ96Jgclf_16,but this should be discussed .,request
rJ96Jgclf_17,Note that there is evidence that the neuronal responses in areas as early as V1 change as monkeys learn to solve discrimination tasks .,fact
rJ96Jgclf_18,It should be stressed that the model does not yet model end-to-end learning in the incremental setting .,request
rJ96Jgclf_19,"- p. 4 , Eq . 4 , is it really necessary to add a loss for the intermediate layers , and not only for the input layer ?",request
rJ96Jgclf_20,I think it would be clearer to define the \ mathcal { L } explictily somewhere .,request
rJ96Jgclf_21,"Also , should n’t the sum start at j = 0 ?",request
BkL0g3a1f_0,"Summary : The paper focuses on the characterization of the landscape of deep neural networks ; i.e. , when and why local minima are global , what are the conditions for saddle critical points , etc .",fact
BkL0g3a1f_1,The paper covers a somewhat wide range of deep nets ( from shallow with linear activation to deeper with non-linear activation ) ;,evaluation
BkL0g3a1f_2,it focuces only on feed forward neural networks .,fact
BkL0g3a1f_3,"As the authors state , this paper provides a unifying perspective to the subject",fact
BkL0g3a1f_4,"( it justifies the results of others through this unifying theory ,",evaluation
BkL0g3a1f_5,"but also provides new results ; e.g. , there are results that do not depend on assumptions on the target data matrix Y ) .",fact
BkL0g3a1f_6,"Originality : The paper provides similar results to previous work , while removing some of the assumptions made in previous work .",evaluation
BkL0g3a1f_7,"In that sense , the originality of the results is weak ,",evaluation
BkL0g3a1f_8,but definitely there is some novelty in the methodology used to get to these results .,evaluation
BkL0g3a1f_9,"Thus , I would say original .",evaluation
BkL0g3a1f_10,Importance : The paper deals with the important problem of when and why training algorithms might get to global/local/saddle critical points .,evaluation
BkL0g3a1f_11,"While there are no direct connections with generalization properties ,",fact
BkL0g3a1f_12,characterizing the landscape of neural networks is an important topic to make further steps into better understanding of deep learning .,evaluation
BkL0g3a1f_13,It will attract some attention at the conference .,evaluation
BkL0g3a1f_14,Clarity : The paper is well-written,evaluation
BkL0g3a1f_15,"- some parts need improvement ,",evaluation
BkL0g3a1f_16,but overall I 'm satisfied with the current version .,evaluation
BkL0g3a1f_17,"Comments : 1 . If problem ( 4 ) is not considered at all in this paper ( in its full generality that considers matrix completion and matrix sensing as special cases ) , then the authors could just start with the model in ( 5 ) .",request
BkL0g3a1f_18,2 . Remark 1 has a nice example,evaluation
BkL0g3a1f_19,- could this example be shown with Y not being the all-zeros vector ?,request
BkL0g3a1f_20,"3 . In section 5 , the authors make a connection with the work of Ge et al. 2016 .",fact
BkL0g3a1f_21,"They state that the problems in ( 10 ) - ( 11 ) constitute generalizations of the symmetric matrix completion case , considered in Ge et al. 2016 .",fact
BkL0g3a1f_22,"However , in that work , the main difficulty of proving global optimality comes from the randomness of the sampling mask operator ( which introduces the notion of incoherence and requires results in expectation ) .",fact
BkL0g3a1f_23,"It is not clear , and maybe it is an overstatement , that the results in section 5 generalize that work .",evaluation
BkL0g3a1f_24,"If that is the case , could the authors describe this a bit further ?",request
ByL47G5lM_0,The paper proposed a new regularization approach that simultaneously encourages the weight vectors ( W ) to be sparse and orthogonal to each other .,fact
ByL47G5lM_1,The argument is that the sparsity helps to eliminate the irrelevant feature vectors by making the corresponding weights zero .,fact
ByL47G5lM_2,Nearly orthogonal sparse vectors will have zeros at different indexes,fact
ByL47G5lM_3,"and hence , encourages the weight vectors to have small overlap in terms of indices of nonzero entries ( called support ) .",fact
ByL47G5lM_4,"Small overlap in support of weight vectors , aids interpretability",fact
ByL47G5lM_5,as each weight vector is associated with a unique subset of feature vectors .,fact
ByL47G5lM_6,"For example , in the topic model , small overlap encourages , each topic to have unique set of representation words .",fact
ByL47G5lM_7,The proposed approach used L1 regularizer for enforcing sparsity in W.,fact
ByL47G5lM_8,"For enforcing orthogonality between different weight vectors ( wi , wj ) , the log-determinant divergence ( LDD ) regularization term encourages the Gram Matrix G ( Gij = wiTwj ) to be close to an identity matrix I.",fact
ByL47G5lM_9,The authors applied and tested the performance of proposed approach on Neural Network and Sparse Coding ( SC ) machine learning models .,fact
ByL47G5lM_10,The authors validated the need for their proposed regularizer through experiments on 4 datasets ( 3 text and 1 images ) .,fact
ByL47G5lM_11,Major * The novelty of the paper is not clear .,evaluation
ByL47G5lM_12,Neither L1 no logdet ( ) are novel regularizers ( see the literature of Determinatal Point Process ) .,evaluation
ByL47G5lM_13,"With the presence of the auto-differentiator , one can not claim the making derivative a novelty .",evaluation
ByL47G5lM_14,* L1 is also encourages diversity although as explicit as logdet .,fact
ByL47G5lM_15,This is also obvious from Fig 2 .,evaluation
ByL47G5lM_16,Perhaps the advantage of diversity is in interpretability,evaluation
ByL47G5lM_17,but that is hard to quantify and the authors did not put enough effort to do that ;,evaluation
ByL47G5lM_18,we only have small anecdotal results in section 4.3 .,evaluation
ByL47G5lM_19,* The Table 1 is not convincing,evaluation
ByL47G5lM_20,"because one can argue , for example , gun ( vec 1 ) and weapon ( vec 4 ) are colinear .",fact
ByL47G5lM_21,"* In section 4.2 , the authors experimented with SC on text dataset .",fact
ByL47G5lM_22,The overlap score decreases as the strength of regularization increases .,fact
ByL47G5lM_23,The authors did n’t show the effect of increasing the regularization strength on the model accuracy and convergence time .,fact
ByL47G5lM_24,"This analysis is important to make sure , the decrease in overlap score is not coming at the expense of model accuracy and performance .",evaluation
ByL47G5lM_25,"* In section 4.4 , increase in test set accuracy and difference between test and train set accuracy is used to validate the claim , that the proposed regularizer helps reducing over fitting .",fact
ByL47G5lM_26,"In Table-2 , , the test accuracy increases between SC and LDD-L1 SC",fact
ByL47G5lM_27,while the train accuracy remains almost the same .,fact
ByL47G5lM_28,"Also , the authors did n’t do any cross validation to support their claim .",fact
ByL47G5lM_29,The difference is numbers is too small to support the claim .,evaluation
ByL47G5lM_30,"* In section on LSTM for Language Modeling , the perplexity score of LDD-L1 regularization on PytorchLM received perplexity score of 1.2 lower than without regularization .",fact
ByL47G5lM_31,"Although , the author mentions it as a significant reduction ,",fact
ByL47G5lM_32,the lowest perplexity score in Table 3 is significantly lower than this result .,evaluation
ByL47G5lM_33,It ’s not clear how 1.2 reduction in perplexity is significant and why the method should be preferred while much better models already exists .,evaluation
ByL47G5lM_34,"* Results of the best perplexity model , Neural Architecture Search + WT V2 , with proposed regularization would also help , validating the generalizability claims of the new approach .",request
ByL47G5lM_35,"* In CNN for Image Classification section , details of increase interpretability of the model , in terms of classification decision , is missing .",fact
ByL47G5lM_36,"* In Table-4 , the proposed LDD-L1 WideResNet is not the best results .",fact
ByL47G5lM_37,"Results of adding the proposed regularization , to the best know method ( Pyramid Sep Drop ) would be interesting .",request
ByL47G5lM_38,* The proposed regularization claims to provide more interpretable representation and less overfit model .,fact
ByL47G5lM_39,The given experiments are inadequate to validate the claims .,evaluation
ByL47G5lM_40,* A more extensive experimentation is required to validate the applicability of the method .,request
ByL47G5lM_41,"* In SC , aj are the linear coefficients or the coefficient vector of the j-th sample .",fact
ByL47G5lM_42,If <EQN> then <EQN> and j ranges between <VAR> as in equation 6 .,fact
ByL47G5lM_43,"The notation in section 2.2 , Sparse Coding section is misleading",evaluation
ByL47G5lM_44,as j ranges between <VAR> .,fact
ByL47G5lM_45,"* In Related works , the authors mention previous work done on interpreting the results of the machine learning models .",fact
ByL47G5lM_46,Related works on enhancing interpretability and reducing overfitting by using regularization is missing .,fact
rJ54aZ9gG_0,This paper presents a very thorough empirical exploration of the qualities and limitations of very simple word-embedding based models .,fact
rJ54aZ9gG_1,"Average and/or max pooling over word embeddings ( which are initialized from pretrained embeddings ) is used to obtain a fixed-length representation for natural language sequences , which is then fed through a single layer MLP classifier .",fact
rJ54aZ9gG_2,"In many of the 9 evaluation tasks , this approach is found to match or outperform single-layer CNNs or RNNs .",fact
rJ54aZ9gG_3,"The varied findings are very clearly presented and helpfully summarized ,",evaluation
rJ54aZ9gG_4,and for each task setting the authors perform an insightful analysis .,evaluation
rJ54aZ9gG_5,"My only criticism would be the fact that the study is limited to English ,",evaluation
rJ54aZ9gG_6,even though the conclusions are explicitly scoped in light of this .,fact
rJ54aZ9gG_7,"Moreover , I wonder how well the findings would hold in a setting with a more severe OOV problem than is perhaps present in the studied datasets .",request
rJ54aZ9gG_8,"Besides concluding from the presented results that these SWEMs should be considered a strong baseline in future work ,",evaluation
rJ54aZ9gG_9,one might also conclude that we need more challenging datasets !,evaluation
rJ54aZ9gG_10,Minor things : - It was n't entirely clear how the text matching tasks are encoded .,evaluation
rJ54aZ9gG_11,"Are the two sequences combined into a single sequence before applying the model , or something else ?",request
rJ54aZ9gG_12,I might have missed this detail .,evaluation
rJ54aZ9gG_13,"- Given the two ways of using the Glove embeddings for initialization ( direct update vs mapping them with an MLP into the task space ) , it would be helpful to know which one ended up being used ( i.e. optimal ) in each setting .",request
rJ54aZ9gG_14,- Something went wrong with the font size for the remainder of the text near Figure 1 .,fact
SkxHS_vlz_0,"Summary of paper : The authors present a novel attack for generating adversarial examples , deemed OptMargin ,",evaluation
SkxHS_vlz_1,in which the authors attack an ensemble of classifiers created by classifying at random L2 small perturbations .,fact
SkxHS_vlz_2,"They compare this optimization method with two baselines in MNIST and CIFAR , and provide an analysis of the decision boundaries by their adversarial examples , the baselines and non-altered examples .",fact
SkxHS_vlz_3,Review summary : I think this paper is interesting .,evaluation
SkxHS_vlz_4,"The novelty of the attack is a bit dim ,",evaluation
SkxHS_vlz_5,since it seems it 's just the straightforward attack against the region cls defense .,fact
SkxHS_vlz_6,"The authors fail to include the most standard baseline attack , namely FSGM .",evaluation
SkxHS_vlz_7,"The authors also miss the most standard defense , training with adversarial examples .",evaluation
SkxHS_vlz_8,"As well , the considered attacks are in L2 norm ,",fact
SkxHS_vlz_9,"and the distortion is measured in L2 ,",fact
SkxHS_vlz_10,while the defenses measure distortion in L _ \ infty ( see detailed comments for the significance of this if considering white-box defenses ) .,fact
SkxHS_vlz_11,"The provided analysis is insightful ,",evaluation
SkxHS_vlz_12,though the authors mostly fail to explain how this analysis could provide further work with means to create new defenses or attacks .,evaluation
SkxHS_vlz_13,If the authors add FSGM to the batch of experiments ( especially section 4.1 ) and address some of the objections I will consider updating my score .,evaluation
SkxHS_vlz_14,A more detailed review follows .,non-arg
SkxHS_vlz_15,Detailed comments : - I think the novelty of the attack is not very strong .,evaluation
SkxHS_vlz_16,The authors essentially develop an attack targeted to the region cls defense .,fact
SkxHS_vlz_17,"Designing an attack for a specific defense is very well established in the literature ,",evaluation
SkxHS_vlz_18,and the fact that the attack fools this specific defense is not surprising .,evaluation
SkxHS_vlz_19,"- I think the authors should make a claim on whether their proposed attack works only for defenses that are agnostic to the attack ( such as PGD or region based ) , or for defenses that know this is a likely attack ( see the following comment as well ) .",request
SkxHS_vlz_20,"If the authors want to make the second claim , training the network with adversarial examples coming from OptMargin is missing .",evaluation
SkxHS_vlz_21,"- The attacks are all based in L2 ,",fact
SkxHS_vlz_22,"in the sense that the look for they measure perturbation in an L2 sense ( as the paper evaluation does ) ,",fact
SkxHS_vlz_23,while the defenses are all L _ \ infty based,fact
SkxHS_vlz_24,"( since the region classifier method samples from a hypercube , and PGD uses an L _ \ infty perturbation limit ) .",fact
SkxHS_vlz_25,This is very problematic if the authors want to make claims about their attack being effective under defenses that know OptMargin is a possible attack .,evaluation
SkxHS_vlz_26,- The simplest most standard baseline of all ( FSGM ) is missing .,evaluation
SkxHS_vlz_27,This is important to compare properly with previous work .,evaluation
SkxHS_vlz_28,- The fact that the attack OptMargin is based in L2 perturbations makes it very susceptible to a defense that backprops through the attack .,evaluation
SkxHS_vlz_29,This and / or the defense of training to adversarial examples is an important experiment to assessing the limitations of the attack .,evaluation
SkxHS_vlz_30,"- I think the authors rush to conclude that "" a small ball around a given input distance can be misleading "" .",evaluation
SkxHS_vlz_31,"Wether balls are in L2 or L _ \ infty , or another norm makes a big difference in defense and attacks ,",evaluation
SkxHS_vlz_32,"given that they are only equivalent to a multiplicative factor of sqrt ( d ) where d is the dimension of the space , and we are dealing with very high dimensional problems .",fact
SkxHS_vlz_33,I find the analysis made by the authors to be very simplistic .,evaluation
SkxHS_vlz_34,"- The analysis of section 4.1 is interesting , it was insightful and to the best of my knowledge novel .",evaluation
SkxHS_vlz_35,Again I would ask the authors to make these plots for FSGM .,request
SkxHS_vlz_36,"Since FSGM is known to be robust to small random perturbations ,",fact
SkxHS_vlz_37,"I would be surprised that for a majority of random directions , the adversarial examples are brought back to the original class .",evaluation
SkxHS_vlz_38,- I think a bit more analysis is needed in section 4.2 .,request
SkxHS_vlz_39,Do the authors think that this distinguishability can lead to a defense that uses these statistics ?,request
SkxHS_vlz_40,"If so , how ?",request
SkxHS_vlz_41,- I think the analysis of section 5 is fairly trivial .,evaluation
SkxHS_vlz_42,"Distinguishability in high dimensions is an easy problem ( as any GAN experiment confirms , see for example Arjovsky & Bottou , ICLR 2017 ) ,",evaluation
SkxHS_vlz_43,so it 's not surprising or particularly insightful that one can train a classifier to easily recognize the boundaries .,evaluation
SkxHS_vlz_44,- Will the authors release code to reproduce all their experiments and methods ?,non-arg
SkxHS_vlz_45,Minor comments : - The justification of why OptStrong is missing from Table2 ( last three sentences of 3.3 ),evaluation
SkxHS_vlz_46,"should be summarized in the caption of table 2 ( even just pointing to the text ) ,",request
SkxHS_vlz_47,otherwise a first reader will mistake this for the omission of a baseline .,evaluation
SkxHS_vlz_48,- I think it 's important to state in table 1 what is the amount of distortion noticeable by a human .,request
rJW-33tlG_0,"This paper present the application of the memory buffer concept to speech synthesis ,",fact
rJW-33tlG_1,"and additionally learns a "" speaker vector "" that makes the system adaptive and work reasonably well on "" in-the-wild "" speech data .",fact
rJW-33tlG_2,"This is a relevant problem , and a novel solution ,",evaluation
rJW-33tlG_3,"but synthesis is a wicked problem to evaluate ,",evaluation
rJW-33tlG_4,so I am not sure if ICLR is the best venue for this paper .,evaluation
rJW-33tlG_5,I see two competing goals :,fact
rJW-33tlG_6,"- If the focus is on showing that the presented approach outperforms other approaches under given conditions , a different task would be better ( for example recognition , or some sort of trajectory reconstruction )",request
rJW-33tlG_7,"- If the focus is on showing that the system outperforms other synthesis systems , then a speech oriented venue might be best",request
rJW-33tlG_8,( and it is unfortunate that optimized hyper-parameters for the other systems are not available for a fair comparsion ),evaluation
rJW-33tlG_9,"- If fair comparisons with the other appraoches can not be made , my sense is that the multi-speaker ( post-training fitting ) option is really the most interesting and novel contribution here ,",request
rJW-33tlG_10,which could be discussed in mroe detail,request
rJW-33tlG_11,"Still , the approach is creative and interesting and deserves to be presented .",evaluation
rJW-33tlG_12,"I have a few questions / suggestions : Introduction - The link to Baddeley 's "" phonological loop "" concept seems weak at best .",evaluation
rJW-33tlG_13,"There is nothing phonological about the features that this model stores and retrieves ,",fact
rJW-33tlG_14,"and no evidence that the model behaves in a way consistent with "" phonologcial "" ( or articulatory ) assumptions or models",fact
rJW-33tlG_15,- maybe best to avoid distracting the reader with this concept and strengthen the speaker adaptation aspect ?,request
rJW-33tlG_16,"- The memory model is not an RNN ,",fact
rJW-33tlG_17,"but it is a recurrently called structure ( as the name "" phonological loop "" also implies )",fact
rJW-33tlG_18,- so I would also not highlight this point much,request
rJW-33tlG_19,"- Why would the four properties of the proposed method ( mid of p. 2 , end of introduction : memory buffer , shared memory , shallow fully connected networks , and simple reader mechanism ) lead to better robustness and improve performance on noisy and limited training data ?",request
rJW-33tlG_20,Maybe the proposed approach works better for any speech synthesis task ?,request
rJW-33tlG_21,"Why specifically for "" in-the-wild "" data ?",request
rJW-33tlG_22,"The results in Table 2 show that the proposed system outperforms other systems on Blizzard 2013 , but not Blizzard 2011",fact
rJW-33tlG_23,- does this support the previous argument ?,fact
rJW-33tlG_24,- Why not also evaluate MCD scores ?,request
rJW-33tlG_25,This should be a quick and automatic way to diagnose what the system is doing ?,evaluation
rJW-33tlG_26,Or is this not meaningful with the noisy training data ?,request
rJW-33tlG_27,"Previous work - Please introduce abbreviations the first time they are used ( "" CBHG "" for example )",request
rJW-33tlG_28,"- There is other work on using "" in-the-wild "" speech as well :",fact
rJW-33tlG_29,"Pallavi Baljekar and Alan W Black . Utterance Selection Techniques for TTS Systems using Found Speech , SSW 2016 , Sunnyvale , USA Sept 2016",reference
rJW-33tlG_30,"The architecture - Please explain the "" GMM "" ( Gaussian Mixture Model ? ) attention mechanism in a bit more detail , how does back-propagation work in this case ?",request
rJW-33tlG_31,- Why was this approach chosen ?,request
rJW-33tlG_32,Does it promise to be robust or good for low data situations specifically ?,request
rJW-33tlG_33,"- The fonts in Figure 2 are very small ,",evaluation
rJW-33tlG_34,"please make them bigger ,",request
rJW-33tlG_35,and the Figure may not print well in b/w .,evaluation
rJW-33tlG_36,Why does the mean of the absolute weights go up for high buffer positions ?,evaluation
rJW-33tlG_37,"Is there some "" leaking "" from even longer contexts ?",evaluation
rJW-33tlG_38,"- I do n't understand "" However , human speech is not deterministic and one can not expect [ ... ] truth "" .",evaluation
rJW-33tlG_39,You are saying that the model can not be excepted to reproduce the input exactly ?,evaluation
rJW-33tlG_40,Or does this apply only to the temporal distribution of the sequence ( but not the spectral characteristics ) ?,fact
rJW-33tlG_41,The previous sentence implies that it does .,fact
rJW-33tlG_42,And how does teacher-forcing help in this case ?,request
rJW-33tlG_43,"- what type of speed is "" x5 "" ?",request
rJW-33tlG_44,Five times slower or faster than real-time ?,request
rJW-33tlG_45,"Experiments - Table 2 : maybe mention how these results were computed , i.e. which systems use optimized hyper parameters , and which do n't ?",request
rJW-33tlG_46,How do these results support the interpretation of hte results in the introruction re in-the-wild data and found data ?,request
rJW-33tlG_47,- I am not sure how to read Figure 4 .,evaluation
rJW-33tlG_48,"Maybe it would be easier to plot the different phone sequences against each other and show how the timings are off , i.e. plot the time of the center of panel one vs the time of the center of panel 2 for the corresponding phone , and show how this is different from a straight line .",evaluation
rJW-33tlG_49,Or maybe plot phones as rectangles that get deformed from square shape as durations get learned ?,evaluation
rJW-33tlG_50,- Figure 5 : maybe provide spectrograms and add pitch contours to better show the effect of the dfifferent intonations ?,request
rJW-33tlG_51,"- Figure 4 uses a lot of space , could be reduced , if needed",request
rJW-33tlG_52,Discussion - I think the first claim is a bit to broad,evaluation
rJW-33tlG_53,"- nowhere is it shown that the method is inherently more robust to clapping and laughs , and variable prosody .",fact
rJW-33tlG_54,"The authors will know the relevant data-sets better than I do ,",evaluation
rJW-33tlG_55,maybe they can simply extend the discussion to show that this is what happens .,request
rJW-33tlG_56,- Efficiency : I think Wavenet has also gotten much faster and runs in less than real-time now,evaluation
rJW-33tlG_57,"- can you expand that discussion a bit , or maybe give estimates in times of FLOPS required , rather than anecdotal evidence for systems that may or may not be comparable ?",request
rJW-33tlG_58,"Conclusion - Now the advantage of the proposed model is with the number of parameters , rather than the computation required .",fact
rJW-33tlG_59,Can you clarify ?,request
rJW-33tlG_60,Are your models smaller than competing models ?,request
SJk7H29xM_0,This paper addresses the question of unsupervised clustering with high classification performance .,fact
SJk7H29xM_1,They propose a deep variational autoencoder architecture with categorical latent variables at the deepest layer and propose to train it with modifications of the standard variational approach with reparameterization gradients .,fact
SJk7H29xM_2,The model is tested on a medical imagining dataset where the task is to distinguish healthy from pathological lymphocytes from blood samples .,fact
SJk7H29xM_3,"I am not an expert on this particular dataset ,",non-arg
SJk7H29xM_4,but to my eye the results look impressive .,evaluation
SJk7H29xM_5,They show high sensitivity and high specificity .,evaluation
SJk7H29xM_6,This paper may be an important contribution to the medical imaging community .,evaluation
SJk7H29xM_7,My primary concern with the paper is the lack of novelty and relatively little in the way of contributions to the ICLR community .,evaluation
SJk7H29xM_8,The proposed model is a simple variant on the standard VAE models,evaluation
SJk7H29xM_9,( see for example the Ladder VAE <URL> for deep models with multiple stochastic layers ) .,fact
SJk7H29xM_10,This would be OK if a thorough evaluation on at least two other datasets showed similar improvements as the lymphocytes dataset .,evaluation
SJk7H29xM_11,"As it stands , it is difficulty for me to assess the value of this model .",evaluation
SJk7H29xM_12,Minor questions / concerns : - The authors claim in the first paragraph of 3.2 that deterministic mappings lack expressiveness .,fact
SJk7H29xM_13,Would be great to see the paper take this claim seriously and investigate it .,request
SJk7H29xM_14,- In equation ( 13 ) it is n't clear whether you use q_phi to be the discrete mass or the concrete density .,evaluation
SJk7H29xM_15,The distinction is discussed in <URL>,fact
SJk7H29xM_16,- Would be nice to report the MCC in experimental results .,request
rkBmo9ryM_0,"In their paper "" CausalGAN : Learning Causal implicit Generative Models with adv. training "" the authors address the following issue : Given a causal structure between "" labels "" of an image ( e.g. gender , mustache , smiling , etc. ) , one tries to learn a causal model between these variables and the image itself from observational data .",fact
rkBmo9ryM_1,"Here , the image is considered to be an effect of all the labels .",fact
rkBmo9ryM_2,"Such a causal model allows us to not only sample from conditional observational distributions , but also from intervention distributions .",fact
rkBmo9ryM_3,"These tasks are clearly different ,",evaluation
rkBmo9ryM_4,"as nicely shown by the authors ' example of "" do ( mustache = 1 ) "" versus "" given mustache = 1 "" ( a sample from the latter distribution contains only men ) .",evaluation
rkBmo9ryM_5,The paper does not aim at learning causal structure from data,fact
rkBmo9ryM_6,( as clearly stated by the authors ) .,fact
rkBmo9ryM_7,The example images look convincing to me .,evaluation
rkBmo9ryM_8,I like the idea of this paper .,evaluation
rkBmo9ryM_9,"IMO , it is a very nice , clean , and useful approach of combining causality and the expressive power of neural networks .",evaluation
rkBmo9ryM_10,The paper has the potential of conveying the message of causality into the ICLR community and thereby trigger other ideas in that area .,evaluation
rkBmo9ryM_11,"For me , it is not easy to judge the novelty of the approach ,",evaluation
rkBmo9ryM_12,"but the authors list related works ,",fact
rkBmo9ryM_13,none of which seems to solve the same task .,fact
rkBmo9ryM_14,"The presentation of the paper , however , should be improved significantly before publication .",request
rkBmo9ryM_15,"( In fact , because of the presentation of the paper , I was hesitating whether I should suggest acceptance . )",evaluation
rkBmo9ryM_16,"Below , I give some examples ( and suggest improvements ) ,",non-arg
rkBmo9ryM_17,but there are many others .,evaluation
rkBmo9ryM_18,"There is a risk that in its current state the paper will not generate much impact ,",evaluation
rkBmo9ryM_19,and that would be a pity .,evaluation
rkBmo9ryM_20,I would therefore like to ask the authors to put a lot of effort into improving the presentation of the paper .,request
rkBmo9ryM_21,"- I believe that I understand the authors ' intention of the caption of Fig. 1 ,",evaluation
rkBmo9ryM_22,"but "" samples outside the dataset "" is a misleading formulation .",evaluation
rkBmo9ryM_23,Any reasonable model does more than just reproducing the data points .,evaluation
rkBmo9ryM_24,I find the argumentation the authors give in Figure 6 much sharper .,evaluation
rkBmo9ryM_25,"Even better : add the expression "" <EQN> "" .",request
rkBmo9ryM_26,"Then , the difference is crystal clear .",evaluation
rkBmo9ryM_27,"- The difference between Figures 1 , 4 , and 6 could be clarified .",evaluation
rkBmo9ryM_28,"- The list of "" prior work on learning causal graphs "" seems a bit random .",evaluation
rkBmo9ryM_29,"I would add Spirtes et al 2000 , Heckermann et al 1999 , Peters et al 2016 , and Chickering et al 2002 .",request
rkBmo9ryM_30,- Male - > Bald does not make much sense causally,evaluation
rkBmo9ryM_31,( it should be Gender - > Baldness ) ...,request
rkBmo9ryM_32,"Aha , now I understand :",non-arg
rkBmo9ryM_33,"The authors seem to switch between "" Gender "" and "" Male "" being random variables .",fact
rkBmo9ryM_34,"Make this consistent , please .",request
rkBmo9ryM_35,- There are many typos and comma mistakes .,evaluation
rkBmo9ryM_36,- I would introduce the do-notation much earlier .,request
rkBmo9ryM_37,The paragraph on p. 2 is now written without do-notation,fact
rkBmo9ryM_38,"( "" intervening Mustache = 1 would not change the distribution "" ) .",quote
rkBmo9ryM_39,"But this way , the statements are at least very confusing",evaluation
rkBmo9ryM_40,"( which one is "" the distribution "" ? ) .",evaluation
rkBmo9ryM_41,- I would get rid of the concept of CiGM .,request
rkBmo9ryM_42,"To me , it seems that this is a causal model with a neural network ( NN ) modeling the functions that appear in the SCM .",fact
rkBmo9ryM_43,"This means , it 's "" just "" using NNs as a model class .",fact
rkBmo9ryM_44,"Instead , one could just say that one wants to learn a causal model and the proposed procedure is called CausalGAN ?",request
rkBmo9ryM_45,( This would also clarify the paper 's contribution . ),evaluation
rkBmo9ryM_46,"- many realizations = one sample ( not samples ) , I think .",evaluation
rkBmo9ryM_47,- Fig 1 : which model is used to generate the conditional sample ?,request
rkBmo9ryM_48,- The notation changes between E and N and Z for the noises .,fact
rkBmo9ryM_49,"I believe that N is supposed to be the noise in the SCM ,",evaluation
rkBmo9ryM_50,but then maybe it should not be called E at the beginning .,evaluation
rkBmo9ryM_51,- I believe Prop 1 ( as it is stated ) is wrong .,fact
rkBmo9ryM_52,"For a reference , see Peters , Janzing , Scholkopf : Elements of Causal Inference : Foundations and Learning Algorithms ( available as pdf ) , Definition 6.32 .",fact
rkBmo9ryM_53,One requires the strict positivity of the densities ( to properly define conditionals ) .,fact
rkBmo9ryM_54,"Also , I believe the Z should be a vector , not a set .",request
rkBmo9ryM_55,"- Below eq . ( 1 ) , I am not sure what the V in P_V refers to .",evaluation
rkBmo9ryM_56,- The concept of data probability density function seems weird to me .,evaluation
rkBmo9ryM_57,"Either it is referring to the fitted model , then it 's a bad name ,",evaluation
rkBmo9ryM_58,"or it 's an empirical distribution , then there is no pdf , but a pmf .",evaluation
rkBmo9ryM_59,- Many subscripts are used without explanation .,evaluation
rkBmo9ryM_60,r - > real ?,request
rkBmo9ryM_61,g - > generating ?,request
rkBmo9ryM_62,G - > generating ?,request
rkBmo9ryM_63,"Sometimes , no subscripts are used ( e.g. , Fig 4 or figures in Sec. 8.13 )",fact
rkBmo9ryM_64,- I would get rid of Theorem 1 and explain it in words for the following reasons .,request
rkBmo9ryM_65,"( 1 ) What is an "" informal "" theorem ?",request
rkBmo9ryM_66,( 2 ) It refers to equations appearing much later .,evaluation
rkBmo9ryM_67,( 3 ) It is stated again later as Theorem 2 .,fact
rkBmo9ryM_68,"- Also : the name P_g does not appear anywhere else in the theorem , I think .",fact
rkBmo9ryM_69,"- Furthermore , I would reformulate the theorem .",request
rkBmo9ryM_70,The main point is that the intervention distributions are correct,fact
rkBmo9ryM_71,"( this fact seems to be there ,",fact
rkBmo9ryM_72,"but is "" hidden "" in the CIGN notation in the corollary ) .",fact
rkBmo9ryM_73,- Re . the formulation in Thm 2 : is it clear that there is a unique global optimum,evaluation
rkBmo9ryM_74,"( my intuition would say there could be several ) ,",evaluation
rkBmo9ryM_75,"thus : better write "" _ a _ global minimum "" ?",request
rkBmo9ryM_76,- Fig. 3 was not very clear to me .,evaluation
rkBmo9ryM_77,I suggest to put more information into its caption .,request
rkBmo9ryM_78,"- In particular , why is the dataset not used for the causal controller ?",request
rkBmo9ryM_79,"I thought , that it should model the joint ( empirical ) distribution over the labels ,",evaluation
rkBmo9ryM_80,and this is part of the dataset .,fact
rkBmo9ryM_81,Am I missing sth ?,non-arg
rkBmo9ryM_82,"- IMO , the structure of the paper can be improved .",evaluation
rkBmo9ryM_83,"Currently , Section 3 is called "" Background """,fact
rkBmo9ryM_84,which does not say much .,evaluation
rkBmo9ryM_85,"Section 4 contains CIGMs , Section 5 Causal GANs , 5.1 . Causal Controller , 5.2 . CausalGAN , 5.2.1 . Architecture ( which the causal controller is part of ) etc .",fact
rkBmo9ryM_86,An alternative could be : Sec 1 : Introduction Sec 1.1 : Related Work Sec 2 : Causal Models Sec 2.1 : Causal Models using Generative Models ( old : CIGM ) Sec 3 : Causal GANs Sec 3.1 : Architecture ( including controller ) Sec 3.2 : loss functions ... Sec 4 : Empricial Results ( old : Sec. 6 : Results ),request
rkBmo9ryM_87,"- "" Causal Graph 1 "" is not a proper reference",evaluation
rkBmo9ryM_88,( it 's Fig 23 I guess ) .,evaluation
rkBmo9ryM_89,"Also , it is quite important for the paper ,",evaluation
rkBmo9ryM_90,I think it should be in the main part .,request
rkBmo9ryM_91,"- There are different references to the "" Appendix "" , "" Suppl . Material "" , or "" Sec. 8 """,fact
rkBmo9ryM_92,-- please be consistent,request
rkBmo9ryM_93,( and try to avoid ambiguity by being more specific,request
rkBmo9ryM_94,-- the appendix contains ~ 20 pages ) .,fact
rkBmo9ryM_95,Have I missed the reference to the proof of Thm 2 ?,non-arg
rkBmo9ryM_96,- 8.1 . contains copy-paste from the main text .,fact
rkBmo9ryM_97,"- "" proposition from Goodfellow "" - > please be more precise",request
rkBmo9ryM_98,- What is Fig 8 used for ?,request
rkBmo9ryM_99,Is it not sufficient to have and discuss Fig 23 ?,request
rkBmo9ryM_100,"- IMO , Section 5.3 . should be rewritten ( also , maybe include another reference for BEGAN ) .",request
rkBmo9ryM_101,- There is a reference to Lemma 15 .,fact
rkBmo9ryM_102,"However , I have not found that lemma .",fact
rkBmo9ryM_103,"- I think it 's quite interesting that the framework seems to also allow answering counterfactual questions for realizations that have been sampled from the model , see Fig 16 .",evaluation
rkBmo9ryM_104,"This is the case since for the generated realizations , the noise values are known .",fact
rkBmo9ryM_105,The authors may think about including a comment on that issue .,request
rkBmo9ryM_106,"- Since this paper 's main proposal is a methodological one ,",evaluation
rkBmo9ryM_107,I would make the publication conditional on the fact that code is released .,request
ByHD_eqxf_0,"Let me first note that I am not very familiar with the literature on program generation , molecule design or compiler theory ,",non-arg
ByHD_eqxf_1,"which this paper draws heavily from ,",evaluation
ByHD_eqxf_2,so my review is an educated guess .,non-arg
ByHD_eqxf_3,"This paper proposes to include additional constraints into a VAE which generates discrete sequences , namely constraints enforcing both semantic and syntactic validity .",fact
ByHD_eqxf_4,"This is an extension to the Grammar VAE of Kusner et . al , which includes syntactic constraints but not semantic ones .",fact
ByHD_eqxf_5,"These semantic constraints are formalized in the form of an attribute grammar , which is provided in addition to the context-free grammar .",fact
ByHD_eqxf_6,"The authors evaluate their methods on two tasks , program generation and molecule generation .",fact
ByHD_eqxf_7,"Their method makes use of additional prior knowledge of semantics ,",fact
ByHD_eqxf_8,which seems task-specific and limits the generality of their model .,evaluation
ByHD_eqxf_9,They report that their method outperforms the Character VAE ( CVAE ) and Grammar VAE ( GVAE ) of Kusner et . al .,fact
ByHD_eqxf_10,"However , it is n't clear whether the comparison is appropriate :",evaluation
ByHD_eqxf_11,"the authors report in the appendix that they use the kekulised version of the Zinc dataset of Kusner et . al ,",fact
ByHD_eqxf_12,whereas Kusner et . al do not make any mention of this .,fact
ByHD_eqxf_13,The baselines they compare against for CVAE and GVAE in Table 1 are taken directly from Kusner et . al though .,fact
ByHD_eqxf_14,Can the authors clarify whether the different methods they compare in Table 1 are all run on the same dataset format ?,request
ByHD_eqxf_15,"Typos : - Page 5 : "" while in sampling procedure "" - > "" while in the sampling procedure """,request
ByHD_eqxf_16,"- Page 6 : "" a deep convolution neural networks "" - > "" a deep convolutional neural network """,request
ByHD_eqxf_17,"- Page 6 : "" KL-divergence that proposed in "" - > "" KL-divergence that was proposed in """,request
ByHD_eqxf_18,"- Page 6 : "" since in training time "" - > "" since at training time """,request
ByHD_eqxf_19,"- Page 6 : "" can effectively computed "" - > "" can effectively be computed """,request
ByHD_eqxf_20,"- Page 7 : "" reset for training "" - > "" rest for training """,request
r1IUXROxz_0,"The claimed results of "" combining transformations "" in the context of RC was done in the works of Herbert Jaeger on conceptors [ 1 ] ,",fact
r1IUXROxz_1,which also should be cited here .,request
r1IUXROxz_2,The argument of biological plausibility is not justified .,evaluation
r1IUXROxz_3,"The authors use an echo-state neural network with standard tanh activations , which is as far away from real neuronal signal processing than ordinary RNNs used in the field , with the difference that the recurrent weights are not trained .",fact
r1IUXROxz_4,"If the authors want to make the case of biological plausibility , they should use spiking neural networks .",request
r1IUXROxz_5,"The experiment on MNIST seems artificial , in particular transforming the image into a time-series and thereby imposing an artificial temporal structure .",evaluation
r1IUXROxz_6,The assumption that column_i is obtained by information of <VAR> is not true for images .,fact
r1IUXROxz_7,"To make a point , the authors should use a datasets with related sets of time-series data , e.g EEG or NLP data .",request
r1IUXROxz_8,In total this paper does not have enough novelty for acceptance,evaluation
r1IUXROxz_9,and the experiments are not well chosen for this kind of work .,evaluation
r1IUXROxz_10,Also the authors overstate the claim of biological plausibility,evaluation
r1IUXROxz_11,( just because we do n't train the recurrent weights does not make a method biologically plausible ) .,evaluation
r1IUXROxz_12,[1] <CIT>,reference
SJzxff6xM_0,This paper proposes the use of an ensemble of regression SVM models to predict the performance curve of deep neural networks .,fact
SJzxff6xM_1,This can be used to determine which model should be trained ( further ) .,fact
SJzxff6xM_2,"The authors compare their method , named Sequential Regression Models ( SRM ) in the paper , to previously proposed methods such as BNN , LCE and LastSeenValue and claim that their method has higher accuracy and less time complexity than the others .",fact
SJzxff6xM_3,They also use SRM in combination with a neural network meta-modeling method and a hyperparameter optimization one and show that it can decrease the running time in these approaches to find the optimized parameters .,fact
SJzxff6xM_4,Pros : The paper is proposing a simple yet effective method to predict accuracy .,evaluation
SJzxff6xM_5,"Using SVM for regression in order to do accuracy curve prediction was for me an obvious approach ,",evaluation
SJzxff6xM_6,I was surprised to see that no one has attempted this before .,evaluation
SJzxff6xM_7,"Using features sur as time-series ( TS ) , Architecture Parameters ( AP ) and Hyperparameters ( HP ) is appropriate ,",evaluation
SJzxff6xM_8,and the study of the effect of these features on the performance has some value .,evaluation
SJzxff6xM_9,Joining SRM with MetaQNN is interesting,evaluation
SJzxff6xM_10,as the method is a computation hog that can benefit from such refinement .,evaluation
SJzxff6xM_11,The overall structure of the paper is appropriate .,evaluation
SJzxff6xM_12,The literature review seems to cover and categorize well the field .,evaluation
SJzxff6xM_13,Cons : I found the paper difficult to read .,evaluation
SJzxff6xM_14,"In particular , the SRM method , which is the core of the paper , is not described properly ,",evaluation
SJzxff6xM_15,I am not able to make sense of the description provided in Sec. 3.1 .,evaluation
SJzxff6xM_16,The paper is not talking about the weaknesses of the method at all .,evaluation
SJzxff6xM_17,"The practicability of the method can be controversial ,",evaluation
SJzxff6xM_18,the number of attempts require to build the ( meta - ) training set of runs can be huge and lead to something that would be much more costful that letting the runs going on for more iterations .,evaluation
SJzxff6xM_19,Questions : 1 . The approach of sequential regression SVM is not explained properly .,evaluation
SJzxff6xM_20,Nothing was given about the combination weights of the method .,fact
SJzxff6xM_21,How is the ensemble of ( 1-T ) training models trained to predict the <VAR> ?,request
SJzxff6xM_22,2 . SRM needs to gather training samples which are 100 accuracy curves for T-1 epochs .,fact
SJzxff6xM_23,This is the big challenge of SRM,evaluation
SJzxff6xM_24,because training different variations of a deep neural networks to T-1 epochs can be a very time consuming process .,evaluation
SJzxff6xM_25,"Therefore , SRM has huge preparing training dataset time complexity that is not mentioned in the paper .",evaluation
SJzxff6xM_26,The other methods use only the first epochs of considered deep neural network to guess about its curve shape for epoch T.,fact
SJzxff6xM_27,These methods are time consuming in prediction time .,evaluation
SJzxff6xM_28,The authors compare only the prediction time of SRM with them,fact
SJzxff6xM_29,which is really fast .,evaluation
SJzxff6xM_30,"By the way still , SRM is interesting method if it can be trained once and then be used for different datasets without retraining .",evaluation
SJzxff6xM_31,Authors should show these results for SRM .,request
SJzxff6xM_32,3 . Discussing about the robustness of SRM for different depth is interesting,evaluation
SJzxff6xM_33,and I suggest to prepare more results to show the robustness of SRM to violation of different hyperparameters .,request
SJzxff6xM_34,4 . There is no report of results on huge datasets like big Imagenet,fact
SJzxff6xM_35,which takes a lot of time for deep training and we need automatic advance stopping algorithms to tune the hyper parameters of our model on it .,evaluation
SJzxff6xM_36,5 . In Table 2 and Figure 3 the results are reported with percentage of using the learning curve .,fact
SJzxff6xM_37,"To be more informative they should be reported by number of epochs , in addition or not to percentage .",request
SJzxff6xM_38,"6 . In section 4 , the authors talk about estimating the model uncertainty in the stopping point and propose a way to estimate it .",fact
SJzxff6xM_39,But we can not find any experimental results that is related to the effectiveness of proposed method and considered assumptions .,fact
SJzxff6xM_40,There are also some typos .,request
SJzxff6xM_41,"In section 3.3 part Ablation Study on Features Sets , line 5 , the sentence should be “ Ap are more important than HP ” .",request
rke8ggtxG_0,This paper discusses using neural networks for super-resolution .,fact
rke8ggtxG_1,"The positive aspects of this work is that the use of two neural networks in tandem for this task may be interesting ,",evaluation
rke8ggtxG_2,and the authors attempt to discuss the network 's behavior by drawing relations to successful sparsity-based super-resolution .,fact
rke8ggtxG_3,"Unfortunately I can not see any novelty in the relationship the authors draw to LASSO style super-resolution and dictionary learning beyond what is already in the literature ( see references below ) , including in one reference that the authors cite .",evaluation
rke8ggtxG_4,"In addition , there are a number of sloppy mistakes ( e.g. Equation 10 as a clear copy-paste error ) in the manuscript .",evaluation
rke8ggtxG_5,"Given that much of the main result seems to already be known ,",evaluation
rke8ggtxG_6,I feel that this work is not novel enough at this time .,evaluation
rke8ggtxG_7,Some other minor points for the authors to consider for future iterations of this work : - The authors mention the computational burden of solving L1-regularized optimizations .,fact
rke8ggtxG_8,"A lat of work has been done to create fast , efficient solvers in many settings ( e.g. homotopy , message passing etc. ) .",evaluation
rke8ggtxG_9,Are these methods still insufficient in some applications ?,request
rke8ggtxG_10,"If so , which applications of interest are the authors considering ?",request
rke8ggtxG_11,"- In figure 1 , it seems that under "" superresolution problem "" : ' f ' should be ' High res data ' and ' g ' should be ' Low res data ' instead of what is there .",request
rke8ggtxG_12,I 'm also not sure how this figure adds to the information already in the text .,evaluation
rke8ggtxG_13,"- In the results , the authors mention how some network features represented by certain neurons resemble the training data .",fact
rke8ggtxG_14,This seems like over-training and not a good quality for generalization .,evaluation
rke8ggtxG_15,"The authors should clarify if , and why , this might be a good thing for their application .",request
rke8ggtxG_16,- Overall a heavy editing pass is needed to fix a number of typos throughout .,request
rke8ggtxG_17,References : [1] <CIT>,reference
rke8ggtxG_18,[2] <CIT>,reference
rke8ggtxG_19,[3] <CIT>,reference
rke8ggtxG_20,[4] <CIT>,reference
HJzVc2sxf_0,"This paper proposes a new method to train residual networks in which one starts by training shallow ResNets , doubling the depth and warm starting from the previous smaller model in a certain way , and iterating .",fact
HJzVc2sxf_1,The authors relate this idea to a recent dynamical systems view of ResNets in which residual blocks are viewed as taking steps in an Euler discretization of a certain differential equation .,fact
HJzVc2sxf_2,This interpretation plays a role in the proposed training method by informing how the “ step sizes ” in the Euler discretization should change when doubling the depth of the network .,evaluation
HJzVc2sxf_3,The punchline of the paper is that the authors are able to achieve similar performance as “ full ResNet training ” but with significantly reduced training time .,evaluation
HJzVc2sxf_4,"Overall , the proposed method is novel",evaluation
HJzVc2sxf_5,"— even though this idea of going from shallow to deep is natural for residual networks ,",evaluation
HJzVc2sxf_6,tying the idea to the dynamical systems perspective is elegant .,evaluation
HJzVc2sxf_7,Moreover the paper is clearly written .,evaluation
HJzVc2sxf_8,Experimental results are decent,evaluation
HJzVc2sxf_9,— there are clear speedups to be had based on the authors ' experiments .,fact
HJzVc2sxf_10,However it is unclear if these gains in training speed are significant enough for people to flock to using this ( more complicated ) method of training .,evaluation
HJzVc2sxf_11,"I only have a few small questions/comments : * A more naive way to do multi-level training would be to again iteratively double the depth , but perhaps not halve the step size .",evaluation
HJzVc2sxf_12,This might be a good baseline to compare against to demonstrate the value of the dynamical systems viewpoint .,request
HJzVc2sxf_13,* One thing I ’m unclear on is how convergence was assessed …,evaluation
HJzVc2sxf_14,my understanding is that the training proceeds for a fixed number of epochs ( ? ),evaluation
HJzVc2sxf_15,- but should n’t this also depend on the depth in some way ?,request
HJzVc2sxf_16,* Would the speedups be more dramatic for a larger dataset like Imagenet ?,request
HJzVc2sxf_17,"* Finally , not being very familiar with multigrid methods from the numerical methods literature",non-arg
HJzVc2sxf_18,— I would have liked to hear about whether there are deeper connections to these methods .,request
ry6zwoief_0,"Summary The authors argue that ensemble prediction takes too much computation time and resource , especially in the case of deep neural networks .",fact
ry6zwoief_1,They then address the problem by proposing an adaptive prediction approach .,fact
ry6zwoief_2,"The approach is based on the observation that it is most important for ensemble approaches to focus on the "" uncertain "" examples .",fact
ry6zwoief_3,"The proposed approach thus conducts early-stopping prediction when the confidence ( certainty ) of the prediction is high enough , where the confidence is based on the confidence intervals of ( multi-class ) labels based on the student-t distribution .",fact
ry6zwoief_4,Experiments on vision datasets demonstrate that the proposed approach is effective in reducing computation resources while maintaining sufficient accuracy .,evaluation
ry6zwoief_5,Comments * The experiments are limited in the scope of ( image ) multi-class classification .,evaluation
ry6zwoief_6,"It is not clear whether the proposed approach is effective for other classification tasks , or even more sophisticated tasks like multi-label classification or sequence tagging .",evaluation
ry6zwoief_7,* The idea appears elegant but rather straightforward .,evaluation
ry6zwoief_8,One important baseline that is easy but not discussed is to set a static threshold on pairwise comparison ( p_max - p_secondmax ) .,evaluation
ry6zwoief_9,Would this baseline be competitive with the proposed approach ?,request
ry6zwoief_10,Such a comparison is able to demonstrate the benefits of using confidence interval .,evaluation
ry6zwoief_11,"* The overall improvement in computation time seems to be within a constant scale ,",fact
ry6zwoief_12,which can be easily achieved by doing ensemble prediction in parallel,evaluation
ry6zwoief_13,( note that the proposed approach would require predicting sequentially ) .,fact
ry6zwoief_14,So are there real applications that can benefit from the improvement ?,request
ry6zwoief_15,"* typo : p4 , line19 , neural "" netowkrs "" - > neural "" networks """,request
SkzzOIcez_0,The authors propose a loss that is based on a RBF loss for metric learning and incorporates additional per exemplar weights in the index for classification .,fact
SkzzOIcez_1,Significant improvements over softmax are shown on several datasets .,fact
SkzzOIcez_2,"IMHO , this could be a worthwhile paper ,",evaluation
SkzzOIcez_3,but the framing of the paper into existing literature is lacking,evaluation
SkzzOIcez_4,and thus it appears as if the authors are re-inventing the wheel ( NCA loss ) under a different name ( RBF solver ) .,evaluation
SkzzOIcez_5,The specific problems are : - The authors completely miss the connection to NCA loss ( <URL> ),fact
SkzzOIcez_6,and thus appear to be re-inventing the wheel .,evaluation
SkzzOIcez_7,"- The proposed metric learning scenario is exactly as proposed in the NCA loss works ,",fact
SkzzOIcez_8,while the classification approach adds an interesting twist by learning per exemplar weights .,evaluation
SkzzOIcez_9,I have n't encountered this before and it could make an interesting proposal .,evaluation
SkzzOIcez_10,Of course the benefit of this should be evaluated in ablation studies,request
SkzzOIcez_11,( Tab 3 shows one experiment with marginal improvements ) .,evaluation
SkzzOIcez_12,- The authors ' use of ' solver ' seems uncommon and confusing .,evaluation
SkzzOIcez_13,What is proposed is a loss in addition to building a weighted index in the case of classification .,fact
SkzzOIcez_14,- In the metric learning comparison with softmax ( end of page 9 ) the authors mentions that a Gaussian standard deviation for softmax is learned .,fact
SkzzOIcez_15,It appears as if the authors use the softmax logits as embedding,fact
SkzzOIcez_16,whereas the more common approach is to use the bottleneck layer .,fact
SkzzOIcez_17,This is also indicated by the discussion at the end of page 10 where the authors mention that softmax is restricted to axis aligned embeddings .,fact
SkzzOIcez_18,All softmax metric learning experiments should be carried out on appropriately sized bottleneck layers .,request
SkzzOIcez_19,"- Some of the motivations of what the various methods learn seem flawed ,",evaluation
SkzzOIcez_20,e.g. triplet loss CAN learn multiple modes per class and there is nothing in the Softmax loss that encourages the classes to fill a large region of the space .,fact
SkzzOIcez_21,- Why do n't the authors compare on ImageNet ?,request
SkzzOIcez_22,Some positive points : - The authors mention in Sec 3.3 that updating the RBF centres is not required .,fact
SkzzOIcez_23,"This is a crucial point that should be made a centerpiece of this work ,",request
SkzzOIcez_24,as there are many metric learning works that struggle with this .,evaluation
SkzzOIcez_25,Additional experiments that can investigate this point would greatly contribute to a well rounded paper .,request
SkzzOIcez_26,- The numbers reported in Tab 1 show very significant improvements,evaluation
SkzzOIcez_27,"If the paper was re-framed and builds on top of the already existing NCA loss , there could be valuable contributions in this paper .",request
SkzzOIcez_28,"The experimental comparisons are lacking in some respect ,",evaluation
SkzzOIcez_29,"as the comparison with Softmax as a metric learning method seems uncommon , i.e. using the logits instead of the bottleneck layer .",evaluation
SkzzOIcez_30,I encourage the authors to extend the paper and flesh out some of the experiments and then submit it again .,request
HkCuu2YxG_0,"This paper misses the point of what VAEs ( or GANs , in general ) are used for .",evaluation
HkCuu2YxG_1,"The idea of using VAEs is not to encode and decode images ( or in general any input ) , but to recover the generating process that created those images so we have an unlimited source of samples .",evaluation
HkCuu2YxG_2,The use of these techniques for compressing is still unclear,evaluation
HkCuu2YxG_3,and their quality today is too low .,evaluation
HkCuu2YxG_4,So the attack that the authors are proposing does not make sense,evaluation
HkCuu2YxG_5,and my take is that we should see significant changes before they can make sense .,evaluation
HkCuu2YxG_6,But let ’s assume that at some point they can be used as the authors propose .,evaluation
HkCuu2YxG_7,"In which one person encodes an image , send the latent variable to a friend , but a foe intercepts it on the way and tampers with it so the receiver recovers the wrong image without knowing .",fact
HkCuu2YxG_8,"Now if the sender believes the sample can be tampered with , if the sender codes z with his private key would not make the attack useless ?",evaluation
HkCuu2YxG_9,I think this will make the first attack useless .,evaluation
HkCuu2YxG_10,The other two attacks require that the foe is inserted in the middle of the training of the VAE .,fact
HkCuu2YxG_11,"This is even less doable ,",evaluation
HkCuu2YxG_12,because the encoder and decoder are not train remotely .,fact
HkCuu2YxG_13,They are train of the same machine or cluster in a controlled manner by the person that would use the system .,fact
HkCuu2YxG_14,Once it is train it will give away the decoder and keep the encoder for sending information .,fact
r1Ps9aPez_0,This paper discusses a text-to-speech system which is based on a convolutional attentive seq2seq architecture .,fact
r1Ps9aPez_1,"It covers experiments on a few datasets , testing the model 's ability to handle increasing numbers of speakers .",fact
r1Ps9aPez_2,"By and large , this is a "" system "" paper",evaluation
r1Ps9aPez_3,"- it mostly describes the successful application of many different existing ideas to an important problem ( with some exceptions , e.g. the novel method of enforcing monotonic alignments during inference ) .",evaluation
r1Ps9aPez_4,"In this type of paper , I typically am most interested in hearing about * why * a particular design choice was made , what alternatives were tried , and how different ideas worked .",evaluation
r1Ps9aPez_5,This paper is lacking in this regard,evaluation
r1Ps9aPez_6,- I frequently was left looking for more insight into the particular system that was designed .,evaluation
r1Ps9aPez_7,"Beyond that , I think more detailed description of the system would be necessary in order to reimplement it suitably",request
r1Ps9aPez_8,"( another important potential takeaway for a "" system "" paper ) .",evaluation
r1Ps9aPez_9,"Separately , I the thousands-of-speakers results are just not that impressive",evaluation
r1Ps9aPez_10,- a MOS of 2 is not really useable in the real-world .,evaluation
r1Ps9aPez_11,"For that reason , I think it 's a bit disingenuous to sell this system as "" 2000-Speaker Neural Text-to-Speech "" .",evaluation
r1Ps9aPez_12,"For the above reasons , I 'm giving the paper a "" marginally above "" rating .",evaluation
r1Ps9aPez_13,"If the authors provide improved insight , discussion of system specifics , and experiments , I 'd be open to raising my review .",request
r1Ps9aPez_14,"Below , I give some specific questions and suggestions that could be addressed in future drafts .",non-arg
r1Ps9aPez_15,- It might be worth giving a sentence or two defining the TTS problem,request
r1Ps9aPez_16,"- the paper is written assuming background knowledge about the problem setting , including different possible input sources , what a vocoder is , etc .",evaluation
r1Ps9aPez_17,The ICLR community at large may not have this domain-specific knowledge .,evaluation
r1Ps9aPez_18,"- Why "" softsign "" and not tanh ?",request
r1Ps9aPez_19,Seems like an unusual choice .,evaluation
r1Ps9aPez_20,"- What do the "" c "" and "" 2c "" in Figure 2a denote ?",request
r1Ps9aPez_21,- Why scale ( h_k + h_e ) by \ sqrt { 0.5 } when computing the attention value vectors ?,request
r1Ps9aPez_22,"- "" An L1 loss is computed using the output spectrograms """,quote
r1Ps9aPez_23,I assume you mean the predicted and target spectrograms are compared via an L1 loss .,evaluation
r1Ps9aPez_24,Why L1 ?,request
r1Ps9aPez_25,"- In Vaswani et al. , it was shown that a learned positional encoding worked about as well as the sinusoidal position encodings despite being potentially more flexible/less "" hand-designed "" for machine translation .",fact
r1Ps9aPez_26,Did you also try this for TTS ?,request
r1Ps9aPez_27,Any insight ?,request
r1Ps9aPez_28,"- Some questions about monotonic attention : Did you use the training-time "" soft "" monotonic attention algorithm from Raffel et al. during training and inference , or did you use the "" hard "" monotonic attention at inference time ?",request
r1Ps9aPez_29,"IIUC the "" soft "" algorithm does n't actually force strict monotonicity .",fact
r1Ps9aPez_30,"You wrote "" monotonic attention results in the model frequently mumbling words "" ,",quote
r1Ps9aPez_31,can you provide evidence/examples of this ?,request
r1Ps9aPez_32,Why do you think this happens ?,request
r1Ps9aPez_33,"The monotonic attention approach seems more principled than post-hoc limiting softmax attention to be monotonic ,",evaluation
r1Ps9aPez_34,why do you think it did n't work as well ?,request
r1Ps9aPez_35,"- I ca n't find an actual reference to what you mean by a "" wavenet vocoder "" .",evaluation
r1Ps9aPez_36,The original wavenet paper describes an autoregressive model for waveform generation .,fact
r1Ps9aPez_37,"In order to use it as a vocoder , you 'd have to do conditioning in some way .",fact
r1Ps9aPez_38,How ?,request
r1Ps9aPez_39,What was the structure of the wavenet you used ?,request
r1Ps9aPez_40,Why ?,request
r1Ps9aPez_41,These details appear to be missing .,fact
r1Ps9aPez_42,All you write is the sentence ( which seems to end without a period ),fact
r1Ps9aPez_43," In the WaveNet vocoder , we use mel-scale spectrograms from the decoder to condition a Wavenet , which was trained separated  .",quote
r1Ps9aPez_44,- Can you provide examples of the mispronunciations etc. which were measured for Table 1 ?,request
r1Ps9aPez_45,Was the evaluation of each attention mechanism done blindly ?,request
r1Ps9aPez_46,"- The 2.07 MOS figure produced for tacotron seems extremely low ,",evaluation
r1Ps9aPez_47,and seems to indicate that something went wrong or that insufficient care was taken to report this baseline .,evaluation
r1Ps9aPez_48,How did you adapt tacotron ( which as I understand is a single-speaker model ) to the multi-speaker setting ?,request
r1Ps9aPez_49,- Table 3 begs the question of whether Deep Voice 3 can outperform Deep Voice 2 when using a wavenet vocoder on VCTK ( or improve upon the poor 2.09 MOS score reported ) .,request
r1Ps9aPez_50,Why was n't this experiment run ?,request
r1Ps9aPez_51,"- The paragraph and appendix about deploying at scale is interesting and impressive ,",evaluation
r1Ps9aPez_52,but seems a bit out of place,evaluation
r1Ps9aPez_53,"- it probably makes more sense to include this information in a separate "" systems "" paper .",request
