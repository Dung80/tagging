ryx2q7_eG_0,"This paper proposes for training a question answering model from answers only and a KB by learning latent trees that capture the syntax and learn the semantic of words , including referential terms like "" red "" and also compositional operators like "" not "" .",fact
ryx2q7_eG_1,"I think this model is elegant , beautiful and timely .",evaluation
ryx2q7_eG_2,The authors do a good job of explaining it clearly .,evaluation
ryx2q7_eG_3,"I like the modules of composition that seem to make a very intuitive sense for the "" algebra "" that is required and the parsing algorithm is clean .",evaluation
ryx2q7_eG_4,"However , I think that the evaluation is lacking , and in some sense the model exposes the weakness of the dataset that it uses for evaluation .",evaluation
ryx2q7_eG_5,I have 2.5 major issues with the paper and a few minor comments :,non-arg
ryx2q7_eG_6,Parsing : * The authors do n't really say what is the base case for \ Psi that scores tokens,fact
ryx2q7_eG_7,( unless I missed it and if indeed it is missing it really needs to be added ),request
ryx2q7_eG_8,and only provide the recursive case .,fact
ryx2q7_eG_9,From that I understand that the only features that they use are whether a certain word makes sense in a certain position of the rule application in the context of the question .,fact
ryx2q7_eG_10,While these features are based on Durrett et al. 's neural syntactic parser it seems like a pretty weak signal to learn from .,evaluation
ryx2q7_eG_11,"This makes me wonder , how does the parser learn whether one parse is better than the other ?",non-arg
ryx2q7_eG_12,Only based on this signal ?,non-arg
ryx2q7_eG_13,It makes me suspicious that the distribution of language is not very ambiguous and that as long as you can construct a tree in some context you can do it in almost any other context .,evaluation
ryx2q7_eG_14,This is probably due to the fact that the CLEVR dataset was generated mostly using templates and is not really natural utterances produced by people .,evaluation
ryx2q7_eG_15,"Of course many people have published on CLEVR although of its language limitations ,",fact
ryx2q7_eG_16,"but I was a bit surprised that only these features are enough to solve the problem completely ,",evaluation
ryx2q7_eG_17,and this makes me curious as to how hard is it to reverse-engineer the way that the language was generated with a context-free mechanism that is similar to how the data was produced .,evaluation
ryx2q7_eG_18,"* Related to that is that the decision for a score of a certain type t for a span ( i , j ) is the sum for all possible rule applications , rather than a max , which again means that there is no competition between different parse trees that result with the same type of a single span .",fact
ryx2q7_eG_19,Can the authors say something about what the parser learns ?,request
ryx2q7_eG_20,Does it learn to extract from the noise clear parse trees ?,non-arg
ryx2q7_eG_21,What is the distribution of rules in those sums ?,non-arg
ryx2q7_eG_22,is there some rule that is more preferred than others usually ?,non-arg
ryx2q7_eG_23,It seems like there is loss of information in the sum,evaluation
ryx2q7_eG_24,and it is unclear what is the effect of that in the paper .,evaluation
ryx2q7_eG_25,Evaluation : * Related to that is indeed the fact that they use CLEVR only .,fact
ryx2q7_eG_26,There is now the Cornell NLVR dataset that is more challenging from a language perspective,fact
ryx2q7_eG_27,and it would be great to have an evaluation there as well .,request
ryx2q7_eG_28,"Also the authors only compare to 3 baselines where 2 do n't even see the entire KB ,",fact
ryx2q7_eG_29,"so the only "" real "" baseline is relation net .",evaluation
ryx2q7_eG_30,The authors indeed state that it is state-of-the-art on clevr .,fact
ryx2q7_eG_31,* It is worth noting that relation net is reported to get 95.5 accuracy while the authors have 89.4 .,fact
ryx2q7_eG_32,"They use a subset so this might be the reason ,",fact
ryx2q7_eG_33,but I am not sure how they compared to relation net exactly .,non-arg
ryx2q7_eG_34,Did they re-tune parameters once you have the new dataset ?,non-arg
ryx2q7_eG_35,This could make a difference in the final accuracy and cause an unfair advantage .,fact
ryx2q7_eG_36,* I would really appreciate more analysis on the trees that one gets .,request
ryx2q7_eG_37,Are sub-trees interpretable ?,non-arg
ryx2q7_eG_38,Can one trace the process of composition ?,non-arg
ryx2q7_eG_39,This could have been really nice if one could do that .,evaluation
ryx2q7_eG_40,"The authors have a figure of a purported tree , but where does this tree come from ?",non-arg
ryx2q7_eG_41,From the mode ?,non-arg
ryx2q7_eG_42,Form the authors ?,non-arg
ryx2q7_eG_43,Scalability : * How much of a problem would it be to scale this ?,non-arg
ryx2q7_eG_44,Will this work in larger domains ?,non-arg
ryx2q7_eG_45,It seems they compute an attention score over every entity and also over a matrix that is squared in the number of entities .,fact
ryx2q7_eG_46,So it seems if the number of entities is large that could be very problematic .,evaluation
ryx2q7_eG_47,Once one moves to larger KBs it might become hard to maintain full differentiability which is one of the main selling points of the paper .,evaluation
ryx2q7_eG_48,"Minor comments : * I think the phrase "" attention "" is a bit confusing -",evaluation
ryx2q7_eG_49,I thought of a distribution over entities at first .,non-arg
ryx2q7_eG_50,* The feature function is not super clearly written I think - perhaps clarify in text a bit more what it does .,evaluation
ryx2q7_eG_51,* I did not get how the denotation that is based on a specific rule applycation t_1 + t_2 -- > t works .,evaluation
ryx2q7_eG_52,Is it by looking at the grounding that is the result of that rule application ?,non-arg
ryx2q7_eG_53,* Authors say that the neural enquirer and neural symbolic machines produce flat programs -,fact
ryx2q7_eG_54,"that is not really true , the programs are just a linearized form of a tree ,",fact
ryx2q7_eG_55,so there is nothing very flat about it in my opinion .,evaluation
ryx2q7_eG_56,"Overall , I really enjoyed reading the paper ,",evaluation
ryx2q7_eG_57,but I was left wondering whether the fact that it works so well mostly attests to the way the data was generated and am still wondering how easy it would be to make this work in for more natural language or when the KB is large .,evaluation
Hyr9bdveG_0,In this paper the authors give a nice review of clustering methods with deep learning and a systematic taxonomy for existing methods .,fact
Hyr9bdveG_1,"Finally , the authors propose a new method by using one unexplored combination of taxonomy features .",fact
Hyr9bdveG_2,The paper is well-written and easy to follow .,evaluation
Hyr9bdveG_3,"The proposed combination is straightforward ,",evaluation
Hyr9bdveG_4,but lack of novelty .,evaluation
Hyr9bdveG_5,"From table 1 , it seems that the only differences between the proposed method and DEPICK is whether the method uses balanced assignment and pretraining .",fact
Hyr9bdveG_6,I am not convinced that these changes will lead to a significant difference .,evaluation
Hyr9bdveG_7,The performance of the proposed method and DEPICK are also similar in table 1 .,fact
Hyr9bdveG_8,"In addition , the experiments section is not comprehensive enough as well .",evaluation
Hyr9bdveG_9,the author only tested on two datasets .,fact
Hyr9bdveG_10,More datasets should be tested for evaluation .,request
Hyr9bdveG_11,"In addition , It seems that nearly all the experiments results from comparison methods are borrowed from the original publications .",fact
Hyr9bdveG_12,The authors should finish the experiments on comparison methods and fill the entries in Table 1 .,request
Hyr9bdveG_13,"In summary , the proposed method is lack of novelty compare to existing methods .",evaluation
Hyr9bdveG_14,"The survey part is nice ,",evaluation
Hyr9bdveG_15,however extensive experiments should be conducted by running existing methods on different datasets and analyzing the pros and cons of the methods and their application scenarios .,request
Hyr9bdveG_16,"Therefore , I think the paper can not be accepted at this stage .",evaluation
Sye2eNDxM_0,This paper aims to learn hierarchical policies by using a recursive policy structure regulated by a stochastic temporal grammar .,fact
Sye2eNDxM_1,"The experiments show that the method is better than a flat policy for learning a simple set of block-related skills in minecraft ( find , get , put , stack )",fact
Sye2eNDxM_2,and generalizes better to a modification of the environment ( size of room ) .,fact
Sye2eNDxM_3,The sequence of subtasks generated by the policy are interpretable .,evaluation
Sye2eNDxM_4,Strengths : - The grammar and policies are trained using a sparse reward upon task completion .,fact
Sye2eNDxM_5,- The method is well ablated ;,evaluation
Sye2eNDxM_6,Figures 4 and 5 answered most questions I had while reading .,non-arg
Sye2eNDxM_7,"- Theoretically , the method makes few assumptions about the environment and the relationships between tasks .",evaluation
Sye2eNDxM_8,- The interpretability of the final behaviors is a good result .,evaluation
Sye2eNDxM_9,Weaknesses : - The implementation gives the agent a -0.5 reward if it generates a currently unexecutable goal g \ u2019 .,fact
Sye2eNDxM_10,Providing this reward requires knowing the full state of the world .,fact
Sye2eNDxM_11,"If this hack is required , then this method would not be useful in a real world setting ,",evaluation
Sye2eNDxM_12,defeating the purpose of the sparse reward mentioned above .,evaluation
Sye2eNDxM_13,I would really like to see how the method performs without this hack .,request
Sye2eNDxM_14,- There are no comparisons to other multitask or hierarchical methods .,fact
Sye2eNDxM_15,Progressive Networks or Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning seem like natural comparisons .,evaluation
Sye2eNDxM_16,- A video to show what the environments and tasks look like during execution would be helpful .,request
Sye2eNDxM_17,- The performances of the different ablations are rather close .,evaluation
Sye2eNDxM_18,Please a standard deviation over multiple training runs .,request
Sye2eNDxM_19,"Also , why does figure 4.b not include a flat policy ?",request
Sye2eNDxM_20,"- The stages are ordered in a semantically meaningful order ( find is the first stage ) ,",fact
Sye2eNDxM_21,but the authors claim that the order is arbitrary .,fact
Sye2eNDxM_22,"If this claim is going to be included in the paper , it needs to be proven ( results shown for random orderings )",request
Sye2eNDxM_23,because right now I do not believe it .,evaluation
Sye2eNDxM_24,"Quality : The method does provide hierarchical and interpretable policies for executing instructions ,",fact
Sye2eNDxM_25,this is a meaningful direction to work on .,evaluation
Sye2eNDxM_26,"Clarity : Although the method is complicated , the paper was understandable .",evaluation
Sye2eNDxM_27,"Originality and significance : Although the method is interesting , I am worried that the environment has been too tailored for the method ,",evaluation
Sye2eNDxM_28,and that it would fail in realistic scenarios .,evaluation
Sye2eNDxM_29,"The results would be more significant if the tasks had an additional degree of complexity ,",request
Sye2eNDxM_30,"e.g. "" put blue block next to the green block "" "" get the blue block in room 2 "" .",non-arg
Sye2eNDxM_31,Then the sequences of subtasks would be a bit less linear,evaluation
Sye2eNDxM_32,"( e.g. , first need to find blue , then get , then find green , then put ) .",fact
Sye2eNDxM_33,At the moment the tasks are barely more than the actions provided in the environment .,evaluation
Sye2eNDxM_34,Another impedance to the paper 's significance is the number of hacks to make the method work,evaluation
Sye2eNDxM_35,"( ordering of stages , alternating policy optimization , first training each stage on only tasks of previous stage ) .",non-arg
Sye2eNDxM_36,"Because the method is only evaluated on one simple environment ,",fact
Sye2eNDxM_37,"it unclear which hacks are for the method generally , and which hacks are for the method to work on the environment .",evaluation
HJSdXVqxG_0,This paper creates a layered representation in order to better learn segmentation from unlabeled images .,fact
HJSdXVqxG_1,"It is well motivated ,",evaluation
HJSdXVqxG_2,"as Fig. 1 clearly shows the idea that if the segmentation was removed properly , the result would still be a natural image .",fact
HJSdXVqxG_3,"However , the method itself as described in the paper leaves many questions about whether they can achieve the proposed goal .",evaluation
HJSdXVqxG_4,I can not see from the formulation why would this model work as it is advertised .,evaluation
HJSdXVqxG_5,"The formulation ( 3-4 ) looks like a standard GAN , with some twist about measuring the GAN loss in the z space ( this has been used in e.g. PPGN and CVAE-GAN ) .",fact
HJSdXVqxG_6,I do n't see any term that would guarantee : 1 ) Each layer is a natural image .,fact
HJSdXVqxG_7,"This was advertised in the paper ,",fact
HJSdXVqxG_8,but the loss function is only on the final product G_K .,fact
HJSdXVqxG_9,"The way it is written in the paper , the result of each layer does not need to go through a discriminator .",fact
HJSdXVqxG_10,Nothing seems to have been done to ensure that each layer outputs a natural image .,fact
HJSdXVqxG_11,2 ) None of the layers is degenerate .,fact
HJSdXVqxG_12,"There does not seem to be any constraint either regularizing the content in each layer , or preventing any layer to be non-degenerate .",fact
HJSdXVqxG_13,3 ) The mask being contiguous .,fact
HJSdXVqxG_14,"I do n't see any term ensuring the mask being contiguous ,",fact
HJSdXVqxG_15,I imagine normally without such terms doing such kinds of optimization would lead to a lot of fragmented small areas being considered as the mask .,fact
HJSdXVqxG_16,The claim that this paper is for unsupervised semantic segmentation is overblown .,evaluation
HJSdXVqxG_17,"A major problem is that when conducting experiments , all the images seem to be taken from a single category , this implicitly uses the label information of the category .",fact
HJSdXVqxG_18,"In that regard , this can not be viewed as an unsupervised algorithm .",fact
HJSdXVqxG_19,"Even with that , the results definitely looked too good to be true .",evaluation
HJSdXVqxG_20,I have a really difficult time believing why such a standard GAN optimization would not generate any of the aforementioned artifacts and would perform exactly as the authors advertised .,evaluation
HJSdXVqxG_21,"Even if it does work as advertised , the utilization of implicit labels would make it subject to comparisons with a lot of weakly-supervised learning papers with far better results than shown in this paper .",evaluation
HJSdXVqxG_22,Hence I am pretty sure that this is not up to the standards of ICLR .,evaluation
HJmMNVDlz_0,This paper proposes a new model for the general task of inducing document representations ( embeddings ) .,fact
HJmMNVDlz_1,"The approach uses a CNN architecture , distinguishing it from the majority of prior efforts on this problem , which have tended to use RNNs .",fact
HJmMNVDlz_2,"This affords obvious computational advantages , as training may be parallelized .",fact
HJmMNVDlz_3,"Overall , the model presented is relatively simple ( a good thing , in my view ) and it indeed seems fast .",evaluation
HJmMNVDlz_4,I can thus see potential practical uses of this CNN based approach to document embedding in future work on language tasks .,evaluation
HJmMNVDlz_5,"The training strategy , which entails selecting documents and then indexes within them stochastically , is also neat .",evaluation
HJmMNVDlz_6,"Furthermore , the work is presented relatively clearly .",evaluation
HJmMNVDlz_7,"That said , my main concerns regarding this paper are that : ( 1 ) there 's not much new here , and ,",fact
HJmMNVDlz_8,"( 2 ) the experimental setup may be flawed ,",fact
HJmMNVDlz_9,in that it would seem model hyperparams were tuned for the proposed approach but not for the baselines ;,fact
HJmMNVDlz_10,I elaborate on these concerns below .,non-arg
HJmMNVDlz_11,Specific comments : ---- It 's hard to tease out exactly what 's new here :,evaluation
HJmMNVDlz_12,the various elements used are all well known .,fact
HJmMNVDlz_13,But perhaps there is merit in putting the specific pieces together .,evaluation
HJmMNVDlz_14,"Essentially , the novelty is using a CNN rather than an RNN to induce document embeddings .",fact
HJmMNVDlz_15,"- In Section 4.1 , the authors write that they report results for their after running "" parameter sweeps ... "" --",fact
HJmMNVDlz_16,"I presume that these were performed on a validation set ,",fact
HJmMNVDlz_17,but the authors should say so .,request
HJmMNVDlz_18,"In any case , a very potential weakness here : were analagous parameter sweeps for this dataset performed for the baseline models ?",fact
HJmMNVDlz_19,"It would seem not , as the authors write "" the IMDB training data using the default hyper-parameters "" for skip-thought .",fact
HJmMNVDlz_20,Surely it is unfair comparison if one model has been tuned to a given dataset while others use only the default hyper-parameters ?,evaluation
HJmMNVDlz_21,- Many important questions were left unaddressed in the experiments .,evaluation
HJmMNVDlz_22,"For example , does one really need to use the gating mechanism borrowed from the Dauphin et al. paper ?",request
HJmMNVDlz_23,What happens if not ?,request
HJmMNVDlz_24,How big of an effect does the stochastic sampling of document indices have on the learned embeddings ?,request
HJmMNVDlz_25,"Does the specific underlying CNN architecture affect results , and how much ?",request
HJmMNVDlz_26,None of these questions are explored .,fact
HJmMNVDlz_27,- I was left a bit confused regarding how the <VAR> embedding is actually estimated ;,evaluation
HJmMNVDlz_28,I think the details here are insufficient in the current presentation .,request
HJmMNVDlz_29,"The authors write that this is a "" function of all words up to <VAR> "" .",fact
HJmMNVDlz_30,"This would seem to imply that at test time , prediction is not in fact parallelizable , no ?",fact
HJmMNVDlz_31,Yet this seems to be one of the main arguments the authors make in favor of the model ( in contrast to RNN based methods ) .,fact
HJmMNVDlz_32,"In fact , I think the authors are proposing using the ( aggregated ) filter activation vectors ( <VAR> ) in eq . 5 ,",fact
HJmMNVDlz_33,but for some reason this is not made explicit .,fact
HJmMNVDlz_34,"Minor comments : - In Eq . 4 , should the product be element-wise to realize the desired gating ( as per the Dauhpin paper ) ?",request
HJmMNVDlz_35,This should be made explicit in the notation .,request
HJmMNVDlz_36,"- On the bottom of page 3 , the authors claim "" Expanding the prediction to multiple words makes the problem more difficult since the only way to achieve that is by ' understanding ' the preceding sequence . """,fact
HJmMNVDlz_37,This claim should either by made more precise or removed .,request
HJmMNVDlz_38,"It is not clear exactly what is meant here , nor what evidence supports it .",evaluation
HJmMNVDlz_39,- Commas are missing in a few .,fact
HJmMNVDlz_40,"For example on page 2 , probably want a comma after "" in parallel "" ( before "" significantly "" ) ; also after "" parallelize "" above "" Approach "" .",request
HJmMNVDlz_41,"- Page 4 : "" In contrast , our model addresses only requires """,quote
HJmMNVDlz_42,"-- > drop the "" addresses "" .",request
BJDxbMvez_0,"The authors propose a generative method that can produce images along a hierarchy of specificity , i.e. both when all relevant attributes are specified , and when some are left undefined , creating a more abstract generation task .",fact
BJDxbMvez_1,"Pros : + The results demonstrating the method 's ability to generate results for ( 1 ) abstract and ( 2 ) novel/unseen attribute descriptions , are generally convincing .",evaluation
BJDxbMvez_2,Both quantitative and qualitative results are provided .,fact
BJDxbMvez_3,+ The paper is fairly clear .,evaluation
BJDxbMvez_4,"Cons : - It is unclear how to judge diversity qualitatively , e.g. in Fig. 4 ( b ) .",evaluation
BJDxbMvez_5,- Fig. 5 could be more convincing ;,evaluation
BJDxbMvez_6," bushy eyebrows  is a difficult attribute to judge ,",evaluation
BJDxbMvez_7,"and in the abstract generation when that is the only attribute specified , it is not clear how good the results are .",evaluation
BJ2J7pFgf_0,This paper presents a method for classifying Tumblr posts with associated images according to associated single emotion word hashtags .,fact
BJ2J7pFgf_1,The method relies on sentiment pre-processing from GloVe and image pre-processing from Inception .,fact
BJ2J7pFgf_2,My strongest criticism for this paper is against the claim that Tumblr post represent self-reported emotions and that this method sheds new insight on emotion representation,evaluation
BJ2J7pFgf_3,"and my secondary criticism is a lack of novelty in the method ,",evaluation
BJ2J7pFgf_4,"which seems to be simply a combination of previously published sentiment analysis module and previously published image analysis module , fused in an output layer .",evaluation
BJ2J7pFgf_5,"The authors claim that the hashtags represent self-reported emotions ,",fact
BJ2J7pFgf_6,but this is not true in the way that psychologists query participants regarding emotion words in psychology studies .,evaluation
BJ2J7pFgf_7,Instead these are emotion words that a person chooses to broadcast along with an associated announcement .,fact
BJ2J7pFgf_8,"As the authors point out , hashtags and words may be used sarcastically or in different ways from what is understood in emotion theory .",fact
BJ2J7pFgf_9,It is quite common for everyday people to use emotion words this way e.g. using #love to express strong approval rather than an actual feeling of love .,evaluation
BJ2J7pFgf_10,In their analysis the authors claim :,fact
BJ2J7pFgf_11,"“ The 15 emotions retained were those with high relative frequencies on Tumblr among the PANAS-X scale ( Watson & Clark , 1999 ) ” .",quote
BJ2J7pFgf_12,"However five of the words the authors retain : bored , annoyed , love , optimistic , and pensive are not in fact found in the PANAS-X scale :",fact
BJ2J7pFgf_13,Reference : The PANAS-X Scale : <URL>,reference
BJ2J7pFgf_14,Also the longer version that the authors cited :,fact
BJ2J7pFgf_15,<URL>,reference
BJ2J7pFgf_16,"It should also be noted that the PANAS ( Positive and Negative Affect Scale ) scale and the PANAS-X ( the “ X ” is for eXtended ) scale are questionnaires used to elicit from participants feelings of positive and negative affect ,",fact
BJ2J7pFgf_17,"they are not collections of "" core "" emotion words ,",fact
BJ2J7pFgf_18,but rather words that are colloquially attached to either positive or negative sentiment .,fact
BJ2J7pFgf_19,"For example PANAS-X includes words like : “ strong ” , “ active ” , “ healthy ” , “ sleepy ” which are not considered emotion words by psychology .",fact
BJ2J7pFgf_20,"If the authors stated goal is "" different than the standard sentiment analysis goal of predicting whether a sentence expresses positive or negative sentiment "" they should be aware that this is exactly what PANAS is designed to do -",fact
BJ2J7pFgf_21,"not to infer the latent emotional state of a person , except to the extent that their affect is positive or negative .",fact
BJ2J7pFgf_22,The work of representing emotions had been an field in psychology for over a hundred years,fact
BJ2J7pFgf_23,and it is still continuing .,fact
BJ2J7pFgf_24,<URL> .,reference
BJ2J7pFgf_25,"One of the most popular theories of emotion is the theory that there exist “ basic ” emotions : Anger , Disgust , Fear , Happiness ( enjoyment ) , Sadness and Surprise",fact
BJ2J7pFgf_26,"( Paul Ekman , cited by the authors ) .",reference
BJ2J7pFgf_27,These are short duration sates lasting only seconds .,fact
BJ2J7pFgf_28,"They are also fairly specific ,",evaluation
BJ2J7pFgf_29,"for example “ surprise ” is sudden reaction to something unexpected ,",fact
BJ2J7pFgf_30,which is it exactly the same as seeing a flower on your car and expressing “ what a nice surprise . ”,fact
BJ2J7pFgf_31,The surprise would be the initial reaction of “ what ’s that on my car ? Is it dangerous ? ”,evaluation
BJ2J7pFgf_32,"but after identifying the object as non-threatening , the emotion of “ surprise ” would likely pass and be replaced with appreciation .",evaluation
BJ2J7pFgf_33,The Circumplex Model of Emotions ( Posner et al 2005 ) the authors refer to actually stands in opposition to the theories of Ekman .,fact
BJ2J7pFgf_34,From the cited paper by Posner et al :,fact
BJ2J7pFgf_35," The circumplex model of affect proposes that all affective states arise from cognitive interpretations of core neural sensations that are the product of two independent neurophysiological systems . This model stands in contrast to theories of basic emotions , which posit that a discrete and independent neural system subserves every emotion . ",quote
BJ2J7pFgf_36,"From my reading of this paper , it is clear to me that the authors do not have a clear understanding of the current state of psychology ’s view of emotion representation",evaluation
BJ2J7pFgf_37,and this work would not likely contribute to a new understanding of the latent structure of peoples ’ emotions .,evaluation
BJ2J7pFgf_38,"In the PCA result , it is not "" clear "" that the first axis represents valence ,",evaluation
BJ2J7pFgf_39,"as "" sad "" has a slight positive on this scale",fact
BJ2J7pFgf_40,"and "" sad "" is one of the emotions most clearly associated with negative valence .",evaluation
BJ2J7pFgf_41,"With respect to the rest of the paper , the level of novelty and impact is "" ok , but not good enough . """,evaluation
BJ2J7pFgf_42,"This analysis does not seem very different from Twitter analysis ,",evaluation
BJ2J7pFgf_43,"because although Tumblr posts are allowed to be longer than Twitter posts ,",fact
BJ2J7pFgf_44,the authors truncate the posts to 50 characters .,fact
BJ2J7pFgf_45,"Additionally , the images do not seem to add very much to the classification .",evaluation
BJ2J7pFgf_46,"The authors algorithm also seems to be essentially a combination of two other , previously published algorithms .",fact
BJ2J7pFgf_47,"For me the novelty of this paper was in its application to the realm of emotion theory ,",evaluation
BJ2J7pFgf_48,but I do not feel there is a contribution here .,evaluation
BJ2J7pFgf_49,This paper is more about classifying Tumblr posts according to emotion word hashtags than a paper that generates a new insights into emotion representation or that can infer latent emotional state .,evaluation
HypMNiy-G_0,Training GAN in a hierarchical optimization schedule shows promising performance recently ( e.g. <CIT> ) .,evaluation
HypMNiy-G_1,"However , these works utilize the prior knowledge of the data ( e.g. image )",fact
HypMNiy-G_2,and it 's hard to generalize it to other data types ( e.g. text ) .,evaluation
HypMNiy-G_3,The paper aims to learn these hierarchies directly instead of designing by human .,fact
HypMNiy-G_4,"However , several parts are missing and not well-explained .",evaluation
HypMNiy-G_5,"Also , many claims in paper are not proved properly by theory results or empirical results .",evaluation
HypMNiy-G_6,( 1 ) It is not clear to me how to train the proposed algorithm .,evaluation
HypMNiy-G_7,"My understanding is train a simple ALI , then using the learned latent as the input and train the new layer .",evaluation
HypMNiy-G_8,Do the authors use a separate training ? or a joint training algorithms .,non-arg
HypMNiy-G_9,The authors should provide a more clear and rigorous objective function .,request
HypMNiy-G_10,It would be even better to have a pseudo code .,request
HypMNiy-G_11,"( 2 ) In abstract , the authors claim the theoretical results are provided .",fact
HypMNiy-G_12,I am not sure whether it is sec 3.2,evaluation
HypMNiy-G_13,The claims is not clear and limited .,evaluation
HypMNiy-G_14,"For example , what 's the theory statement of [ <CIT> ] .",non-arg
HypMNiy-G_15,What is the error measure used in the paper ?,non-arg
HypMNiy-G_16,"For different error , the matrix concentration bound might be different .",evaluation
HypMNiy-G_17,"Also , the union bound discussed in sec 3.2 is also problematic .",evaluation
HypMNiy-G_18,"Lats , for using simple standard GAN to learn mixture of Gaussian , the rigorous theory result does n't seem easy ( e.g. [1] )",evaluation
HypMNiy-G_19,The author should strive for this results if they want to claim any theory guarantee .,request
HypMNiy-G_20,( 3 ) The experiments part is not complete .,evaluation
HypMNiy-G_21,The experiment settings are not described clearly .,evaluation
HypMNiy-G_22,"Therefore , it is hard to justify whether the proposed algorithm is really useful based on Fig 3 .",evaluation
HypMNiy-G_23,"Also , the authors claims it is applicable to text data in Section 1 , this part is missing in the experiment .",fact
HypMNiy-G_24,"Also , the idea of "" local "" disentangled LV is not well justified to be useful .",evaluation
HypMNiy-G_25,[1] <CIT>,reference
HJ0Hc82gM_0,Review for Deformation of Bregman divergence and its application,non-arg
HJ0Hc82gM_1,Summary : This paper considers parameter estimation for discrete probability models .,fact
HJ0Hc82gM_2,The authors propose an estimator that is computed by minimizing a deformed Bregman divergence .,fact
HJ0Hc82gM_3,"The authors prove that the proposed estimator is more computationally efficient and more robust than the maximal likelihood estimator ( MLE ) , both in theory and simulation .",fact
HJ0Hc82gM_4,"Major Comments : 1 . After the definition 1 , the likelihood <VAR> is defined to be the sum of <VAR> .",fact
HJ0Hc82gM_5,Why the gradient of <VAR> is a related to <VAR> .,non-arg
HJ0Hc82gM_6,"2 . After the equation ( 4 ) , when the authors say <EQN> the authors assume that the first order derivative of <VAR> should be a strictly increasing function",fact
HJ0Hc82gM_7,"( otherwise the inverse function is not well defined , at least in classic notations ) .",evaluation
HJ0Hc82gM_8,I would like to know whether we only need assume the convexity of <VAR> .,non-arg
HJ0Hc82gM_9,Are there other assumptions ?,non-arg
HJ0Hc82gM_10,"3 . In Proposition 1 , I think the “ Fisher consistent ” means that ( 6 ) holds for any reasonable <VAR> and <VAR> just as the authors said before Proposition 1 .",fact
HJ0Hc82gM_11,It is better to add this in the statement of Proposition 1 too .,request
HJ0Hc82gM_12,4 . The “ Proof 1 ” is better to be replaced with “ Proof of Proposition 1 ”,request
HJ0Hc82gM_13,"( same issues for “ Proof 2 ” , “ Proof 3 ” , etc ) .",request
HJ0Hc82gM_14,"5 . In the statement of Theorem 1 , do the authors have any constraint for <VAR> ?",request
HJ0Hc82gM_15,6 . <VAR> appears in Theorem 2 without a clear definition .,evaluation
HJ0Hc82gM_16,"Even if it seems to be defined in ( 17 ) , it is better to be defined again .",request
HJ0Hc82gM_17,7 . Why Theorem 2 indicates that “ the estimator ( 5 ) is not influenced so much by the outlier ” ?,evaluation
HJ0Hc82gM_18,8 . How to solve ( 5 ) ?,request
HJ0Hc82gM_19,Is it trivial ?,request
HJ0Hc82gM_20,I expect to see something like “ We use … algorithm or toolbox to solve ( 5 ) “ .,request
HJ0Hc82gM_21,"Minor Comments : 1 . In Example 2 , I suggest use some more beautiful symbol like <VAR> to denote the transpose instead of <VAR> .",request
HJ0Hc82gM_22,"2 . The length of the equations should not exceed the line-width ( e.g. , ( 4 ) and ( 7 ) ) .",request
HJ0Hc82gM_23,"3 . In page 5 , “ We find some examples satisfying 25 in Theorem 2 ” .",quote
HJ0Hc82gM_24,The “ 25 ” should be “ ( 25 ) ” .,request
SyuPmP3lM_0,The collaborative block that authors propose is a generalized module that can be inserted in deep architectures for better multi-task learning .,fact
SyuPmP3lM_1,The problem is relevant as we are pushing deep networks to learn representation for multiple tasks .,fact
SyuPmP3lM_2,The proposed method while simple is novel .,evaluation
SyuPmP3lM_3,The few places where the paper needs improvement are : 1 . The authors should test their collaborative block on multiple tasks where the tasks are less related .,request
SyuPmP3lM_4,Ex : Scene and object classification .,request
SyuPmP3lM_5,The current datasets where the model is evaluated is limited to Faces which is a constrained setting .,fact
SyuPmP3lM_6,It would be great if Authors provide more experiments beyond Faces to test the universality of the proposed approach .,request
SyuPmP3lM_7,2 . The Face datasets are rather small .,request
SyuPmP3lM_8,I wonder if the accuracy improvements hold on larger datasets and if authors can comment on any large scale experiments they have done using the proposed architecture .,request
SyuPmP3lM_9,In it 's current form I would say the experiment section and large scale experiments are two places where the paper falls short .,evaluation
H1E1RgqxM_0,# Summary This paper presents a new external-memory-based neural network ( Neural Map ) for handling partial observability in reinforcement learning .,fact
H1E1RgqxM_1,The proposed memory architecture is spatially-structured so that the agent can read/write from/to specific positions in the memory .,fact
H1E1RgqxM_2,The results on several memory-related tasks in 2D and 3D environments show that the proposed method outperforms existing baselines such as LSTM and MQN/FRMQN .,fact
H1E1RgqxM_3,[ Pros ] - The overall direction toward more flexible/scalable memory is an important research direction in RL .,evaluation
H1E1RgqxM_4,- The proposed memory architecture is new .,evaluation
H1E1RgqxM_5,- The paper is well-written .,evaluation
H1E1RgqxM_6,[ Cons ] - The proposed memory architecture is new but a bit limited to 2D/3D navigation tasks .,evaluation
H1E1RgqxM_7,- Lack of analysis of the learned memory behavior .,fact
H1E1RgqxM_8,# Novelty and Significance The proposed idea is novel in general .,evaluation
H1E1RgqxM_9,"Though [ Gupta et al. ] proposed an ego-centric neural memory in the RL context ,",fact
H1E1RgqxM_10,"the proposed memory architecture is still new in that read/write operations are flexible enough for the agent to write any information to the memory ,",fact
H1E1RgqxM_11,whereas [ Gupta et al. ] designed the memory specifically for predicting free space .,fact
H1E1RgqxM_12,"On the other hand , the proposed method is also specific to navigation tasks in 2D or 3D environment ,",fact
H1E1RgqxM_13,which is hard to apply to more general memory-related tasks in non-spatial environments .,evaluation
H1E1RgqxM_14,"But , it is still interesting to see that the ego-centric neural memory works well on challenging tasks in a 3D environment .",evaluation
H1E1RgqxM_15,# Quality The experiment does not show any analysis of the learned memory read/write behavior especially for ego-centric neural map and the 3D environment .,fact
H1E1RgqxM_16,It is hard to understand how the agent utilizes the external memory without such an analysis .,evaluation
H1E1RgqxM_17,# Clarity The paper is overall clear and easy-to-follow except for the following .,evaluation
H1E1RgqxM_18,"In the introduction section , the paper claims that "" the expert must set M to a value that is larger than the time horizon of the currently considered task "" when mentioning the limitation of the previous work .",fact
H1E1RgqxM_19,"In some sense , however , Neural Map also requires an expert to specify the proper size of the memory based on prior knowledge about the task .",evaluation
Byz0IGvgz_0,This paper combines the tensor contraction method and the tensor regression method and applies them to CNN .,fact
Byz0IGvgz_1,This paper is well written and easy to read .,evaluation
Byz0IGvgz_2,"However , I can not find a strong or unique contribution from this paper .",evaluation
Byz0IGvgz_3,"Both of the methods ( tensor contraction and tensor decomposition ) are well developed in the existing studies ,",evaluation
Byz0IGvgz_4,and combining these ideas does not seem non-trivial .,evaluation
Byz0IGvgz_5,-- Main question Why authors focus on the combination of the methods ?,evaluation
Byz0IGvgz_6,Both of the two methods can perform independently .,fact
Byz0IGvgz_7,Is there a special synergy effect ?,non-arg
Byz0IGvgz_8,-- Minor question The performance of the tensor contraction method depends on a size of tensors .,fact
Byz0IGvgz_9,Is there any effective way to determine the size of tensors ?,non-arg
r1cczyqef_0,The authors propose a new network architecture for RL that contains some relevant inductive biases about planning .,fact
r1cczyqef_1,This fits into the recent line of work on implicit planning where forms of models are learned to be useful for a prediction/planning task .,evaluation
r1cczyqef_2,The proposed architecture performs something analogous to a full-width tree search using an abstract model ( learned end-to-end ) .,evaluation
r1cczyqef_3,This is done by expanding all possible transitions to a fixed depth before performing a max backup on all expanded nodes .,fact
r1cczyqef_4,"The final backup value is the Q-value prediction for a given state , or can represent a policy through a softmax .",fact
r1cczyqef_5,I thought the paper was clear and well-motivated .,evaluation
r1cczyqef_6,The architecture ( and various associated tricks like state vector normalization ) are well-described for reproducibility .,evaluation
r1cczyqef_7,Experimental results seem promising,evaluation
r1cczyqef_8,but I was n’t fully convinced of its conclusions .,evaluation
r1cczyqef_9,"In both domains , TreeQN and AtreeC are compared to a DQN architecture ,",fact
r1cczyqef_10,but it was n’t clear to me that this is the right baseline .,evaluation
r1cczyqef_11,"Indeed TreeQN and AtreeC share the same conv stack in the encoder ( I think ? ) ,",evaluation
r1cczyqef_12,but also have the extra capacity of the tree on top .,fact
r1cczyqef_13,Can the performance gain we see in the Push task as a function of tree depth be explained by the added network capacity ?,request
r1cczyqef_14,"Same comment in Atari ,",request
r1cczyqef_15,but there it ’s not really obvious that the proposed architecture is helping .,evaluation
r1cczyqef_16,"Baselines could include unsharing the weights in the tree , removing the max backup , having a regular MLP with similar capacity , etc .",request
r1cczyqef_17,"Page 5 , the auxiliary loss on reward prediction seems appropriate ,",evaluation
r1cczyqef_18,but it ’s not clear from the text and experiments whether it actually was necessary .,evaluation
r1cczyqef_19,Is it that makes interpretability of the model easier ( like we see in Fig 5c ) ?,request
r1cczyqef_20,Or does it actually lead to better performance ?,request
r1cczyqef_21,"Despite some shortcomings in the result section , I believe this is good work and worth communicating as is .",evaluation
rynqOnBez_0,"My problem with this paper that all the theoretical contributions / the new approach refer to 2 arXiv papers ,",evaluation
rynqOnBez_1,what 's then left is an application of that approach to learning form imperfect demonstrations .,fact
rynqOnBez_2,Quality ====== The approach seems sound,evaluation
rynqOnBez_3,but the paper does not provide many details on the underlying approach .,evaluation
rynqOnBez_4,The application to learning from ( partially adversarial ) demonstrations is a cool idea,evaluation
rynqOnBez_5,but effectively is a very straightforward application based on the insight that the approach can handle truly off-policy samples .,fact
rynqOnBez_6,The experiments are OK,evaluation
rynqOnBez_7,but I would have liked a more thorough analysis .,request
rynqOnBez_8,"Clarity ===== The paper reads well ,",evaluation
rynqOnBez_9,but it is not really clear what the claimed contribution is .,evaluation
rynqOnBez_10,Originality ========= The application seems original .,evaluation
rynqOnBez_11,Significance ========== Having an RL approach that can benefit from truly off-policy samples is highly relevant .,evaluation
rynqOnBez_12,Pros and Cons ============ + good results,evaluation
rynqOnBez_13,+ interesting idea of using the algorithm for RLfD,evaluation
rynqOnBez_14,- weak experiments for an application paper,evaluation
rynqOnBez_15,- not clear what 's new,evaluation
BkcUX-5eG_0,This paper investigates learning representations for the problem of nearest neighbor ( NN ) search by exploring various deep learning architectural choices .,fact
BkcUX-5eG_1,The crux of the paper is the connection between NN and the angles between the closest neighbors --,evaluation
BkcUX-5eG_2,"the higher this angle , more data points need to be explored for finding the nearest one , and thus more computational expense .",fact
BkcUX-5eG_3,"Thus , the paper proposes to learn a network that tries to reduce the angles between the inputs and the corresponding class vectors in a supervised framework using softmax cross-entropy loss .",fact
BkcUX-5eG_4,"Three architectural choices are investigated ,",fact
BkcUX-5eG_5,"( i ) controlling the norm of output layers of the CNN ( using batch norm essentially ) ,",fact
BkcUX-5eG_6,"( ii ) removing relu so that the outputs are well-distributed in both positive and negative orthants ,",fact
BkcUX-5eG_7,and ( iii ) normalizing the class vectors .,fact
BkcUX-5eG_8,Experiments are given on multiMNIST and Sports 1M and show improvements .,fact
BkcUX-5eG_9,Pros : 1 ) The paper explores different architectural choices for the deep network to some depth and show extensive results .,evaluation
BkcUX-5eG_10,2 ) The results do demonstrate clearly the advantage of the various choices and is useful,evaluation
BkcUX-5eG_11,"3 ) The theoretical connections between data angles and query times are quite interesting ,",evaluation
BkcUX-5eG_12,Cons : 1 ) Unclear Problem Statement .,evaluation
BkcUX-5eG_13,I find the problem statement a bit vague .,evaluation
BkcUX-5eG_14,Standard NN search finds a data point in the database closest to a query under some distance metric .,fact
BkcUX-5eG_15,"While , the current paper uses the cosine similarity as the distance , the deep framework is trained on class vectors using cross-entropy loss .",fact
BkcUX-5eG_16,"I do not think class labels are usually assumed to be given in the standard definition of NN ,",fact
BkcUX-5eG_17,and it is not clear to me how the proposed setup can accommodate NN without class labels .,evaluation
BkcUX-5eG_18,"Thus as such , I see this paper is perhaps proposing a classification problem and not an NN problem per se .",fact
BkcUX-5eG_19,2 ) Lacks Focus The paper lacks a good organization in my opinion .,evaluation
BkcUX-5eG_20,Things that are perhaps technically important are moved to the Appendix .,fact
BkcUX-5eG_21,"For example , I find the theoretical part of the paper ( e.g. , Theorem 1 ) quite elegant and perhaps the main innovation in this paper .",evaluation
BkcUX-5eG_22,"However , that is moved completely to the Appendix .",fact
BkcUX-5eG_23,So it can not be really considered a contribution .,evaluation
BkcUX-5eG_24,It is also not clear if those theoretical results are novel .,evaluation
BkcUX-5eG_25,2 ) Disconnect/Unclear Assumptions There seems to be some disconnect between LSH and deep learning architectures explored in Sections 2 and 3 respectively .,evaluation
BkcUX-5eG_26,Are the assumptions used in the theoretical results for LSH also assumed in the deep networks ?,request
BkcUX-5eG_27,"For example , as far as I know , the standard LSH works assumes the projection hyperplanes are randomly chosen and the theoretical results are based on such assumptions .",fact
BkcUX-5eG_28,"It is not clear how a softmax output of a CNN , which is trained in a supervised way , follow such assumptions .",evaluation
BkcUX-5eG_29,It would be important if the paper could clarify such assumptions to make sure the sections are congruent .,request
BkcUX-5eG_30,3 ) No Related Work,fact
BkcUX-5eG_31,There have been several efforts for adapting deep frameworks into KNN .,fact
BkcUX-5eG_32,The paper ignores all such works .,fact
BkcUX-5eG_33,"Thus , it is not clear how significant is the proposed contribution .",evaluation
BkcUX-5eG_34,There are also not comparisons what-so-ever to competitive prior works .,fact
BkcUX-5eG_35,4 ) Novelty The main contribution of this paper is basically a set of experiments looking into architectural choices .,fact
BkcUX-5eG_36,"However , the results of this study do not provide any surprises .",evaluation
BkcUX-5eG_37,"It appears that batch normalization is essential for good performances ,",fact
BkcUX-5eG_38,while using RELU is not so when one wants to use all directions for effective data encoding .,fact
BkcUX-5eG_39,"Thus , as such , the novelty or the contributions of this paper are minor .",evaluation
BkcUX-5eG_40,"Overall , while I find there are some interesting theoretical bits in this paper ,",evaluation
BkcUX-5eG_41,"it lacks focus ,",evaluation
BkcUX-5eG_42,"the experiments do not offer any surprises ,",evaluation
BkcUX-5eG_43,and there are no comparisons with prior literature .,fact
BkcUX-5eG_44,"Thus , I do not think this paper is ready to be accepted in its present form .",evaluation
SkWQLvebf_0,This paper proposes a deep learning ( DL ) approach ( pre-trained CNNs ) to the analysis of histopathological images for disease localization .,fact
SkWQLvebf_1,"It correctly identifies the problem that DL usually requires large image databases to provide competitive results ,",fact
SkWQLvebf_2,while annotated histopathological data repositories are costly to produce and not on that size scale .,fact
SkWQLvebf_3,It also correctly identifies that this is a daunting task for human medical experts,fact
SkWQLvebf_4,and therefore one that could surely benefit from the use of automated methods like the ones proposed .,fact
SkWQLvebf_5,The study seems sound from a technical viewpoint to me,evaluation
SkWQLvebf_6,"and its contribution is incremental , as it builds on existing research ,",evaluation
SkWQLvebf_7,which is correctly identified .,fact
SkWQLvebf_8,"Results are not always too impressive ,",evaluation
SkWQLvebf_9,but authors seem intent on making them useful for pathogists in practice,evaluation
SkWQLvebf_10,( an intention that is always worth the effort ) .,evaluation
SkWQLvebf_11,I think the paper would benefit from a more explicit statement of its original contributions ( against contextual published research ),request
SkWQLvebf_12,Minor issues : Revise typos ( e.g. title of section 2 ),request
SkWQLvebf_13,Please revise list of references,request
SkWQLvebf_14,"( right now a mess in terms of format , typos , incompleteness",evaluation
ByhgguzeM_0,The paper presents a method to parametrize unitary matrices in an RNN as a Kronecker product of smaller matrices .,fact
ByhgguzeM_1,"Given N inputs and output , this method allows one to specify a linear transformation with <VAR> parameters , and perform a forward and backward pass in <VAR> time .",fact
ByhgguzeM_2,In addition a relaxation is performed allowing each constituent to deviate a bit from unitarity ( “ soft unitary constraint ” ) .,fact
ByhgguzeM_3,The paper shows nice results on a number of small tasks .,evaluation
ByhgguzeM_4,The idea is original to the best of my knowledge and is presented clearly .,evaluation
ByhgguzeM_5,I especially like the idea of “ soft unitary constraint ” which can be applied very efficiently in this factorized setup .,evaluation
ByhgguzeM_6,I think this is the main contribution of this work .,evaluation
ByhgguzeM_7,However the paper in its current form has a number of problems :,fact
ByhgguzeM_8,- The authors state that a major shortcoming of previous ( efficient ) unitary RNN methods is the lack of ability to span the entire space of unitary matrices .,fact
ByhgguzeM_9,"This method presents a family that can span the entire space , but the efficient parts of this family ( which give the promised speedup ) only span a tiny fraction of it ,",fact
ByhgguzeM_10,as they require only <VAR> params to specify an <VAR> unitary matrix .,fact
ByhgguzeM_11,Indeed in the experimental section only those members are tested .,fact
ByhgguzeM_12,"- Another claim that is made is that complex numbers are key , and again the argument is the need to span the entire space of unitary matrices ,",fact
ByhgguzeM_13,"but the same comment still hold - that is not the space this work is really dealing with ,",fact
ByhgguzeM_14,and no experimental evidence is provided that using complex numbers was really needed .,fact
ByhgguzeM_15,"- In the experimental section an emphasis is made as to how small the number of recurrent params are ,",fact
ByhgguzeM_16,"but at the same time the input/output projections are very large , leaving the reader wondering if the workload simply shifted from the RNN to the projections .",evaluation
ByhgguzeM_17,This needs to be addressed .,request
ByhgguzeM_18,- Another aspect of the previous points is that it ’s not clear if stacking KRU layers will work well .,evaluation
ByhgguzeM_19,This is important,evaluation
ByhgguzeM_20,as stacking LSTMs is a common practice .,evaluation
ByhgguzeM_21,Efficient KRU span a restricted subspace whose elements might not compose into structures that are expressive enough .,fact
ByhgguzeM_22,"One way to overcome this potential problem is to add projection matrices between layers that will do some mixing ,",fact
ByhgguzeM_23,but this will blow the number of parameters .,fact
ByhgguzeM_24,This needs to be explored .,request
ByhgguzeM_25,"- The authors claim that the soft unitary constraint was key for the success of the network ,",fact
ByhgguzeM_26,"yet no details are provided as to how this constraint was applied ,",fact
ByhgguzeM_27,and no analysis was made for its significance .,fact
HyWGBr5lf_0,Summary : The authors proposed an unsupervised time series clustering methods built with deep neural networks .,fact
HyWGBr5lf_1,The proposed model is equipped with an encoder-decoder and a clustering model .,fact
HyWGBr5lf_2,"First , the encoder employs CNN to shorten the time series and extract local temporal features ,",fact
HyWGBr5lf_3,and the CNN is followed by bidirectional LSTMs to get the encoded representations .,fact
HyWGBr5lf_4,A temporal clustering model and a DCNN decoder are applied on the encoded representations and jointly trained .,fact
HyWGBr5lf_5,An additional heatmap generator component can be further included in the clustering model .,fact
HyWGBr5lf_6,The authors compared the proposed method with hierarchical clustering with 4 different temporal similarity methods on several univariate time series datasets .,fact
HyWGBr5lf_7,Detailed comments : The problem of unsupervised time series clustering is important and challenging .,evaluation
HyWGBr5lf_8,The idea of utilizing deep learning models to learn encoded representations for clustering is interesting and could be a promising solution .,evaluation
HyWGBr5lf_9,"One potential limitation of the proposed method is that it is only designed for univariate time series of the same temporal length ,",fact
HyWGBr5lf_10,which limits the usage of this model in practice .,fact
HyWGBr5lf_11,"In addition , given that the input has fixed length , clustering baselines for static data can be easily applied",evaluation
HyWGBr5lf_12,and should be compared to demonstrate the necessity of temporal clustering .,request
HyWGBr5lf_13,Some important details are missing or lack of explanations .,evaluation
HyWGBr5lf_14,"For example , what is the size of each layer and the dimension of the encoded space ?",non-arg
HyWGBr5lf_15,How much does the model shorten the input time series and how is this be determined ?,non-arg
HyWGBr5lf_16,How does the model combine the heatmap output ( which is a sequence of the same length as the time series ) and the clustering output ( which is a vector of size K ) in Figure 1 ?,non-arg
HyWGBr5lf_17,"The heatmap shown in Figure 3 looks like the negation of the decoded output ( i.e. , lower value in time series - > higher value in heatmap ) .",evaluation
HyWGBr5lf_18,How do we interpret the generated heatmap ?,non-arg
HyWGBr5lf_19,"From the experimental results , it is difficult to judge which method/metric is the best .",evaluation
HyWGBr5lf_20,"For example , in Figure 4 , all 4 DTC-methods achieved the best performance on one or two datasets .",fact
HyWGBr5lf_21,"Though several datasets are evaluated in experiments , they are relatively small .",evaluation
HyWGBr5lf_22,"Even the largest dataset ( Phalanges OutlinesCorrect ) has only 2 thousand samples ,",fact
HyWGBr5lf_23,"and the best performance is achieved by one of the baseline , with AUC score only 0.586 for binary classification .",fact
HyWGBr5lf_24,"Minor suggestion : In Figure 3 , instead of showing the decoded output ( reconstruction ) , it may be more helpful to visualize the encoded time series",request
HyWGBr5lf_25,since the clustering method is applied directly on those encoded representations .,fact
SkSMlWcgG_0,The paper provides methods for training deep networks using half-precision floating point numbers without losing model accuracy or changing the model hyper-parameters .,fact
SkSMlWcgG_1,"The main ideas are to use a master copy of weights when updating the weights , scaling the loss before back-prop and using full precision variables to store products .",evaluation
SkSMlWcgG_2,"Experiments are performed on a large number of state-of-art deep networks , tasks and datasets",evaluation
SkSMlWcgG_3,which show that the proposed mixed precision training does provide the same accuracy at half the memory .,fact
SkSMlWcgG_4,"Positives - The experimental evaluation is fairly exhaustive on a large number of deep networks , tasks and datasets",evaluation
SkSMlWcgG_5,and the proposed training preserves the accuracy of all the tested networks at half the memory cost .,fact
SkSMlWcgG_6,Negatives - The overall technical contribution is fairly small and are ideas that are regularly implemented when optimizing systems .,evaluation
SkSMlWcgG_7,- The overall advantage is only a 2x reduction in memory which can be gained by using smaller batches at the cost of extra compute .,fact
B104VQCgM_0,"The paper basically propose keep using the typical data-augmentation transformations done during training also in evaluation time , to prevent adversarial attacks .",fact
B104VQCgM_1,"In the paper they analyze only 2 random resizing and random padding ,",fact
B104VQCgM_2,"but I suppose others like random contrast , random relighting , random colorization , ... could be applicable .",evaluation
B104VQCgM_3,"Some of the pros of the proposed tricks is that it does n't require re-training existing models ,",evaluation
B104VQCgM_4,although as the authors pointed out re-training for adversarial images is necessary to obtain good results .,fact
B104VQCgM_5,Typically images have different sizes,fact
B104VQCgM_6,", however in the Dataset are described as having 299x299x3 size ,",fact
B104VQCgM_7,are all the test images resized before hand ?,non-arg
B104VQCgM_8,How would this method work with variable size images ?,non-arg
B104VQCgM_9,"The proposed defense requires increasing the size of the input images ,",fact
B104VQCgM_10,have you analyzed the impact in performance ?,non-arg
B104VQCgM_11,Also it would be good to know how robust is the method for smaller sizes .,request
B104VQCgM_12,"Section 4.6.2 seems to indicate that 1 pixel padding or just resizing 1 pixel is enough to get most of the benefit ,",evaluation
B104VQCgM_13,please provide an analysis of how results improve as the padding or size increase .,request
B104VQCgM_14,"In section 5 for the challenge authors used a lot more evaluations per image ,",evaluation
B104VQCgM_15,could you provide how much extra computation is needed for that model ?,request
ry2OdYCeM_0,Paper presents an interesting attention mechanism for fine-grained image classification .,evaluation
ry2OdYCeM_1,Introduction states that the method is simple and easy to understand .,fact
ry2OdYCeM_2,"However , the presentation of the method is bit harder to follow .",evaluation
ry2OdYCeM_3,It is not clear to me if the attention modules are applied over all pooling layers .,evaluation
ry2OdYCeM_4,How they are combined ?,request
ry2OdYCeM_5,Why use cross - correlation as the regulariser ?,request
ry2OdYCeM_6,Why not much stronger constraint such as orthogonality over elements of M in equation 1 ?,request
ry2OdYCeM_7,What is the impact of this regularisation ?,request
ry2OdYCeM_8,Why use soft-max in equation 1 ?,request
ry2OdYCeM_9,One may use a Sigmoid as well ?,non-arg
ry2OdYCeM_10,Is it better to use soft-max ?,non-arg
ry2OdYCeM_11,Equation 9 is not entirely clear to me .,evaluation
ry2OdYCeM_12,Undefined notations .,fact
ry2OdYCeM_13,"In Table 2 , why stop from AD = 2 and AW = 2 ?",request
ry2OdYCeM_14,"What is the performance of AD = 1 , AW = 1 with G ?",request
ry2OdYCeM_15,Why not perform this experiment over all 5 datasets ?,request
ry2OdYCeM_16,"Is this performances , dataset specific ?",request
ry2OdYCeM_17,The method is compared against 5 datasets .,fact
ry2OdYCeM_18,Obtained results are quite good .,evaluation
B1P-gBclf_0,"The quality of the paper is good , and clarity is mostly good .",evaluation
B1P-gBclf_1,"The proposed metric is interesting ,",evaluation
B1P-gBclf_2,but it is hard to judge the significance without more thorough experiments demonstrating that it works in practice .,evaluation
B1P-gBclf_3,Pros : - clear definitions of terms,evaluation
B1P-gBclf_4,- overall outline of paper is good,evaluation
B1P-gBclf_5,- novel metric,evaluation
B1P-gBclf_6,"Cons - text is a bit over-wordy , and flow/meaning sometimes get lost .",evaluation
B1P-gBclf_7,"A strict editor would be helpful ,",request
B1P-gBclf_8,because the underlying content is good,evaluation
B1P-gBclf_9,"- odd that your definition of generalization in GANs appears immediately preceding the section titled "" Generalisation in GANs """,evaluation
B1P-gBclf_10,"- the paragraph at the end of the "" Generalisation in GANs "" section is confusing .",evaluation
B1P-gBclf_11,"I think this section and the previous ( "" The objective of unsupervised learning "" ) could be combined , removing some repetition , adding some subtitles to improve clarity .",request
B1P-gBclf_12,This would cut down the text a bit to make space for more experiments .,evaluation
B1P-gBclf_13,- why is your definition of generalization that the test set distance is strictly less than training set ?,non-arg
B1P-gBclf_14,I would think this should be less-than-or-equal,evaluation
B1P-gBclf_15,"- there is a sentence that does n't end at the top of p. 3 : "" ... the original GAN paper showed that [ ends here ] """,fact
B1P-gBclf_16,"- should state in the abstract what your "" notion of generalization "" for gans is , instead of being vague about it",request
B1P-gBclf_17,"- more experiments showing a comparison of the proposed metric to others ( e.g. inception score , Mturk assessments of sample quality , etc. ) would be necessary to find the metric convincing",request
B1P-gBclf_18,"- what is a "" pushforward measure "" ? ( p. 2 )",non-arg
B1P-gBclf_19,"- the related work section is well-written and interesting ,",evaluation
B1P-gBclf_20,but it 's a bit odd to have it at the end .,evaluation
B1P-gBclf_21,Earlier in the work ( e.g. before experiments and discussion ) would allow the comparison with MMD to inform the context of the introduction,request
B1P-gBclf_22,- there are some errors in figures that I think were all mentioned by previous commentators .,evaluation
BJCXSFZgz_0,This paper proposes leveraging labelled controlled data to accelerate reinforcement-based learning of a control policy .,fact
BJCXSFZgz_1,It provides two main contributions : pre-training the policy network of a DDPG agent in a supervised manner so that it begins in reasonable state-action distribution and regalurizing the Q-updates of the q-network to be biased towards existing actions .,evaluation
BJCXSFZgz_2,The authors use the TORCS enviroment to demonstrate the performance of their method both in final cumulative return of the policy and speed of learning .,fact
BJCXSFZgz_3,This paper is easy to understand but has a couple shortcomings and some fatal ( but reparable ) flaws : .,evaluation
BJCXSFZgz_4,"1 ) When using RL please try to standardize your notation to that used by the community ,",request
BJCXSFZgz_5,it makes things much easier to read .,evaluation
BJCXSFZgz_6,I would strongly suggest avoiding your notation <VAR> and using <VAR>,request
BJCXSFZgz_7,( subscripting theta or making conditional is somewhat less important ) .,evaluation
BJCXSFZgz_8,"Your a(.) function seems to be the policy here ,",evaluation
BJCXSFZgz_9,which is invariable denoted \ pi in the RL literature .,fact
BJCXSFZgz_10,There has been recent effort to clean up RL notation which is presented here :,fact
BJCXSFZgz_11,<URL> .,reference
BJCXSFZgz_12,You have no obligation to use this notation but it does make reading of your paper much easier on others in the community .,request
BJCXSFZgz_13,This is more of a shortcoming than a fundamental issue .,evaluation
BJCXSFZgz_14,"2 ) More fatally , you have failed to compare your algorithm 's performance against benchline implementations of similar algorithms .",evaluation
BJCXSFZgz_15,It is almost trivial to run DDPG on Torcs using the openAI baselines package,evaluation
BJCXSFZgz_16,[ <URL> ] .,reference
BJCXSFZgz_17,"I would have loved , for example , to see the effects of simply pre-training the DDPG actor on supervised data , vs. adding your mixture loss on the critic .",request
BJCXSFZgz_18,"Using the baselines would have ( maybe ) made a very compelling graph showing DDPG , DDPG + actor pre-training , and then your complete method .",evaluation
BJCXSFZgz_19,"3 ) And finally , perhaps complementary to point 2 ) , you really need to provide examples on more than one environment .",request
BJCXSFZgz_20,"Each of these simulated environments has its own pathologies linked to determenism , reward structure , and other environment particularities .",fact
BJCXSFZgz_21,"Almost every algorithm I 've seen published will often beat baselines on one environment and then fail to improve or even be wors on others ,",evaluation
BJCXSFZgz_22,so it is important to at least run on a series of these .,request
BJCXSFZgz_23,Mujoco + AI Gym should make this really easy to do,evaluation
BJCXSFZgz_24,"( for reference , I have no relatinship with OpenAI ) .",reference
BJCXSFZgz_25,"Running at least cartpole ( which is a very well understood control task ) , and then perhaps reacher , swimmer , half-cheetah etc. using a known contoller as your behavior policy ( behavior policy is a good term for your data-generating policy . )",request
BJCXSFZgz_26,"4 ) In terms of state of the art you are very close to Todd Hester et . al 's paper on imitation learning ,",evaluation
BJCXSFZgz_27,"and although you cite it , you should contrast your approach more clearly with the one in that paper .",request
BJCXSFZgz_28,"Please also have a look at some more recent work my Matej Vecerik , Todd Hester & Jon Scholz : ' Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards ' for an approach that is pretty similar to yours .",request
BJCXSFZgz_29,"Overall I think your intuitions and ideas are good ,",evaluation
BJCXSFZgz_30,but the paper does not do a good enough job justifying empirically that your approach provides any advantages over existing methods .,evaluation
BJCXSFZgz_31,The idea of pre-training the policy net has been tried before,fact
BJCXSFZgz_32,( although I ca n't find a published reference ),evaluation
BJCXSFZgz_33,"and in my experience will help on certain problems , and hinder on others ,",non-arg
BJCXSFZgz_34,"primarily because the policy network is already ' overfit ' somewhat to the expert , and may have a hard time moving to a more optimal space .",evaluation
BJCXSFZgz_35,Because of this experience I would need more supporting evidence that your method actually generalizes to more than one RL environment .,request
rk156h2gf_0,The manuscript proposes a new framework for inference in RNN based upon the Bayes by Backprop ( BBB ) algorithm .,fact
rk156h2gf_1,"In particular , the authors propose a new framework to "" sharpen "" the posterior .",evaluation
rk156h2gf_2,"In particular , the hierarchical prior in ( 6 ) and ( 7 ) frame an interesting modification to directly learning a multivariate normal variational approximation .",evaluation
rk156h2gf_3,"In the experimental results , it seems clear that this approach is beneficial , but it 's not clear as to why .",evaluation
rk156h2gf_4,"In particular , how does the variational posterior change as a result of the hierarchical prior ?",evaluation
rk156h2gf_5,It seems that ( 7 ) would push the center of the variational structure back towards the MAP point and reduces the variance of the output of the hierarchical prior ;,evaluation
rk156h2gf_6,"however , with the two layers in the prior it 's unclear what actually is happening .",evaluation
rk156h2gf_7,Carefully explaining * what * the authors believe is happening and exploring how it changes the variational approximation in a classic modeling framework would be beneficial to understanding the proposed change and evaluating it .,request
rk156h2gf_8,"As a final point , the authors state , "" as long as the improvement along the gradient is great than the KL loss incurred ... this method is guaranteed to make progress towards optimizing L. """,quote
rk156h2gf_9,Do the authors mean that the negative log-likelihood will be improved in this case ?,request
rk156h2gf_10,Or the actual optimization ?,request
rk156h2gf_11,"Improving the negative log-likelihood seems straightforward ,",evaluation
rk156h2gf_12,but I am confused by what the authors mean by optimization .,evaluation
rk156h2gf_13,"The new evaluation metric proposed in Section 6.1.1 is confusing ,",evaluation
rk156h2gf_14,and I do not understand what the metric is trying to capture .,evaluation
rk156h2gf_15,This needs significantly more detail and explanation .,request
rk156h2gf_16,"Also , it is unclear to me what would happen when you input data examples that are opposite to the original input sequence ;",evaluation
rk156h2gf_17,"in particular , for many neural networks the predictions are unstable outside of the input domain and inputting infeasible data leads to unusable outputs .",evaluation
rk156h2gf_18,"It 's completely feasible that these outputs would just be highly uncertain ,",evaluation
rk156h2gf_19,and I 'm not sure how you can ascribe meaning to them .,evaluation
rk156h2gf_20,The authors should not compare to the uniform prior as a baseline for entropy .,request
rk156h2gf_21,It 's much more revealing to compare it to the empirical likelihoods of the words .,request
SJs7uYYeM_0,"At the heart of the paper , there is a single idea : to decouple the weight decay from the number of steps taken by the optimization process ( the paragraph at the end of page 2 is the key to the paper ) .",fact
SJs7uYYeM_1,This is an important and largely overlooked area of implementation,evaluation
SJs7uYYeM_2,"and most off-the-shelf optimization algorithms , unfortunately , miss this point , too .",evaluation
SJs7uYYeM_3,"I think that the proposed implementation should be taken seriously , especially in conjunction with the discussion that has been carried out with the work of Wilson et al. , 2017",evaluation
SJs7uYYeM_4,( <URL> ) .,reference
SJs7uYYeM_5,The introduction does a decent job explaining why it is necessary to pay attention to the norm of the weights as the training progresses within its scope .,evaluation
SJs7uYYeM_6,"However , I would like to add a couple more points to the discussion : - "" Optimal weight decay is a function ( among other things ) of the total number of epochs / batch passes . """,quote
SJs7uYYeM_7,"in principle , it is a function of weight updates .",fact
SJs7uYYeM_8,"Clearly , it depends on the way the decay process is scheduled .",fact
SJs7uYYeM_9,"However , there is a bad habit in DL where time is scaled by the number of epochs rather than the number of weight updates which sometimes lead to misleading plots ( for instance , when comparing two algorithms with different batch sizes ) .",evaluation
SJs7uYYeM_10,- Another ICLR 2018 submission has an interesting take on the norm of the weights and the algorithm,evaluation
SJs7uYYeM_11,( <URL> ) .,reference
SJs7uYYeM_12,"Figure 3 shows the histograms of SGD/ADAM with and without WD ( the * un-fixed * version ) ,",fact
SJs7uYYeM_13,and it clearly shows how the landscape appear misleadingly different when one does n't pay attention to the weight distribution in visualizations .,evaluation
SJs7uYYeM_14,"- In figure 2 , it appears that the training process has three phases , an initial decay , a steady progress , and a final decay that is more pronounced in AdamW .",fact
SJs7uYYeM_15,This final decay also correlates with the better test error of the proposed method .,fact
SJs7uYYeM_16,This third part also seems to correspond to the difference between Adam and AdamW through the way they branch out after following similar curves .,fact
SJs7uYYeM_17,One wonders what causes this branching and whether the key the desired effects are observed at the bottom of the landscape .,request
SJs7uYYeM_18,"- The paper concludes with "" Advani & Saxe ( 2017 ) analytically showed that in the limited data regime of deep networks the presence of eigenvalues that are zero forms a frozen subspace in which no learning occurs and thus smaller ( e.g. , zero ) initial weight norms should be used to achieve best generalization results . """,fact
SJs7uYYeM_19,Related to this there is another ICLR 2018 submission,fact
SJs7uYYeM_20,"( <URL> ) ,",reference
SJs7uYYeM_21,"figure 1 shows that the eigenvalues of the Hessian of the loss have zero forms at the bottom of the landscape , not at the beginning .",fact
SJs7uYYeM_22,"Back to the previous point , maybe that discussion should focus on the second and third phases of the training , not the beginning .",request
SJs7uYYeM_23,"- Finally , it would also be interesting to discuss the relation of the behavior of the weights at the last parts of the training and its connection to pruning .",request
SJs7uYYeM_24,I 'm aware that one can easily go beyond the scope of the paper by adding more material .,evaluation
SJs7uYYeM_25,"Therefore , it is not completely reasonable to expect all such possible discussions to take place at once .",evaluation
SJs7uYYeM_26,The paper as it stands is reasonably self-contained and to the point .,evaluation
SJs7uYYeM_27,Just a minor last point that is irrelevant to the content of the work : The slash punctuation mark that is used to indicate ' or ' should be used without spaces as in ' epochs/batch ' .,request
HJ_m58weG_0,This paper proposes to use neural network and gradient descent to automatically design for engineering tasks .,fact
HJ_m58weG_1,"It uses two networks , parameterization network and prediction network to model the mapping from design parameters to fitness .",fact
HJ_m58weG_2,It uses back propagation ( gradient descent ) to improve the design .,fact
HJ_m58weG_3,The method is evaluated on heat sink design and airfoil design .,fact
HJ_m58weG_4,This paper targets at a potentially very useful application of neural networks that can have real world impacts .,evaluation
HJ_m58weG_5,"However , I have three main concerns : 1 ) Presentation . The organization of the paper could be improved .",request
HJ_m58weG_6,"It mixes the method , the heat sink example and the airfoil example throughout the entire paper .",fact
HJ_m58weG_7,Sometimes I am very confused about what is being described .,evaluation
HJ_m58weG_8,My suggestion would be to completely separate these three parts :,request
HJ_m58weG_9,"present a general method first ,",request
HJ_m58weG_10,then use heat sink as the first experiment and airfoil as the second experiment .,request
HJ_m58weG_11,This organization would make the writing much clearer .,evaluation
HJ_m58weG_12,"2 ) In the paragraph above Section 4.1 , the paper made two arguments .",fact
HJ_m58weG_13,"I might be wrong , but I do not agree with either of them in general .",evaluation
HJ_m58weG_14,"First of all , "" neural networks are good at generalizing to examples outside their train set "" .",quote
HJ_m58weG_15,This depends entirely on whether the sample distribution of training and testing are similar and whether you have enough training examples that cover important sample space .,evaluation
HJ_m58weG_16,This is especially critical if a deep neural network is used since overfitting is a real issue .,evaluation
HJ_m58weG_17,"Second , "" it is easy to imagine a hybrid system where a network is trained on a simulation and fine tuned ... "" .",quote
HJ_m58weG_18,Implementing such a hybrid system is nontrivial due to the reality gap .,evaluation
HJ_m58weG_19,There is an entire research field about closing the reality gap and transfer learning .,fact
HJ_m58weG_20,So I am not convinced by these two arguments made by this paper .,evaluation
HJ_m58weG_21,They might be true for a narrow field of application .,evaluation
HJ_m58weG_22,"But in general , I think they are not quite correct .",evaluation
HJ_m58weG_23,3 ) The key of this paper is to approximate the dynamics using neural network ( which is a continuous mapping ) and take advantage of its gradient computation .,fact
HJ_m58weG_24,"However , many of dynamic systems are inherently discontinuous ( collision/contact dynamics ) or chaotic ( turbulent flow ) .",fact
HJ_m58weG_25,"In those scenarios , the proposed method might not work well and we may have to resort to the gradient free methods .",evaluation
HJ_m58weG_26,"It seems that the proposed method works well for heat sink problem and the steady flow around airfoil ,",fact
HJ_m58weG_27,both of which do not fall into the more complex physics regime .,fact
HJ_m58weG_28,It would be great that the paper could be more explicit about its limitations .,request
HJ_m58weG_29,"In summary , I like the idea , the application and the result of this paper .",evaluation
HJ_m58weG_30,The writing could be improved .,request
HJ_m58weG_31,"But more importantly , I think that the proposed method has its limitation about what kind of physical systems it can model .",evaluation
HJ_m58weG_32,These limitation should be discussed more explicitly and more thoroughly .,request
Hk96V1clf_0,"This paper generates adversarial examples using the fast gradient sign ( FGS ) and iterated fast gradient sign ( IFGS ) methods , but replacing the gradient computation with finite differences or another gradient approximation method .",fact
Hk96V1clf_1,"Since finite differences is expensive in high dimensions ,",fact
Hk96V1clf_2,the authors propose using directional derivatives based on random feature groupings or PCA .,fact
Hk96V1clf_3,This paper would be much stronger if it surveyed a wider variety of gradient-free optimization methods .,evaluation
Hk96V1clf_4,"Notably , there 's two important black-box optimization baselines that were not included :",fact
Hk96V1clf_5,"simultaneous perturbation stochastic approximation ( https://en.wikipedia.org/wiki/Simultaneous_perturbation_stochastic_approximation ) , which avoids computing the gradient explicitly , and evolutionary strategies ( https://blog.openai.com/evolution-strategies/ ) , a similar method that uses several random directions to estimate a better descent direction .",fact
Hk96V1clf_6,The gradient approximation methods proposed in this paper may or may not be better than SPSA or ES .,evaluation
Hk96V1clf_7,"Without a direct comparison , it 's hard to know .",evaluation
Hk96V1clf_8,"Thus , the main contribution of this paper is in demonstrating that gradient approximation methods are sufficient for generating good adversarial attacks and applying those attacks to Clarifai models .",evaluation
Hk96V1clf_9,"That 's interesting and useful to know , but is still a relatively small contribution , making this paper borderline .",evaluation
Hk96V1clf_10,"I lean towards rejection ,",evaluation
Hk96V1clf_11,since the paper proposes new methods without comparing to or even mentioning well-known alternatives .,fact
HJ2pirpxG_0,This paper considers the problem of improving sequence generation by learning better metrics .,fact
HJ2pirpxG_1,"Specifically , it focuses on addressing the exposure bias problem , where traditional methods such as SeqGAN uses GAN framework and reinforcement learning .",fact
HJ2pirpxG_2,"Different from these work , this paper does not use GAN framework .",fact
HJ2pirpxG_3,"Instead , it proposed an expert-based reward function training , which trains the reward function ( the discriminator ) from data that are generated by randomly modifying parts of the expert trajectories .",fact
HJ2pirpxG_4,"Furthermore , it also introduces partial reward function that measures the quality of the subsequences of different lengths in the generated data .",fact
HJ2pirpxG_5,"This is similar to the idea of hierarchical RL , which divide the problem into potential subtasks , which could alleviate the difficulty of reinforcement learning from sparse rewards .",evaluation
HJ2pirpxG_6,The idea of the paper is novel .,evaluation
HJ2pirpxG_7,"However , there are a few points to be clarified .",evaluation
HJ2pirpxG_8,"In Section 3.2 and in ( 4 ) and ( 5 ) , the authors explains how the action value <VAR> is modeled and estimated for the partial reward function <VAR> of length <VAR> .",fact
HJ2pirpxG_9,But the authors do not explain how the rewards ( or action value functions ) of different lengths are aggregated together to update the model using policy gradient .,fact
HJ2pirpxG_10,Is it a simple sum of all of them ?,non-arg
HJ2pirpxG_11,It is not clear why the future subsequences that do not contain <VAR> are ignored for estimating the action value function Q in ( 4 ) and ( 5 ) .,evaluation
HJ2pirpxG_12,The authors stated that it is for reducing the computation complexity .,fact
HJ2pirpxG_13,But it is not clear why specifically dropping the sequences that do not contain <VAR> .,evaluation
HJ2pirpxG_14,Please clarify more on this point .,request
SJTAcW5xf_0,"This paper describes a method for computing representations for out-of-vocabulary words , e.g. based on their spelling or dictionary definitions .",fact
SJTAcW5xf_1,"The main difference from previous approaches is that the model is that the embeddings are trained end-to-end for a specific task , rather than trying to produce generically useful embeddings .",fact
SJTAcW5xf_2,"The method leads to better performance than using no external resources , but not as high performance as using Glove embeddings .",fact
SJTAcW5xf_3,"The paper is clearly written , and has useful ablation experiments .",evaluation
SJTAcW5xf_4,"However , I have a couple of questions/concerns : - Most of the gains seem to come from using the spelling of the word .",evaluation
SJTAcW5xf_5,"As the authors note , this kind of character level modelling has been used in many previous works .",fact
SJTAcW5xf_6,"- I would be slightly surprised if no previous work has used external resources for training word representations using an end-task loss ,",evaluation
SJTAcW5xf_7,but I do n’t know the area well enough to make specific suggestions,evaluation
SJTAcW5xf_8,- I ’m a little skeptical about how often this method would really be useful in practice .,evaluation
SJTAcW5xf_9,"It seems to assume that you do n’t have much unlabelled text ( or you ’d use Glove ) ,",fact
SJTAcW5xf_10,but you probably need a large labelled dataset to learn how to read dictionary definitions well .,evaluation
SJTAcW5xf_11,All the experiments use large tasks,evaluation
SJTAcW5xf_12,- it would be helpful to have an experiment showing an improvement over character-level modelling on a smaller task .,request
SJTAcW5xf_13,"- The results on SQUAD seem pretty weak - 52-64 % , compared to the SOTA of 81 .",evaluation
SJTAcW5xf_14,"It seems like the proposed method is quite generic ,",evaluation
SJTAcW5xf_15,so why not apply it to a stronger baseline ?,request
B1y7_3YgM_0,"This submission proposes a new seq2sel solution by adopting two new techniques , a sequence-to-set model and column attention mechanism .",fact
B1y7_3YgM_1,They show performance improve over existing studies on WikiSQL dataset .,fact
B1y7_3YgM_2,"While the paper is written clearly ,",evaluation
B1y7_3YgM_3,the contributions of the work heavily depends on the WikiSQL dataset .,fact
B1y7_3YgM_4,It is not sure if the approach is generally applicable to other sequence-to-sql workloads .,evaluation
B1y7_3YgM_5,"Detailed comments are listed below : 1 . WikiSQL dataset contains only a small class of SQL queries , with aggregation over single table and various filtering conditions .",evaluation
B1y7_3YgM_6,"It does not involve any complex operator in relational database system , e.g. , join and groupby .",fact
B1y7_3YgM_7,"Due to its simple structure , the problem of sequence-to-sql translation over WikiSQL is actually simplified as a parameter selection problem for a fixed template .",fact
B1y7_3YgM_8,This greatly limits the generalization of approaches only applicable to WikiSQL .,evaluation
B1y7_3YgM_9,The authors are encouraged to explore other datasets available in the literature .,request
B1y7_3YgM_10,"2 . The "" order-matters "" motivation is not very convincing .",evaluation
B1y7_3YgM_11,"It is straightforward to employ a global ordering approach to rank the columns and filtering conditions based on certain rules , e.g. , alphabetical order .",evaluation
B1y7_3YgM_12,That could ensure the orders in the SQL results are always consistent .,fact
B1y7_3YgM_13,3 . The experiments do not fully verify how the approaches bring performance improvements .,fact
B1y7_3YgM_14,"In the current version , the authors only report superficial accuracy results on final outcomes , without any deep investigation into why and how their approach works .",evaluation
B1y7_3YgM_15,"For instance , they could verify how much accuracy improvement is due to the insensitivity to order in filtering expressions .",request
B1y7_3YgM_16,4 . They do not compare against state-of-the-art solution on column and expression selection .,fact
B1y7_3YgM_17,"While their attention mechanism over the columns could bring performance improvement ,",fact
B1y7_3YgM_18,they should have included experiments over existing solutions designed for similar purpose .,request
B1y7_3YgM_19,"In ( Yin , et al. , IJCAI 2016 ) , for example , representations over the columns are learned to generate better column selection .",fact
B1y7_3YgM_20,"As a conclusion , I find the submission contains certain interesting ideas but lacks serious research investigations .",evaluation
B1y7_3YgM_21,"The quality of the paper could be much enhanced , if the authors deepen their studies on this direction .",evaluation
r1zEZ9ief_0,The paper proposes a method which jointly learns the label embedding ( in the form of class similarity ) and a classification model .,fact
r1zEZ9ief_1,"While the motivation of the paper makes sense ,",evaluation
r1zEZ9ief_2,"the model is not properly justified ,",evaluation
r1zEZ9ief_3,and I learned very little after reading the paper .,evaluation
r1zEZ9ief_4,There are 5 terms in the proposed objective function .,fact
r1zEZ9ief_5,"There are also several other parameters associated with them : for example , the label temperature of <VAR> ’’ and and parameter alpha in the second last term etc .",fact
r1zEZ9ief_6,"For all the experiments , the same set of parameters are used ,",fact
r1zEZ9ief_7,and it is claimed that “ the method is robust in our experiment and simply works without fine tuning ” .,quote
r1zEZ9ief_8,While I agree that a robust and fine-tuning-free model is ideal,evaluation
r1zEZ9ief_9,1 ) this has to be justified by experiment .,request
r1zEZ9ief_10,2 ) showing the experiment with different parameters will help us understand the role each component plays .,evaluation
r1zEZ9ief_11,"This is perhaps more important than improving the baseline method by a few point ,",evaluation
r1zEZ9ief_12,especially given that the goal of this work is not to beat the state-of-the-art .,evaluation
rksMwz9xG_0,This paper presents a new reinforcement learning architecture called Reactor by combining various improvements in deep reinforcement learning algorithms and architectures into a single model .,fact
rksMwz9xG_1,"The main contributions of the paper are to achieve a better bias-variance trade-off in policy gradient updates , multi-step off-policy updates withdistributional RL , and prioritized experience replay for transition sequences .",fact
rksMwz9xG_2,The different modules are integrated well and the empirical results are very promising .,evaluation
rksMwz9xG_3,The experiments ( though limited to Atari ) are well carried out and the evaluation is performed on both sample efficiency and training time .,evaluation
rksMwz9xG_4,"Pros : 1 . Nice integration of several recent improvements in deep RL , along with a few novel tricks to improve training .",evaluation
rksMwz9xG_5,"2 . The empirical results on 57 Atari games are impressive , in terms of final scores as well as real-time training speed .",evaluation
rksMwz9xG_6,"Cons : 1 . Reactor is still less sample-efficient than Rainbow , with significantly lower scores after 200M frames .",fact
rksMwz9xG_7,"While the reactor trains much faster , it does use more parallel compute ,",fact
rksMwz9xG_8,so the comparison with Rainbow on wall clock time is not entirely fair .,evaluation
rksMwz9xG_9,Would a distributed version of Rainbow perform better in this respect ?,request
rksMwz9xG_10,2 . Empirical comparisons are restricted to the Atari domain .,fact
rksMwz9xG_11,The conclusions of the paper will be much stronger if results are also shown on other environments like Mujoco/Vizdoom/Deepmind Lab .,request
rksMwz9xG_12,"3 . Since the paper introduces a few new ideas like prioritized sequence replay ,",fact
rksMwz9xG_13,"it would help if a more detailed analysis was performed on the impact of these individual schemes , even if in a model simpler than the Reactor .",request
rksMwz9xG_14,"For instance , one could investigate the impact of prioritized sequence replay in models like multi-step DQN or recurrent DQN .",request
rksMwz9xG_15,This will help us understand the impact of each of these ideas in a more comprehensive fashion .,evaluation
BJJTve9gM_0,This paper proposes to adapt convnet representations to new tasks,fact
BJJTve9gM_1,while avoiding catastrophic forgetting by learning a per-task “ controller ” specifying weightings of the convolution-al filters throughout the network,fact
BJJTve9gM_2,while keeping the filters themselves fixed .,fact
BJJTve9gM_3,Pros The proposed approach is novel and broadly applicable .,evaluation
BJJTve9gM_4,"By definition it maintains the exact performance on the original task ,",fact
BJJTve9gM_5,and enables the network to transfer to new tasks using a controller with a small number of parameters ( asymptotically smaller than that of the base network ) .,fact
BJJTve9gM_6,The method is tested on a number of datasets ( each used as source and target ) and shows good transfer learning performance on each one .,fact
BJJTve9gM_7,A number of different fine-tuning regimes are explored .,fact
BJJTve9gM_8,The paper is mostly clear and well-written,evaluation
BJJTve9gM_9,( though with a few typos that should be fixed ) .,fact
BJJTve9gM_10,Cons/Questions/Suggestions The distinction between the convolutional and fully-connected layers ( called “ classifiers ” ) in the approach description ( sec 3 ) is somewhat arbitrary,evaluation
BJJTve9gM_11,"-- after all , convolutional layers are a generalization of fully-connected layers .",fact
BJJTve9gM_12,( This is hinted at by the mention of fully convolutional networks . ),evaluation
BJJTve9gM_13,The method could just as easily be applied to learn a task-specific rotation of the fully-connected layer weights .,evaluation
BJJTve9gM_14,"A more systematic set of experiments could compare learning the proposed weightings on the first K layers of the network ( for <EQN> ) and learning independent weights for the latter N-K layers ,",request
BJJTve9gM_15,but I understand this would be a rather large experimental burden .,evaluation
BJJTve9gM_16,"When discussing the controller initialization ( sec 4.3 ) , it ’s stated that the diagonal init works the best , and that this means one only needs to learn the diagonals to get the best results .",fact
BJJTve9gM_17,"Is this implying that the gradients wrt off-diagonal entries of the controller weight matrix are 0 under the diagonal initialization , hence the off-diagonal entries remain zero after learning ?",non-arg
BJJTve9gM_18,It ’s not immediately clear to me whether this is the case,evaluation
BJJTve9gM_19,-- it could help to clarify this in the text .,request
BJJTve9gM_20,"If the off-diag gradients are indeed 0 under the diag init , it could also make sense to experiment with an “ identity + noise ” initialization of the controller matrix ,",request
BJJTve9gM_21,which might give the best of both worlds in terms of flexibility and inductive bias to maintain the original representation .,evaluation
BJJTve9gM_22,"( Equivalently , one could treat the controller-weighted filters as a “ residual ” term on the original filters F with the controller weights W initialized to noise , with the final filters being <VAR> rather than just <VAR> . )",evaluation
BJJTve9gM_23,The dataset classifier ( sec 4.3.4 ) could be learnt end-to-end by using a softmax output of the dataset classifier as the alpha weighting .,request
BJJTve9gM_24,It would be interesting to see how this compares with the hard thresholding method used here .,request
BJJTve9gM_25,"( As an intermediate step , the performance could also be measured with the dataset classifier trained in the same way but used as a soft weighting , rather than the hard version rounding alpha to 0 or 1 . )",request
BJJTve9gM_26,"Overall , the paper is clear and the proposed method is sensible , novel , and evaluated reasonably thoroughly .",evaluation
BkE3cW5gG_0,Summary : This paper presents a thorough examination of the effects of pruning on model performance .,fact
BkE3cW5gG_1,"Importantly , they compare the performance of "" large-sparse "" models ( large models that underwent pruning in order to reduce memory footprint of model ) and "" small-dense "" models , showing that "" large-sparse "" models typically perform better than the "" small-dense "" models of comparable size ( in terms of number of non-zero parameters , and/or memory footprint ) .",fact
BkE3cW5gG_2,"They present results across a number of domains ( computer vision , language modelling , and neural machine translation ) and model types ( CNNs , LSTMs ) .",fact
BkE3cW5gG_3,"They also propose a way of performing pruning with a pre-defined sparsity schedule , simplifying the pruning process in a way which works across domains .",fact
BkE3cW5gG_4,"They are able to show convincingly that pruning is an effective way of trading off accuracy for model size ( more effective than simply reducing the size of model architecture ) ,",evaluation
BkE3cW5gG_5,although there does come a point where too much sparsity degrades the model performance considerably ;,fact
BkE3cW5gG_6,this suggests that pruning a medium size model to 80 % -90 % sparsity is likely better than pruning a larger model to > = 95 % sparsity .,evaluation
BkE3cW5gG_7,Review : Quality : The quality of the work is high,evaluation
BkE3cW5gG_8,--- the experiments are extensive and thorough .,evaluation
BkE3cW5gG_9,"I would have liked to see "" small-dense "" vs. "" large-sparse "" comparisons on Inception ( only large-sparse results are reported ) .",request
BkE3cW5gG_10,"Clarity : The paper is clearly written ,",evaluation
BkE3cW5gG_11,though there is room for improvement .,evaluation
BkE3cW5gG_12,"For example , many of the results are presented in a redundant manner ( in both tables and figures , where the table and figure are often not next to each other in the document ) .",evaluation
BkE3cW5gG_13,"Also , it is not clear in several cases exactly which training/heldout/test sets are used , and on which partition of the data the accuracies/BLEU scores/perplexities presented correspond to .",evaluation
BkE3cW5gG_14,"A small section ( before "" Methods "" ) describing the datasets/features in detail would be helpful .",request
BkE3cW5gG_15,"Also , it would have probably been nice to explain all of the tasks and datasets early on , and then present all the results at once ( NIT : include the plots in paper , and move the tables to an appendix ) .",request
BkE3cW5gG_16,"Originality : Although the experiments are informative ,",evaluation
BkE3cW5gG_17,the work as a whole is not very original .,evaluation
BkE3cW5gG_18,"The method proposed of using a sparsity schedule to perform pruning is simple and effective ,",evaluation
BkE3cW5gG_19,but is a rather incremental contribution .,evaluation
BkE3cW5gG_20,"The primary contribution of this paper is its experiments , which for the most part compare known methods .",evaluation
BkE3cW5gG_21,"Significance : The paper makes a nice contribution ,",evaluation
BkE3cW5gG_22,though it is not particularly significant or surprising .,evaluation
BkE3cW5gG_23,"The primary observations are : ( 1 ) large-sparse is typically better than small-dense , for a fixed number of non-zero parameters and/or memory footprint .",fact
BkE3cW5gG_24,"( 2 ) There is a point at which increasing the sparsity percentage severely degrades the performance of the model ,",fact
BkE3cW5gG_25,"which suggests that there is a "" sweet-spot "" when it comes to choosing the model architecture and sparsity percentage which give the best performance ( for a fixed memory footprint ) .",evaluation
BkE3cW5gG_26,"Result # 1 is not very surprising ,",evaluation
BkE3cW5gG_27,given that Han et al ( 2016 ) were able to show significant compression without loss in accuracy ;,fact
BkE3cW5gG_28,"thus , because one would expect a smaller dense model to perform worse than the large dense model ,",evaluation
BkE3cW5gG_29,it would also perform worse than the large sparse model .,fact
BkE3cW5gG_30,"Result # 2 had already been seen in Han et al ( 2016 ) ( for example , in Figure 6 ) .",fact
BkE3cW5gG_31,Pros : - Very thorough experiments across a number of domains,evaluation
BkE3cW5gG_32,Cons : - Methodological contributions are minor .,evaluation
BkE3cW5gG_33,"- Results are not surprising , and are in line with previous papers .",evaluation
SJxF3VsxG_0,This paper describes computationally efficient methods for training adversarially robust deep neural networks for image classification .,fact
SJxF3VsxG_1,"( These methods may extend to other machine learning models and domains as well , but that 's beyond the scope of this paper . )",fact
SJxF3VsxG_2,The former standard method for generating adversarially images quickly and using them in training was to do a single gradient step to increase the loss of the true label or decrease the loss of an alternate label .,evaluation
SJxF3VsxG_3,"This paper shows that such training methods only lead to robustness against these "" weak "" adversarial examples , leaving the adversarially-trained models vulnerable to multi-step white-box attacks and black-box attacks ( adversarial examples generated to attack alternate models ) .",fact
SJxF3VsxG_4,There are two proposed solutions .,fact
SJxF3VsxG_5,The first is to generate additional adversarial examples from other models and use them in training .,fact
SJxF3VsxG_6,This seems to yield robustness against black-box attacks from held-out models as well .,evaluation
SJxF3VsxG_7,"Of course , it requires that you have a somewhat diverse group of models to choose from .",fact
SJxF3VsxG_8,"If that 's the case , why not directly build an ensemble of all the models ?",request
SJxF3VsxG_9,"An ensemble of neural networks can still be represented as a neural network , although a more computationally costly one .",fact
SJxF3VsxG_10,"Thus , while this heuristic appears to be useful with current models against current attacks ,",evaluation
SJxF3VsxG_11,I do n't know how well it will hold up in the future .,evaluation
SJxF3VsxG_12,The second solution is to add random noise before taking the gradient step .,fact
SJxF3VsxG_13,"This yields more effective adversarial examples , both for attacking models and for training ,",fact
SJxF3VsxG_14,because it relies less on the local gradient .,fact
SJxF3VsxG_15,This is another simple idea that appears to be effective .,evaluation
SJxF3VsxG_16,"However , I would be interested to see a comparison to a 2-step gradient-based attack .",request
SJxF3VsxG_17,R+S tep-LL can be viewed as a 2-step attack : a random step followed by a gradient step .,evaluation
SJxF3VsxG_18,What if both steps were gradient steps instead ?,non-arg
SJxF3VsxG_19,"This interpolates between Step-LL and I-Step-LL , with an intermediate computational cost .",fact
SJxF3VsxG_20,"It would be very interesting to know if R+S tep-LL is more or less effective than 2 + Step-LL , and how large the difference is .",evaluation
SJxF3VsxG_21,"I like that this paper demonstrates the weakness of previous methods , including extensive experiments and a very nice visualization of the loss landscape in two adversarial dimensions .",fact
SJxF3VsxG_22,"The proposed heuristics seem effective in practice ,",evaluation
SJxF3VsxG_23,but they 're somewhat ad hoc,evaluation
SJxF3VsxG_24,and there is no analysis of how these heuristics might or might not be vulnerable to future attacks .,fact
Hk7vlKsxz_0,"Summary : The authors present a simple variation of vanilla recurrent neural networks , which use ReLU hiddens and a fixed identity matrix that is added to the hidden-to-hidden weight matrix .",fact
Hk7vlKsxz_1,"This identity connection acts as a “ surrogate memory ” component , preserving hidden activations over time steps .",fact
Hk7vlKsxz_2,The experiments demonstrate that this architecture reliably solves the addition task for up to 400 input frames .,fact
Hk7vlKsxz_3,It also achieves a very good performance on sequential and permuted MNIST and achieves SOTA performance on bAbI .,fact
Hk7vlKsxz_4,The authors observe that the proposed recurrent identity network ( RIN ) is relatively robust to hyperparameter choices .,fact
Hk7vlKsxz_5,"After Le et al. ( 2015 ) , the paper presents another convincing case for the application of ReLUs in RNNs .",evaluation
Hk7vlKsxz_6,Review : I very much like the paper .,evaluation
Hk7vlKsxz_7,The motivation and architecture is presented very clearly,evaluation
Hk7vlKsxz_8,and I am happy to also see explorations of simpler recurrent architectures in parallel to research of gated architectures !,evaluation
Hk7vlKsxz_9,"I have a few comments and questions : 1 ) Clarification : In Section 2.2 , do you really mean bit-wise multiplication or element-wise ?",non-arg
Hk7vlKsxz_10,"If bit-wise , can you elaborate why ?",non-arg
Hk7vlKsxz_11,I might have missed something .,evaluation
Hk7vlKsxz_12,2 ) Why does the learning curve of the IRNN stop around epoch 270 in Figure 2c ?,request
Hk7vlKsxz_13,Also some curves in the appendix stop abruptly without visible explosions .,fact
Hk7vlKsxz_14,Were these experiments run until completion ?,non-arg
Hk7vlKsxz_15,"If so , would it be possible to plot the complete curves ?",request
Hk7vlKsxz_16,3 ) I think for a fair comparison with LSTMs and IRNNs a limited hyperparameter search should be performed separately on all three architectures at least for the addition task .,request
Hk7vlKsxz_17,Optimal hyperparameters are usually model-specific .,evaluation
Hk7vlKsxz_18,"Admittedly , the authors mention that they do not intend to make claims about superior performance to LSTMs ,",fact
Hk7vlKsxz_19,however the competitive performance of small RINs is mentioned a couple of times in the manuscript .,fact
Hk7vlKsxz_20,Le et al. ( 2015 ) for instance perform a coarse grid search for each model .,fact
Hk7vlKsxz_21,"4 ) I would n't say that ResNets are Gated Neural Networks ,",fact
Hk7vlKsxz_22,as the branches are just summed up .,fact
Hk7vlKsxz_23,There is no ( multiplicative ) gating as in Highway Networks .,fact
Hk7vlKsxz_24,"5 ) I think what enables the training of very deep networks or LSTMs on long sequences is the presence of a ( close-to - ) identity component in forward/backward propagation , not the gating .",fact
Hk7vlKsxz_25,The use of ReLU activations in IRNNs ( with identity initialization of the hidden-to-hidden weights ) and RINs ( effectively initialized with identity plus some noise ) makes the recurrence more linear than with squashing activation functions .,fact
Hk7vlKsxz_26,6 ) Regarding the absence of gating in RINs : What is your intuition on how the model would perform in tasks for which conditional forgetting is useful .,request
Hk7vlKsxz_27,"Consider for example a task with long sequences , outputs at every time step and hidden activations not necessarily being encouraged to estimate last step hidden activations .",fact
Hk7vlKsxz_28,Would RINs readily learn to reset parts of the hidden state ?,request
Hk7vlKsxz_29,"7 ) Henaff et al. ( 2016 ) might be related ,",evaluation
Hk7vlKsxz_30,as they are also looking into the addition task with long sequences .,fact
Hk7vlKsxz_31,"Overall , the presented idea is novel to the best of my knowledge",evaluation
Hk7vlKsxz_32,and the manuscript is well-written .,evaluation
Hk7vlKsxz_33,"I would recommend it for acceptance ,",evaluation
Hk7vlKsxz_34,but would like to see the above points addressed ( especially 1-3 and some comments on 4-6 ) .,request
Hk7vlKsxz_35,After a revision I would consider to increase the score .,evaluation
Hk7vlKsxz_36,"References : Henaff , Mikael , Arthur Szlam , and Yann LeCun . "" Recurrent orthogonal networks and long-memory tasks . "" In International Conference on Machine Learning , pp. 2034-2042 . 2016 .",reference
Hk7vlKsxz_37,"Le , Quoc V. , Navdeep Jaitly , and Geoffrey E. Hinton . "" A simple way to initialize recurrent networks of rectified linear units . "" arXiv preprint arXiv : 1504.00941 ( 2015 ) .",reference
Sy8Kdltgz_0,This paper proposes a method of learning sparse dictionary learning by introducing new types of priors .,fact
Sy8Kdltgz_1,"Specifically , they designed a novel idea of defining a metric to measure discriminative properties along with the quality of presentations .",fact
Sy8Kdltgz_2,It is also presented the power of the proposed method in comparison with the existing methods in the literature .,fact
Sy8Kdltgz_3,"Overall , the paper deals with an important issue in dictionary learning and proposes a novel idea of utilizing a set of priors .",evaluation
Sy8Kdltgz_4,"To this reviewer ’s understanding , the thresholding parameter <VAR> is specific for a class <VAR> only ,",fact
Sy8Kdltgz_5,thus different classes have different <VAR> vectors .,fact
Sy8Kdltgz_6,"If so , Eq . ( 6 ) for approximation of the measure <VAR> is not clear how the similarity measure between <VAR> and <VAR> , ie , <EQN> and <EQN> , works to approximate it .",evaluation
Sy8Kdltgz_7,"It would be appreciated to give more detailed description on it and geometric illustration , if possible .",request
Sy8Kdltgz_8,"There are many typos and grammatical errors ,",fact
Sy8Kdltgz_9,which distract from reading and understanding the manuscript .,evaluation
HkN9lyRxG_0,This paper proposes to bring together multiple inductive biases that hope to correct for inconsistencies in sequence decoding .,fact
HkN9lyRxG_1,"Building on previous works that utilize modified objectives to generate sequences , this work proposes to optimize for the parameters of a pre-defined combination of various sub-objectives .",fact
HkN9lyRxG_2,The human evaluation is straight-forward and meaningful to compensate for the well-known inaccuracies of automatic evaluation .,evaluation
HkN9lyRxG_3,"While the paper points out that they introduce multiple inductive biases that are useful to produce human-like sentences ,",fact
HkN9lyRxG_4,it is not entirely correct that the objective is being learnt as claimed in portions of the paper .,evaluation
HkN9lyRxG_5,I would like this point to be clarified better in the paper .,request
HkN9lyRxG_6,I think showing results on grounded generation tasks like machine translation or image-captioning would make a stronger case for evaluating relevance .,request
HkN9lyRxG_7,I would like to see comparisons on these tasks .,request
BkCxP2Fez_0,"The paper presents a Depthwise Separable Graph Convolution network that aims at generalizing Depthwise convolutions , that exhibit a nice performance in image related tasks , to the graph domain .",fact
BkCxP2Fez_1,In particular it targets Graph Convolutional Networks .,fact
BkCxP2Fez_2,In the abstract the authors mention that the Depthwise Separable Graph Convolution that they propose is the key to understand the connections between geometric convolution methods and traditional 2D ones .,fact
BkCxP2Fez_3,I am afraid I have to disagree,evaluation
BkCxP2Fez_4,as the proposed approach is not giving any better understanding of what needs to be done and why .,fact
BkCxP2Fez_5,It is an efficient way to mimic what has worked so far for the planar domain,fact
BkCxP2Fez_6,"but I would not consider it as fundamental in "" closing the gap "" .",evaluation
BkCxP2Fez_7,I feel that the text is often redundant and that it could be simplified a lot .,evaluation
BkCxP2Fez_8,For example the authors state in various parts that DSC does not work on non-Euclidean data .,fact
BkCxP2Fez_9,Section 2 should be clearer and used to better explain related approaches to motivate the proposed one .,request
BkCxP2Fez_10,"In fact , the entire motivation , at least for me , never went beyond the simple fact that this happens to be a good way to improve performance .",evaluation
BkCxP2Fez_11,The intuition given is not sufficient to substantiate some of the claims on generality and understanding of graph based DL .,evaluation
BkCxP2Fez_12,"In 3.1 , at point ( 2 ) , the authors mention that DSC filters are learned from the data whereas GC uses a constant matrix .",fact
BkCxP2Fez_13,"This is not correct ,",fact
BkCxP2Fez_14,as also reported in equation 2 .,fact
BkCxP2Fez_15,The matrix U is learned from the data as well .,fact
BkCxP2Fez_16,Equation ( 4 ) shows that the proposed approach would weight Q different GC layers .,fact
BkCxP2Fez_17,In practical terms this is a linear combination of these graph convolutional layers .,fact
BkCxP2Fez_18,What is not clear is the \ Delta _ { ij } definition .,evaluation
BkCxP2Fez_19,"It is first introduced in 2.3 and described as the relative position of pixel i and pixel j on the image , but then used in the context of a graph in ( 4 ) .",fact
BkCxP2Fez_20,What is the coordinate system used by the authors in this case ?,request
BkCxP2Fez_21,This is a very important point that should be made clearer .,request
BkCxP2Fez_22,Why is the Related Work section at the end ?,evaluation
BkCxP2Fez_23,I would put it at the front .,request
BkCxP2Fez_24,The experiments compare with the recent relevant literature .,fact
BkCxP2Fez_25,I think that having less number of parameters is a good thing in this setting,evaluation
BkCxP2Fez_26,"as the data is scarce ,",evaluation
BkCxP2Fez_27,however I would like to see a more in-depth comparison with respect to the number of features produced by the model itself .,request
BkCxP2Fez_28,For example GCN has a representation space ( latent ) much smaller than DSCG .,evaluation
BkCxP2Fez_29,"No statistics over multiple runs are reported ,",fact
BkCxP2Fez_30,and given the high variance of results on these datasets I would like them to be reported .,request
BkCxP2Fez_31,"I think the separability of the filters in this case brings the right level of simplification to the learning task ,",evaluation
BkCxP2Fez_32,however as it also holds for the planar case it is not clear whether this is necessarily the best way forward .,evaluation
BkCxP2Fez_33,What are the underlying mathematical insights that lead towards selecting separable convolutions ?,request
BkCxP2Fez_34,Overall I found the paper interesting but not ground-breaking .,evaluation
BkCxP2Fez_35,A nice application of the separable principle to GCN .,evaluation
BkCxP2Fez_36,Results are also interesting,evaluation
BkCxP2Fez_37,but should be further verified by multiple runs .,request
SJJrhg5lf_0,"## Review Summary Overall , the paper 's paper core claim , that increasing batch sizes at a linear rate during training is as effective as decaying learning rates , isinteresting",evaluation
SJJrhg5lf_1,but does n't seem to be too surprising given other recent work in this space .,evaluation
SJJrhg5lf_2,"The most useful part of the paper is the empirical evidence to backup this claim ,",evaluation
SJJrhg5lf_3,which I ca n't easily find in previous literature .,non-arg
SJJrhg5lf_4,"I wish the paper had explored a wider variety of dataset tasks and models to better show how well this claim generalizes , better situated the practical benefits of the approach",request
SJJrhg5lf_5,( how much wallclock time is actually saved ?,request
SJJrhg5lf_6,"how well can it be integrated into a distributed workflow ? ) ,",request
SJJrhg5lf_7,and included some comparisons with other recent recommended ways to increase batch size over time .,request
SJJrhg5lf_8,## Pros / Strengths + effort to assess momentum / Adam / other modern methods,evaluation
SJJrhg5lf_9,+ effort to compare to previous experimental setups,evaluation
SJJrhg5lf_10,## Cons / Limitations - lack of wallclock measurements in experiments,evaluation
SJJrhg5lf_11,"- only ~ 2 models / datasets examined ,",fact
SJJrhg5lf_12,so difficult to assess generalization,evaluation
SJJrhg5lf_13,- lack of discussion about distributed/asynchronous SGD,evaluation
SJJrhg5lf_14,"## Significance Many recent previous efforts have looked at the importance of batch sizes during training ,",evaluation
SJJrhg5lf_15,so topic is relevant to the community .,evaluation
SJJrhg5lf_16,"Smith and Le ( 2017 ) present a differential equation model for the scale of gradients in SGD , finding a linear scaling rule proportional to eps N/B , where eps = learning rate , N = training set size , and B = batch size .",fact
SJJrhg5lf_17,Goyal et al ( 2017 ) show how to train deep models on ImageNet effectively with large ( but fixed ) batch sizes by using a linear scaling rule .,fact
SJJrhg5lf_18,A few recent works have directly tested increasing batch sizes during training .,fact
SJJrhg5lf_19,"De et al ( AISTATS 2017 ) have a method for gradually increasing batch sizes , as do Friedlander and Schmidt ( 2012 ) .",fact
SJJrhg5lf_20,"Thus , it is already reasonable to practitioners that the proposed linear scaling of batch sizes during training would be effective .",fact
SJJrhg5lf_21,"While increasing batch size at the proposed linear scale is simple and seems to be effective ,",evaluation
SJJrhg5lf_22,a careful reader will be curious how much more could be gained from the backtracking line search method proposed in De et al .,evaluation
SJJrhg5lf_23,"## Quality Overall , only single training runs from a random initialization are used .",fact
SJJrhg5lf_24,"It would be better to take the best of many runs or to somehow show error bars ,",request
SJJrhg5lf_25,to avoid the reader wondering whether gains are due to changes in algorithm or to poor exploration due to bad initialization .,evaluation
SJJrhg5lf_26,This happens a lot in Sec. 5.2 .,evaluation
SJJrhg5lf_27,Some of the experimental setting seem a bit haphazard and not very systematic .,evaluation
SJJrhg5lf_28,"In Sec. 5.2 , only two learning rate scales are tested ( 0.1 and 0.5 ) .",fact
SJJrhg5lf_29,Why not examine a more thorough range of values ?,request
SJJrhg5lf_30,Why not report actual wallclock times ?,request
SJJrhg5lf_31,"Of course having reduced number of parameter updates is useful ,",evaluation
SJJrhg5lf_32,but it 's difficult to tell how big of a win this could be .,evaluation
SJJrhg5lf_33,What about distributed SGD or asyncronous SGD ( hogwild ) ?,request
SJJrhg5lf_34,Small batch sizes sometimes make it easier for many machines to be working simultaneously .,evaluation
SJJrhg5lf_35,"If we scale up to batch sizes of ~ N/10 , we can only get 10x speedups in parallelization ( in terms of number of parameter updates ) .",fact
SJJrhg5lf_36,I think there is some subtle but important discussion needed on how this framework fits into modern distributed systems for SGD .,request
SJJrhg5lf_37,## Clarity Overall the paper reads reasonably well .,evaluation
SJJrhg5lf_38,"Offering a related work "" feature matrix "" that helps readers keep track of how previous efforts scale learning rates or minibatch sizes for specific experiments could be valueable .",request
SJJrhg5lf_39,"Right now , lots of this information is just provided in text ,",evaluation
SJJrhg5lf_40,so it 's not easy to make head-to-head comparisons .,evaluation
SJJrhg5lf_41,Several figure captions should be updated to clarify which model and dataset are studied .,request
SJJrhg5lf_42,"For example , when skimming Fig. 3 's caption there is no such information .",fact
SJJrhg5lf_43,## Paper Summary The paper examines the influence of batch size on the behavior of stochastic gradient descent to minimize cost functions .,evaluation
SJJrhg5lf_44,"The central thesis is that instead of the "" conventional wisdom "" to fix the batch size during training and decay the learning rate , it is equally effective ( in terms of training/test error reached ) to gradually increase batch size during training while fixing the learning rate .",fact
SJJrhg5lf_45,"These two strategies are thus "" equivalent "" .",evaluation
SJJrhg5lf_46,"Furthermore , using larger batches means fewer parameter updates per epoch ,",fact
SJJrhg5lf_47,so training is potentially much faster .,evaluation
SJJrhg5lf_48,Section 2 motivates the suggested linear scaling using previous SGD analysis from Smith and Le ( 2017 ) .,fact
SJJrhg5lf_49,Section 3 makes connections to previous work on finding optimal batch sizes to close the generaization gap .,fact
SJJrhg5lf_50,Section 4 extends analysis to include SGD methods with momentum .,fact
SJJrhg5lf_51,"In Section 5.1 , experiments training a 16-4 ResNet on CIFAR-10 compare three possible SGD schedules :",fact
SJJrhg5lf_52,* increasing batch size * decaying learning rate * hybrid ( increasing batch size and decaying learning rate ),fact
SJJrhg5lf_53,"Fig. 2 , 3 and 4 show that across a range of SGD variants ( + / - momentum , etc ) these three schedules have similar error vs. epoch curves .",fact
SJJrhg5lf_54,"This is the core claimed contribution : empirical evidence that these strategies are "" equivalent "" .",evaluation
SJJrhg5lf_55,"In Section 5.3 , experiments look at Inception-ResNet-V2 on ImageNet ,",fact
SJJrhg5lf_56,"showing the proposed approach can reach comparable accuracies to previous work at even fewer parameter updates ( 2500 here , vs. ∼ 14000 for Goyal et al 2007 )",fact
ryfzqnhxz_0,Summary : The paper propose a method for generating adversarial examples in image recognition problems .,fact
ryfzqnhxz_1,The Adversarial scheme is inspired in the one proposed by Goodgellow et al 2015 ( AT ) that introduces small perturbations to the data in the direction that increases the error .,fact
ryfzqnhxz_2,Such a perturbations are random ( they have not structure ) and lack of interpretation for a human user .,fact
ryfzqnhxz_3,"The proposal is to limit the perturbations to just three kind of global motion fields : shift , centered rotation and scale ( zoom in/out ) .",fact
ryfzqnhxz_4,"Since the motions are small in scale ,",fact
ryfzqnhxz_5,the authors use a first-order Taylor series approximation ( as in classical optical flow ) .,fact
ryfzqnhxz_6,This approximation allows to obtain close formulas for the perturbed examples ; i.e. the correction factor of the Back-propagation computed derivatives w.r.t. original example .,fact
ryfzqnhxz_7,"As result , the method is computational efficient respect to the AT and the perturbations are interpretable .",evaluation
ryfzqnhxz_8,Experiments demonstrate that with the MNIST database is not obtained an improvement in the error reduction but a reduction of the computational time .,fact
ryfzqnhxz_9,"However , with ta more general recognition problem conducted with the CIFAR-10 database , the use of the proposed method improves both the error and the computational time , when compared with AT and Virtual Adversarial Train .",fact
ryfzqnhxz_10,"Comments : 1 . The paper presents a series os typos : FILED ( title ) , obouve , freedm , nerual , ; please check carfully .",request
ryfzqnhxz_11,"2 . The Derivation of eq . ( 13 ) should be explained ,",request
ryfzqnhxz_12,It could be said that ( 12 ) can be casted as a eigenvalue problem [ for example : <EQN> and ( 13 ) is the largest eigenvalue of <EQN>,fact
ryfzqnhxz_13,3 . The improvement in the error results in the db CIFAR-10 is good enough to see merit in the proposal approach .,evaluation
ryfzqnhxz_14,Maybe other perturbations with closed formula could be considered and linear combinations of them,request
Sy_JLe5lz_0,"This paper uses an information geometric view on hierarchical models to discuss a bias - variance decomposition in Boltzmann machines ,",fact
Sy_JLe5lz_1,"presenting interesting conclusions ,",evaluation
Sy_JLe5lz_2,whereby some more care appears to be needed for making these claims .,evaluation
Sy_JLe5lz_3,The paper arrives at the main conclusion that it is possible to reduce both the bias and the variance in a hierarchical model .,fact
Sy_JLe5lz_4,"The discussion is not specific to deep learning nor to Boltzmann machines , but actually addresses hierarchical exponential family models .",fact
Sy_JLe5lz_5,The methods pertaining hierarchical models are interesting and presented in a clear way .,evaluation
Sy_JLe5lz_6,"My concern are the following points : The main theorem presents only a lower bound , meaning that it provides no guarantee that the variance can indeed be reduced .",fact
Sy_JLe5lz_7,"The paper seems to ignore that a model with hidden variables may be singular ,",fact
Sy_JLe5lz_8,in which case the Fisher metric is not positive definite and the Cramer Rao bound has no meaning .,fact
Sy_JLe5lz_9,This interferes with the claims and derivations made in the paper in the case of models with hidden variables .,fact
Sy_JLe5lz_10,"The problem seems to lie in the fact that the presented derivations assume that an optimal distribution in the data manifold is given ( see Theorem 1 and proof ) , effectively making this a discussion about a fully observed hierarchical model .",fact
Sy_JLe5lz_11,"In particular , it is not further specified how to obtain <VAR> in page 6 before ( 13 ) .",fact
Sy_JLe5lz_12,"Also , in page 5 the paper states that `` it is known that the EM-algorithm can obtain the global optimum of Equation ( 12 ) ( Amari , 2016 , Section 8.1.3 ) '' .",fact
Sy_JLe5lz_13,"However , what is shown in that reference is only that : ( Theorem 8.2 . , Amari , 2016 ) `` The KL-divergence decreases monotonically by repeating the E-step and the M-step . Hence , the algorithm converges to an equilibrium . ''",fact
Sy_JLe5lz_14,A model with hidden variables can have several global and local optimisers,fact
Sy_JLe5lz_15,"( see , e.g. <URL> ) .",reference
Sy_JLe5lz_16,"The critical points of the EM algorithm can have a non trivial structure ,",fact
Sy_JLe5lz_17,as has been observed in the case of non negative rank matrix varieties,fact
Sy_JLe5lz_18,"( see , e.g. , <URL> ) .",reference
Sy_JLe5lz_19,"OTHER In page 3 , `` S _ \ beta is e-flat and S _ \ alpha ... '' , should this not be the other way around ?",request
Sy_JLe5lz_20,( See also page 5 last paragraph of Section 2 . ),request
Sy_JLe5lz_21,Please also indicate the precise location in the provided reference .,request
Sy_JLe5lz_22,All pages up to page 5 are introduction .,evaluation
Sy_JLe5lz_23,Section 2.3 . as presented is very vague and does not add much to the discussion .,evaluation
Sy_JLe5lz_24,"In page 7 , please explain <EQN>",request
Hyl2iJgGG_0,"This paper examines the very popular and useful ADAM optimization algorithm , and locates a mistake in its proof of convergence ( for convex problems ) .",fact
Hyl2iJgGG_1,"Not only that , the authors also show a specific toy convex problem on which ADAM fails to converge .",fact
Hyl2iJgGG_2,"Once the problem was identified to be the decrease in v_t ( and increase in learning rate ) , they modified the algorithm to solve that problem .",fact
Hyl2iJgGG_3,They then show the modified algorithm does indeed converge and show some experimental results comparing it to ADAM .,fact
Hyl2iJgGG_4,"The paper is well written , interesting and very important given the popularity of ADAM .",evaluation
Hyl2iJgGG_5,Remarks : - The fact that your algorithm can not increase the learning rate seems like a possible problem in practice .,evaluation
Hyl2iJgGG_6,A large gradient at the first steps due to bad initialization can slow the rest of training .,fact
Hyl2iJgGG_7,"The experimental part is limited ,",evaluation
Hyl2iJgGG_8,"as you state "" preliminary "" ,",fact
Hyl2iJgGG_9,which is a unfortunate for a work with possibly an important practical implication .,evaluation
Hyl2iJgGG_10,"Considering how easy it is to run experiments with standard networks using open-source software ,",evaluation
Hyl2iJgGG_11,this can easily improve the paper .,request
Hyl2iJgGG_12,"That being said , I understand that the focus of this work is theoretical and well deserves to be accepted based on the theoretical work .",evaluation
Hyl2iJgGG_13,- On page 14 the fourth inequality not is clear to me .,evaluation
Hyl2iJgGG_14,- On page 6 you talk about an alternative algorithm using smoothed gradients which you do not mention anywhere else,fact
Hyl2iJgGG_15,and this is n't that clear ( more then one way to smooth ) .,evaluation
Hyl2iJgGG_16,A simple pseudo-code in the appendix would be welcome .,request
Hyl2iJgGG_17,Minor remarks : - After the proof of theorem 1 you jump to the proof of theorem 6,fact
Hyl2iJgGG_18,( which is n't in the paper ),fact
Hyl2iJgGG_19,and then continue with theorem 2 .,fact
Hyl2iJgGG_20,It is a bit confusing .,evaluation
Hyl2iJgGG_21,- Page 16 at the bottom v_t = ... sum beta ^ { t-1-i } g_i should be g_i ^ 2,request
Hyl2iJgGG_22,"- Page 19 second line , you switch between j & t and it is confusing .",evaluation
Hyl2iJgGG_23,Better notation would help .,request
Hyl2iJgGG_24,- The cifarnet uses LRN layer that is n't used anymore .,fact
Hy7Gjh9eM_0,The authors proposed to supplement adversarial training with an additional regularization that forces the embeddings of clean and adversarial inputs to be similar .,fact
Hy7Gjh9eM_1,The authors demonstrate on MNIST and CIFAR that the added regularization leads to more robustness to various kinds of attacks .,fact
Hy7Gjh9eM_2,"The authors further propose to enhance the network with cascaded adversarial training , that is , learning against iteratively generated adversarial inputs , and showed improved performance against harder attacks .",fact
Hy7Gjh9eM_3,The idea proposed is fairly straight-forward .,evaluation
Hy7Gjh9eM_4,"Despite being a simple approach , the experimental results are quite promising .",evaluation
Hy7Gjh9eM_5,The analysis on the gradient correlation coefficient and label leaking phenomenon provide some interesting insights .,evaluation
Hy7Gjh9eM_6,"As pointed out in section 4.2 , increasing the regularization coefficient leads to degenerated embeddings .",fact
Hy7Gjh9eM_7,"Have the authors consider distance metrics that are less sensitive to the magnitude of the embeddings , for example , normalizing the inputs before sending it to the bidirectional or pivot loss , or use cosine distance etc . ?",request
Hy7Gjh9eM_8,"Table 4 and 5 seem to suggest that cascaded adversarial learning have more negative impact on test set with one-step attacks than clean test set ,",fact
Hy7Gjh9eM_9,which is a bit counter-intuitive .,evaluation
Hy7Gjh9eM_10,Do the authors have any insight on this ?,non-arg
Hy7Gjh9eM_11,Comments : 1 . The writing of the paper could be improved .,evaluation
Hy7Gjh9eM_12,"For example , "" Transferability analysis "" in section 1 is barely understandable ;",evaluation
Hy7Gjh9eM_13,2 . Arrow in Figure 3 are not quite readable ;,evaluation
Hy7Gjh9eM_14,3 . The paper is over 11 pages .,fact
Hy7Gjh9eM_15,The authors might want to consider shrink it down the recommended length .,request
HkeOU0qgf_0,"The author unveils some properties of the resnets , for example , the cosine loss and l2 ratio of the layers .",fact
HkeOU0qgf_1,"I think the author should place more focus to study "" real "" iterative inference with shared parameters rather than analyzing original resnets .",request
HkeOU0qgf_2,"In resnet without sharing parameters , it is quite ambiguous to say whether it is doing representation learning or iterative refinement .",evaluation
HkeOU0qgf_3,1 . The cosine loss is not meaningful in the sense that the classification layer is trained on the output of the last residual block and fixed .,evaluation
HkeOU0qgf_4,Moving the classification layer to early layers will definitely result in accuracy loss .,evaluation
HkeOU0qgf_5,"Even in non-residual network , we can always say that the vector <VAR> is refining <VAR> towards the negative gradient direction .",fact
HkeOU0qgf_6,The motivation of iterative inference would be to generate a feature that is easier to classify rather than to match the current fixed classifier .,evaluation
HkeOU0qgf_7,Thus the final classification layer should be retrained for every addition or removal of residual blocks .,request
HkeOU0qgf_8,"2 . The l2 ratio . The l2 ratio is small for higher residual layers , I 'm not sure how much this phenomenon can prove that resnet is actually doing iterative inference .",evaluation
HkeOU0qgf_9,3 . In section 4.4 it is shown that unrolling the layers can improve the performance of the network .,fact
HkeOU0qgf_10,"However , the same can be achieved by adding more unshared layers .",evaluation
HkeOU0qgf_11,I think the study should focus more on whether shared or unshared is better .,request
HkeOU0qgf_12,"4 . Section 4.5 is a bit weak in experiments ,",evaluation
HkeOU0qgf_13,"my conclusion is that currently it is still limited by batch normalization and optimization ,",evaluation
HkeOU0qgf_14,the evidence is still not strong enough to show that iterative inference is advantageous / disadvantageous .,evaluation
HkeOU0qgf_15,"The the above said , I think the more important thing is how we can benefit from iterative inference interpretation , which is relatively weak in this paper .",evaluation
HJ1MEAYxG_0,"The authors are motivated by two problems : Inputting non-Euclidean data ( such as graphs ) into deep CNNs , and analyzing optimization properties of deep networks .",fact
HJ1MEAYxG_1,"In particular , they look at the problem of maze testing , where , given a grid of black and white pixels , the goal is to answer whether there is a path from a designated starting point to an ending point .",fact
HJ1MEAYxG_2,They choose to analyze mazes because they have many nice statistical properties from percolation theory .,fact
HJ1MEAYxG_3,"For one , the problem is solvable with breadth first search in O ( L ^ 2 ) time , for an L x L maze .",fact
HJ1MEAYxG_4,"They show that a CNN can essentially encode a BFS ,",fact
HJ1MEAYxG_5,so theoretically a CNN should be able to solve the problem .,fact
HJ1MEAYxG_6,"Their architecture is a deep feedforward network where each layer takes as input two images : one corresponding to the original maze ( a skip connection ) , and the output of the previous layer .",fact
HJ1MEAYxG_7,Layers alternate between convolutional and sigmoidal .,fact
HJ1MEAYxG_8,The authors discuss how this architecture can solve the problem exactly .,fact
HJ1MEAYxG_9,The pictorial explanation for how the CNN can mimic BFS is interesting,evaluation
HJ1MEAYxG_10,but I got a little lost in the 3 cases on page 4 .,evaluation
HJ1MEAYxG_11,"For example , what is r ?",request
HJ1MEAYxG_12,And what is the relation of the black/white and orange squares ?,request
HJ1MEAYxG_13,I thought this could use a little more clarity .,request
HJ1MEAYxG_14,"Though experiments , they show that there are two kinds of minima , depending on whether we allow negative initializations in the convolution kernels .",fact
HJ1MEAYxG_15,"When positive initializations are enforced , the network can more or less mimic the BFS behavior , but never when initializations can be negative .",fact
HJ1MEAYxG_16,"They offer a rigorous analysis into the behavior of optimization in each of these cases , concluding that there is an essential singularity in the cost function around the exact solution ,",fact
HJ1MEAYxG_17,yet learning succumbs to poor optima due to poor initial predictions in training .,fact
HJ1MEAYxG_18,I thought this was an impressive paper that looked at theoretical properties of CNNs .,evaluation
HJ1MEAYxG_19,"The problem was very well-motivated ,",evaluation
HJ1MEAYxG_20,and the analysis was sharp and offered interesting insights into the problem of maze solving .,evaluation
HJ1MEAYxG_21,What I thought was especially interesting is how their analysis can be extended to other graph problems ;,evaluation
HJ1MEAYxG_22,"while their analysis was specific to the problem of maze solving , they offer an approach -- e.g. that of finding "" bugs "" when dealing with graph objects -- that can extend to other problems .",fact
HJ1MEAYxG_23,I would be excited to see similar analysis of other toy problems involving graphs .,request
HJ1MEAYxG_24,One complaint I had was inconsistent clarity :,evaluation
HJ1MEAYxG_25,"while a lot was well-motivated and straightforward to understand ,",evaluation
HJ1MEAYxG_26,"I got lost in some of the details ( as an example , the figure on page 4 did not initially make much sense to me ) .",evaluation
HJ1MEAYxG_27,"Also , in the experiments , the authors mention multiple attempt with the same settings --",fact
HJ1MEAYxG_28,are these experiments differentiated only by their initialization ?,request
HJ1MEAYxG_29,"Finally , there were various typos throughout",fact
HJ1MEAYxG_30,"( one example is "" neglect minimua "" on page 2 should be "" neglect minima "" ) .",request
HJ1MEAYxG_31,"Pros : Rigorous analysis ,",evaluation
HJ1MEAYxG_32,"well motivated problem ,",evaluation
HJ1MEAYxG_33,generalizable results to deep learning theory,fact
HJ1MEAYxG_34,Cons : Clarity,evaluation
Hym3oxKlf_0,"In this paper , the authors have proposed a GAN based method to conduct data augmentation .",fact
Hym3oxKlf_1,The cross-class transformations are mapped to a low dimensional latent space using conditional GAN .,fact
Hym3oxKlf_2,The paper is technically sound and the novelty is significant .,evaluation
Hym3oxKlf_3,The motivation of the proposed methods is clearly illustrated .,evaluation
Hym3oxKlf_4,Experiments on three datasets demonstrate the advantage of the proposed framework .,fact
Hym3oxKlf_5,"However , this paper still suffers from some drawbacks as below :",evaluation
Hym3oxKlf_6,( 1 ) The illustration of the framework is not clear enough .,evaluation
Hym3oxKlf_7,"For example , in figure 3 , it says the GAN is designed for “ class c ” , which is ambiguous whether the authors trained only one network for all class or trained multiple networks and each is trained on one class .",evaluation
Hym3oxKlf_8,"( 2 ) Some details is not clearly given , such as the dimension of the Gaussian distribution , the dimension of the projected noise and .",evaluation
Hym3oxKlf_9,( 3 ) The proposed method needs to sample image pairs in each class .,fact
Hym3oxKlf_10,"As far as I am concerned , in most cases sampling strategy will affect the performance to some extent .",evaluation
Hym3oxKlf_11,The authors need to show the robustness to sampling strategy of the proposed method .,request
SyZQxkmxG_0,CONTRIBUTION The main contribution of the paper is not clearly stated .,evaluation
SyZQxkmxG_1,"To the reviewer , It seems “ life-long learning ” is the same as “ online learning ” .",evaluation
SyZQxkmxG_2,"However , the whole paper does not define what “ life-long learning ” is .",fact
SyZQxkmxG_3,The limited budget scheme is well established in the literature .,fact
SyZQxkmxG_4,1 <CIT>,reference
SyZQxkmxG_5,2 <CIT>,reference
SyZQxkmxG_6,It is not clear what the new proposal in the paper .,evaluation
SyZQxkmxG_7,WRITING QUALITY The paper is not well written in a good shape .,evaluation
SyZQxkmxG_8,"Many meanings of the equations are not stated clearly , e.g. , <VAR> in eq . ( 7 ) .",evaluation
SyZQxkmxG_9,"Furthermore , the equation in algorithm 2 is not well formatted .",evaluation
SyZQxkmxG_10,DETAILED COMMENTS 1 . The mapping function <VAR> appears in Eq . ( 1 ) without definition .,fact
SyZQxkmxG_11,2 . The last equation in pp. 3 defines the decision function f by an inner product .,fact
SyZQxkmxG_12,"In the equation , the notation x_t and i_t is not clearly defined .",evaluation
SyZQxkmxG_13,"More seriously , a comma is missed in the definition of the inner product .",fact
SyZQxkmxG_14,"3 . Some equations are labeled but never referenced , e.g. , Eq . ( 4 ) .",fact
SyZQxkmxG_15,4 . The physical meaning of Eq . ( 7 ) is unclear .,evaluation
SyZQxkmxG_16,"However , this equation is the key proposal of the paper .",evaluation
SyZQxkmxG_17,"For example , what is the output of the Eq . ( 7 ) ?",request
SyZQxkmxG_18,What is the main objective of Eq . ( 7 ) ?,request
SyZQxkmxG_19,"Moreover , what support vectors should be removed by optimizing Eq . ( 7 ) ?",request
SyZQxkmxG_20,One main issue is that the notation <VAR> is not clearly defined .,evaluation
SyZQxkmxG_21,The computation of <VAR> makes it hard to understand .,evaluation
SyZQxkmxG_22,"Especially , the dimension of <VAR> in Eq . ( 7 ) is unknown .",fact
SyZQxkmxG_23,ABOUT EXPERIMENTS 1 . It is unclear how to tune the hyperparameters .,evaluation
SyZQxkmxG_24,"2 . In Table 1 , the results only report the standard deviation of AUC .",fact
SyZQxkmxG_25,No standard deviations of nSV and Time are reported .,fact
S1ck4rYxM_0,"[ Overview ] In this paper , the authors proposed a novel model called MemoryGAN , which integrates memory network with GAN .",fact
S1ck4rYxM_1,"As claimed by the authors , MemoryGAN is aimed at addressing two problems of GAN training :",fact
S1ck4rYxM_2,1 ) difficult to model the structural discontinuity between disparate classes in the latent space ;,fact
S1ck4rYxM_3,2 ) catastrophic forgetting problem during the training of discriminator about the past synthesized samples by the generator .,fact
S1ck4rYxM_4,It exploits the life-long memory network and adapts it to GAN .,fact
S1ck4rYxM_5,"It consists of two parts , discriminative memory network ( DMN ) and Memory Conditional Generative Network ( MCGN ) .",fact
S1ck4rYxM_6,"DMN is used for discriminating input samples by integrating the memory learnt in the memory network , and MCGN is used for generating images based on random vector and the sampled memory from the memory network .",fact
S1ck4rYxM_7,"In the experiments , the authors evaluated memoryGAN on three datasets , CIFAR-10 , affine-MNIST and Fashion-MNIST , and demonstrated the superiority to previous models .",fact
S1ck4rYxM_8,"Through ablation study , the authors further showed the effects of separate components in memoryGAN .",fact
S1ck4rYxM_9,[ Strengths ] 1 . This paper is well-written .,evaluation
S1ck4rYxM_10,All modules in the proposed model and the experiments were explained clearly .,evaluation
S1ck4rYxM_11,I enjoyed much to read the paper .,evaluation
S1ck4rYxM_12,2 . The paper presents a novel method called MemoryGAN for GAN training .,fact
S1ck4rYxM_13,"To address the two infamous problems mentioned in the paper , the authors proposed to integrate a memory network into GAN .",fact
S1ck4rYxM_14,"Through memory network , MemoryGAN can explicitly learn the data distribution of real images and fake images .",fact
S1ck4rYxM_15,I think this is a very promising and meaningful extension to the original GAN .,evaluation
S1ck4rYxM_16,"3 . With MemoryGAN , the authors achieved best Inception Score on CIFAR-10 .",fact
S1ck4rYxM_17,"By ablation study , the authors demonstrated each part of the model helps to improve the final performance .",fact
S1ck4rYxM_18,[ Comments ] My comments are mainly about the experiment part :,non-arg
S1ck4rYxM_19,"1 . In Table 2 , the authors show the Inception Score of images generated by DCGAN at the last row .",fact
S1ck4rYxM_20,"On CIFAR-10 , it is ~ 5.35 .",fact
S1ck4rYxM_21,"As the authors mentioned , removing EM , MCGCN and Memory will result in a conventional DCGAN .",fact
S1ck4rYxM_22,"However , as far as I know , DCGAN could achieve > 6.5 Inception Score in general .",fact
S1ck4rYxM_23,I am wondering what makes such a big difference between the reported numbers in this paper and other papers ?,non-arg
S1ck4rYxM_24,"2 . In the experiments , the authors set N = 16,384 , and M = 512 , and z is with dimension 16 .",fact
S1ck4rYxM_25,I did not understand why the memory size is such large .,evaluation
S1ck4rYxM_26,"Take CIFAR-10 as the example , its training set contains 50k images .",fact
S1ck4rYxM_27,"Using such a large memory size , each memory slot will merely count for several samples .",fact
S1ck4rYxM_28,Is a large memory size necessary to make MemoryGAN work ?,non-arg
S1ck4rYxM_29,"If not , the authors should also show ablated study on the effect of different memory size ;",request
S1ck4rYxM_30,"If it is true , please explain why is that .",request
S1ck4rYxM_31,"Also , the authors should mention the training time compared with DCGAN .",request
S1ck4rYxM_32,Updating memory with such a large size seems very time-consuming .,fact
S1ck4rYxM_33,3 . Still on the memory size in this model .,non-arg
S1ck4rYxM_34,I am curious about the results if the size is decreased to the same or comparable number of image categories in the training set .,non-arg
S1ck4rYxM_35,"As the author claimed , if the memory network could learn to cluster training data into different category , we should be able to see some interesting results by sampling the keys and generate categoric images .",fact
S1ck4rYxM_36,"4 . The paper should be compared with InfoGAN ( Chen et al. 2016 ) ,",request
S1ck4rYxM_37,and the authors should explain the differences between two models in the related work .,request
S1ck4rYxM_38,"Similar to MemoryGAN , InfoGAN also did not need any data annotations , but could learn the latent code flexibly .",fact
S1ck4rYxM_39,[ Summary ],non-arg
S1ck4rYxM_40,This paper proposed a new model called MemoryGAN for image generation .,fact
S1ck4rYxM_41,"It combined memory network with GAN , and achieved state-of-art performance on CIFAR-10 .",fact
S1ck4rYxM_42,The arguments that MemoryGAN could solve the two infamous problem make sense .,evaluation
S1ck4rYxM_43,"As I mentioned above , I did not understand why the authors used such large memory size .",non-arg
S1ck4rYxM_44,More explanations and experiments should be conducted to justify this setting .,request
S1ck4rYxM_45,"Overall , I think MemoryGAN opened a new direction of GAN and worth to further explore .",evaluation
SyKUVctlM_0,This paper proposes a recurrent neural network for visual question answering .,fact
SyKUVctlM_1,"The recurrent neural network is equipped with a carefully designed recurrent unit called MAC ( Memory , Attention and Control ) cell , which encourages sequential reasoning by restraining interaction between inputs and its hidden states .",fact
SyKUVctlM_2,"The proposed model shows the state-of-the-art performance on CLEVR and CLEVR-Humans dataset , which are standard benchmarks for visual reasoning problem .",fact
SyKUVctlM_3,"Additional experiments with limited training data shows the data efficiency of the model , which supports its strong generalization ability .",fact
SyKUVctlM_4,The proposed model in this paper is designed with reasonable motivations and shows strong experimental results in terms of overall accuracy and the data efficiency .,evaluation
SyKUVctlM_5,"However , an issue in the writing , usage of external component and lack of experimental justification of the design choices hinder the clear understanding of the proposed model .",evaluation
SyKUVctlM_6,"An issue in the writing Overall , the paper is well written and easy to understand ,",evaluation
SyKUVctlM_7,but Section 3.2.3 ( The Write Unit ) has contradictory statements about their implementation .,fact
SyKUVctlM_8,"Specifically , they proposed three different ways to update the memory ( simple update , self attention and memory gate ) ,",fact
SyKUVctlM_9,but it is not clear which method is used in the end .,evaluation
SyKUVctlM_10,"Usage of external component The proposed model uses pretrained word vectors called GloVE , which has boosted the performance on visual question answering .",fact
SyKUVctlM_11,This experimental setting makes fair comparison with the previous works difficult,evaluation
SyKUVctlM_12,as the pre-trained word vectors are not used for the previous works .,fact
SyKUVctlM_13,"To isolate the strength of the proposed reasoning module , I ask to provide experiments without pretrained word vectors .",request
SyKUVctlM_14,"Lack of experimental justification of the design choices The proposed recurrent unit contains various design choices such as separation of three different units ( control unit , read unit and memory unit ) , attention based input processing and different memory updates stem from different motivations .",fact
SyKUVctlM_15,"However , these design choices are not justified well",evaluation
SyKUVctlM_16,because there is neither ablation study nor visualization of internal states .,fact
SyKUVctlM_17,Any analysis or empirical study on these design choices is necessary to understand the characteristics of the model .,request
SyKUVctlM_18,"Here , I suggest to provide few visualizations of attention weights and ablation study that could support indispensability of the design choices .",request
HyM5JWhgG_0,This paper discusses an application of survival analysis in social networks .,fact
HyM5JWhgG_1,"While the application area seems to be pertinent , the statistics as presented in this paper are suboptimal at best .",evaluation
HyM5JWhgG_2,"There is no useful statistical setup described ( what is random ? etc etc ) ,",fact
HyM5JWhgG_3,"the interplay between censoring and end-of-life is left rather fuzzy ,",evaluation
HyM5JWhgG_4,and mentioned clustering approaches are extensively studied in the statistical literature in so-called frailty analysis .,fact
HyM5JWhgG_5,The setting is also covered in statistics in the extensive literature on repeated measurements and even time-series analysis .,fact
HyM5JWhgG_6,It 's up to the authors discuss similarities and differences of results of the present approach and those areas .,request
HyM5JWhgG_7,The numerical result is not assessing the different design decisions of the approach ( why use a Kuyper loss ? ) in this empirical paper .,evaluation
B1_TQ-clG_0,This paper studies learning to play two-player general-sum games with state ( Markov games ) .,fact
B1_TQ-clG_1,The idea is to learn to cooperate ( think prisoner 's dilemma ) but in more complex domains .,fact
B1_TQ-clG_2,"Generally , in repeated prisoner 's dilemma , one can punish one 's opponent for noncooperation .",fact
B1_TQ-clG_3,"In this paper , they design an apporach to learn to cooperate in a more complex game , like a hybrid pong meets prisoner 's dilemma game .",fact
B1_TQ-clG_4,This is fun but I did not find it particularly surprising from a game-theoretic or from a deep learning point of view .,evaluation
B1_TQ-clG_5,"From a game-theoretic point of view , the paper begins with somewhat sloppy definitions followed by a theorem that is not very surprising .",evaluation
B1_TQ-clG_6,"It is basically a straightforward generalization of the idea of punishing , which is common in "" folk theorems "" from game theory , to give a particular equilibrium for cooperating in Markov games .",evaluation
B1_TQ-clG_7,"Many Markov games do not have a cooperative equilibrium , so this paper restricts attention to those that do .",fact
B1_TQ-clG_8,"Even in games where there is a cooperative solution that maximizes the total welfare , it is not clear why players would choose to do so .",evaluation
B1_TQ-clG_9,"When the game is symmetric , this might be "" the natural "" solution",evaluation
B1_TQ-clG_10,but in general it is far from clear why all players would want to maximize the total payoff .,fact
B1_TQ-clG_11,The paper follows with some fun experiments implementing these new game theory notions .,fact
B1_TQ-clG_12,"Unfortunately , since the game theory was not particularly well-motivated ,",evaluation
B1_TQ-clG_13,I did not find the overall story compelling .,evaluation
B1_TQ-clG_14,"It is perhaps interesting that one can make deep learning learn to cooperate ,",evaluation
B1_TQ-clG_15,but one could have illustrated the game theory equally well with other techniques .,evaluation
B1_TQ-clG_16,"In contrast , the paper "" Coco-Q : Learning in Stochastic Games with Side Payments "" by Sodomka et . al. is an example where they took a well-motivated game theoretic cooperative solution concept and explored how to implement that with reinforcement learning .",fact
B1_TQ-clG_17,I would think that generalizing such solution concepts to stochastic games and/or deep learning might be more interesting .,request
B1_TQ-clG_18,"It should also be noted that I was asked to review another ICLR submission entitled "" CONSEQUENTIALIST CONDITIONAL COOPERATION IN SOCIAL DILEMMAS WITH IMPERFECT INFORMATION """,non-arg
B1_TQ-clG_19,"which amazingly introduced the same "" Pong Player ’s Dilemma "" game as in this paper .",fact
B1_TQ-clG_20,"Notice the following suspiciously similar paragraphs from the two papers : From "" MAINTAINING COOPERATION IN COMPLEX SOCIAL DILEMMAS USING DEEP REINFORCEMENT LEARNING "" :",reference
B1_TQ-clG_21,We also look at an environment where strategies must be learned from raw pixels .,quote
B1_TQ-clG_22,We use the method of Tampuu et al. ( 2017 ) to alter the reward structure of Atari Pong so that whenever an agent scores a point they receive a reward of 1 and the other player receives − 2 .,quote
B1_TQ-clG_23,We refer to this game as the Pong Player ’s Dilemma ( PPD ) .,quote
B1_TQ-clG_24,In the PPD the only ( jointly ) winning move is not to play .,quote
B1_TQ-clG_25,"However , a fully cooperative agent can be exploited by a defector .",quote
B1_TQ-clG_26,"From "" CONSEQUENTIALIST CONDITIONAL COOPERATION IN SOCIAL DILEMMAS WITH IMPERFECT INFORMATION "" :",reference
B1_TQ-clG_27,To demonstrate this we follow the method of Tampuu et al. ( 2017 ) to construct a version of Atari Pong which makes the game into a social dilemma .,quote
B1_TQ-clG_28,In what we call the Pong Player ’s Dilemma ( PPD ) when an agent scores they gain a reward of 1 but the partner receives a reward of − 2 .,quote
B1_TQ-clG_29,"Thus , in the PPD the only ( jointly ) winning move is not to play ,",quote
B1_TQ-clG_30,but selfish agents are again tempted to defect and try to score points even though this decreases total social reward .,quote
B1_TQ-clG_31,"We see that CCC is a successful , robust , and simple strategy in this game .",quote
ByCeFUNgz_0,The authors propose a new episodic reinforcement learning algorithm based on contextual bandit oracles .,fact
ByCeFUNgz_1,"The key specificity of this algorithm is its ability to deal with the credit assignment problem by learning automatically a progressive "" reward shaping "" ( the residual losses ) from a feedback that is only provided at the end of the epochs .",fact
ByCeFUNgz_2,The paper is dense but well written .,evaluation
ByCeFUNgz_3,The theoretical grounding is a bit thin or hard to follow .,evaluation
ByCeFUNgz_4,"The authors provide a few regret theoretical results ( that I did not check deeply ) obtained by reduction to "" value-aware "" contextual bandits .",fact
ByCeFUNgz_5,The experimental section is solid .,evaluation
ByCeFUNgz_6,The method is evaluated on several RL environments against state of the art RL algorithms .,fact
ByCeFUNgz_7,It is also evaluated on bandit structured prediction tasks .,fact
ByCeFUNgz_8,An interesting synthetic experiment ( Figure 4 ) is also proposed to study the ability of the algorithm to work on both decomposable and non-decomposable structured prediction tasks .,fact
ByCeFUNgz_9,Question 1 : The credit assignment approach you propose seems way more sophisticated than eligibility traces in TD learning .,evaluation
ByCeFUNgz_10,But sometimes old and simple methods are not that bad .,evaluation
ByCeFUNgz_11,Could you develop a bit on the relation between RESLOPE and eligibility traces ?,request
ByCeFUNgz_12,Question 2 : RESLOPE is built upon contextual bandits which require a stationary environment .,fact
ByCeFUNgz_13,Does RESLOPE inherit from this assumption ?,request
ByCeFUNgz_14,"Typos : page 1 "" scalar loss that output . "" - > "" scalar loss . """,request
ByCeFUNgz_15," , effectively a representation  - > "" . By effective we mean effective in term of credit assignment . """,request
ByCeFUNgz_16,"page 5 "" and MTR "" - > "" and DR """,request
ByCeFUNgz_17,"page 6 "" in simultaneously . "" - > ???",request
ByCeFUNgz_18," . In greedy  - > "" . In greedy """,request
S1gFMVoeM_0,"This paper introduces a simple correlation-based metric to measure whether filters in neural networks are being used effectively , as a proxy for effective capacity .",fact
S1gFMVoeM_1,The authors then introduce a greedy algorithm,fact
S1gFMVoeM_2,that expands the different layers in a neural network until the metric indicates that additional features will end up not being used effectively .,fact
S1gFMVoeM_3,The application of this algorithm is shown to lead to architectures that differ substantially from hand-designed models with the same number of layers :,fact
S1gFMVoeM_4,"most of the parameters end up in intermediate layers , with fewer parameters in earlier and later layers .",fact
S1gFMVoeM_5,"This indicates that common heuristics to divide capacity over the layers of a network are suboptimal ,",fact
S1gFMVoeM_6,as they tend to put most parameters in later layers .,fact
S1gFMVoeM_7,It 's also nice that simpler tasks yield smaller models ( e.g. MNIST vs. CIFAR in figure 3 ) .,evaluation
S1gFMVoeM_8,The experimental section is comprehensive and the results are convincing .,evaluation
S1gFMVoeM_9,I especially appreciate the detailed analysis of the results ( figure 3 is great ) .,evaluation
S1gFMVoeM_10,"Although most experiments were conducted on the classic benchmark datasets of MNIST , CIFAR-10 and CIFAR-100 ,",fact
S1gFMVoeM_11,"the paper also includes some promising preliminary results on ImageNet ,",fact
S1gFMVoeM_12,which nicely demonstrates that the technique scales to more practical problems as well .,fact
S1gFMVoeM_13,"That said , it would be nice to demonstrate that the algorithm also works for other tasks than image classification .",request
S1gFMVoeM_14,"I also like the alternative perspective compared to pruning approaches ,",evaluation
S1gFMVoeM_15,which most research seems to have been focused on in the past .,fact
S1gFMVoeM_16,"The observation that the cross-correlation of a weight vector with its initial values is a good measure for effective filter use seems obvious in retrospect ,",evaluation
S1gFMVoeM_17,but hindsight is 20/20 and the fact is that apparently this has n't been tried before .,fact
S1gFMVoeM_18,It is definitely surprising that a simple method like this ends up working this well .,evaluation
S1gFMVoeM_19,"The fact that all parameters are reinitialised whenever any layer width changes seems odd at first ,",evaluation
S1gFMVoeM_20,but I think it is sufficiently justified .,evaluation
S1gFMVoeM_21,"It would be nice to see some comparison experiments as well though ,",request
S1gFMVoeM_22,as the intuitive thing to do would be to just keep the existing weights as they are .,evaluation
S1gFMVoeM_23,Other remarks : Formula ( 2 ) seems needlessly complicated because of all the additional indices .,evaluation
S1gFMVoeM_24,Maybe removing some of those would make things easier to parse .,request
S1gFMVoeM_25,It would also help to mention that it is basically just a normalised cross-correlation .,request
S1gFMVoeM_26,"This is mentioned two paragraphs down ,",fact
S1gFMVoeM_27,but should probably be mentioned right before the formula is given instead .,request
S1gFMVoeM_28,"page 6 , section 3.1 : "" it requires convergent training of a huge architecture with lots of regularization before complexity can be introduced "" ,",reference
S1gFMVoeM_29,"I guess this should be "" reduced "" instead of "" introduced "" .",request
HydgKG5ez_0,The paper proposes a CNN-based based approach for speech processing using raw waveforms as input .,fact
HydgKG5ez_1,An analysis of convolution and pooling layers applied on waveforms is first presented .,fact
HydgKG5ez_2,An architecture called SimpleNet is then presented and evaluated on two speech tasks : emotion recognition and gender classification .,fact
HydgKG5ez_3,This paper propose a theoretical analysis of convolution and pooling layers to motivate the SimpleNet architecture .,fact
HydgKG5ez_4,"To my understanding , the analysis is flawed ( see comments below ) .",evaluation
HydgKG5ez_5,The SimpleNet approach is interesting but not sufficiently backed with experimental results .,evaluation
HydgKG5ez_6,The network analysis is minimal and provides almost no insights .,evaluation
HydgKG5ez_7,I therefore recommend to reject the paper .,evaluation
HydgKG5ez_8,"Detailed comments : Section 1 : * “ Therefore , it remains unknown what actual features CNNs learn from waveform ” .",quote
HydgKG5ez_9,"This is not true ,",fact
HydgKG5ez_10,several works on speech recognition have shown that a convolution layer taking raw speech as input can be seen as a bank of learned filters .,fact
HydgKG5ez_11,"For instance in the context of speech recognition , [ 9 ] showed that the filters learn phoneme-specific responses ,",fact
HydgKG5ez_12,[ 10 ] showed that the learned filters are close to Mel filter banks,fact
HydgKG5ez_13,and [ 7 ] showed that the learned filters are related to MRASTA features and Gabor filters .,fact
HydgKG5ez_14,The authors should discuss these previous works in the paper .,request
HydgKG5ez_15,"Section 2 : * Section 2.1 seems unnecessary ,",evaluation
HydgKG5ez_16,I think it ’s safe to assume that the Shannon-Nyquist theorem and the definition of convolution are known by the reader .,evaluation
HydgKG5ez_17,* Section 2.2.2 & 2.2.3 : I do n't follow the justification that stacking convolutions are not needed :,evaluation
HydgKG5ez_18,"the example provided is correct if two convolutions are directly stacked without non-linearity , but the conclusion does not hold with a non-linearity and/or a pooling layer between the convolutions :",fact
HydgKG5ez_19,two stacked convolutions with non-linearities are not equivalent to a single convolution .,fact
HydgKG5ez_20,"To my understanding , the same problem is present for the pooling layer :",fact
HydgKG5ez_21,the presented conclusion that pooling introduces aliasing is only valid for two directly stacked pooling layers and is not correct for stacked blocks of convolution/pooling/non-linearity .,fact
HydgKG5ez_22,* Section 2.2.5 : The ReLU can be seen as a half-wave rectifier if it is applied directly to the waveform .,fact
HydgKG5ez_23,"However , it is usually not the case",fact
HydgKG5ez_24,as it is applied on the output of the convolution and/or pooling layers . Therefore I do n’t see the point of this section .,fact
HydgKG5ez_25,"* Section 2.2.6 : In this section , the authors discuss the differences between spectrogram-based and waveforms-based approaches , assuming that spectrogram-based approach have fixed filters .",fact
HydgKG5ez_26,But spectrogram can also be used as input to CNNs ( i.e. using learned filters ) for instance in speech recognition [ 1 ] or emotion recognition [ 11 ] .,fact
HydgKG5ez_27,Thus the comparison could be more interesting if it was between spectrogram-based and raw waveform-based approaches when the filters are learned in both cases .,request
HydgKG5ez_28,"Section 3 : * Figure 4 is very interesting ,",evaluation
HydgKG5ez_29,and is in my opinion a stronger motivation for SimpleNet that the analysis presented in Section 2 .,evaluation
HydgKG5ez_30,"* Using known filterbanks such as Mel or Gammatone filters as initialization point for the convolution layer is not novel and has been already investigated in [ 7,8,10 ] in the context of speech recognition .",fact
HydgKG5ez_31,"Section 4 : * On emotion recognition , the results show that the proposed approach is slightly better ,",evaluation
HydgKG5ez_32,but there is some issues : the average recall metric is usually used for this task due to class imbalance ( see [ 1 ] for instance ) .,fact
HydgKG5ez_33,Could the authors provide results with this metric ?,request
HydgKG5ez_34,"Also IEMOCAP is a well-used corpus for this task ,",evaluation
HydgKG5ez_35,could the authors provide some baselines performance for comparison ( e.g. [ 11 ] ) ?,request
HydgKG5ez_36,"* For gender classification , there is no gain from SimpleNet compared to the baselines .",fact
HydgKG5ez_37,The authors also mention that some utterances have overlapping speech .,fact
HydgKG5ez_38,"These utterances are easy to find from the annotations provided with the corpus ,",evaluation
HydgKG5ez_39,so it should be easy to remove them for the train and test set .,evaluation
HydgKG5ez_40,"Overall , in the current form , the results are not convincing .",evaluation
HydgKG5ez_41,* Section 4.3 : The analysis is minimal :,evaluation
HydgKG5ez_42,it shows that filters changed after training ( as already presented in Figure 4 ) .,fact
HydgKG5ez_43,I do n't follow completely the argument that the filters should focus on low frequency .,evaluation
HydgKG5ez_44,"It is more informative ,",evaluation
HydgKG5ez_45,"but one could expect that the filters will specialized , thus some of them will focus on high frequencies , to model the high frequency events such as consonants or unvoiced event .",evaluation
HydgKG5ez_46,It could be very interesting to relate the learned filters to the labels :,request
HydgKG5ez_47,are some filters learned to model specific emotions ?,request
HydgKG5ez_48,"For gender classification , are some filters focusing on the average pitch frequency of male and female speaker ?",request
HydgKG5ez_49,"* Finally , it would be nice to see if the claims in Section 2 about the fact that only one convolution layer is needed and that stacking pooling layers can hurt the performance are verified experimentally : for instance , experiments with more than one pair of convolution/pooling could be presented .",request
HydgKG5ez_50,"Minor comments : * More references for raw waveforms-based approach for speech recognition should be added [ 3,4,6,7,8,9 ] in the introduction .",request
HydgKG5ez_51,"* I do n’t understand the first sentence of the paper : “ In the field of speech and audio processing , due to the lack of tools to directly process high dimensional data … ” .",evaluation
HydgKG5ez_52,Is this also true for any pattern recognition fields ?,evaluation
HydgKG5ez_53,"* For the MFCCs reference in 2.2.2 , the authors should cite [ 12 ] .",request
HydgKG5ez_54,* Figure 6 : Only half of the spectrum should be presented .,request
HydgKG5ez_55,References : [1] <CIT>,reference
HydgKG5ez_56,[2] <CIT>,reference
HydgKG5ez_57,[3] <CIT>,reference
HydgKG5ez_58,[4] <CIT>,reference
HydgKG5ez_59,[5] <CIT>,reference
HydgKG5ez_60,[6] <CIT>,reference
HydgKG5ez_61,[7] <CIT>,reference
HydgKG5ez_62,[8] <CIT>,reference
HydgKG5ez_63,[9] <CIT>,reference
HydgKG5ez_64,[10] <CIT>,reference
HydgKG5ez_65,[11] <CIT>,reference
HydgKG5ez_66,[12] <CIT>,reference
SkPNib9ez_0,"This paper extends and speeds up PROSE , a programming by example system , by posing the selection of the next production rule in the grammar as a supervised learning problem .",fact
SkPNib9ez_1,This paper requires a large amount of background knowledge,evaluation
SkPNib9ez_2,as it depends on understanding program synthesis as it is done in the programming languages community .,fact
SkPNib9ez_3,"Moreover the work mentions a neurally-guided search ,",fact
SkPNib9ez_4,but little time is spent on that portion of their contribution .,evaluation
SkPNib9ez_5,I am not even clear how their system is trained .,evaluation
SkPNib9ez_6,The experimental results do show the programs can be faster but only if the user is willing to suffer a loss in accuracy .,fact
SkPNib9ez_7,It is difficult to conclude overall if the technique helps in synthesis .,evaluation
HkZ8Gb9eG_0,This paper is well constructed and written .,evaluation
HkZ8Gb9eG_1,It consists of a number of broad ideas regarding density estimation using transformations of autoregressive networks .,fact
HkZ8Gb9eG_2,"Specifically , the authors examine models involving linear maps from past states ( LAM ) and recurrence relationships ( RAM ) .",fact
HkZ8Gb9eG_3,The critical insight is that the hidden states in the LAM are not coupled allowing considerable flexibility between consecutive conditional distributions .,fact
HkZ8Gb9eG_4,This is at the expense of an increased number of parameters and a lack of information sharing .,fact
HkZ8Gb9eG_5,"In contrast , the RAM transfers information between conditional densities via the coupled hidden states allowing for more constrained smooth transitions .",fact
HkZ8Gb9eG_6,The authors then explored a variety of transformations designed to increase the expressiveness of LAM and RAM .,fact
HkZ8Gb9eG_7,The authors importantly note that one important restriction on the class of transformations is the ability to evaluate the Jacobian of the transformation efficiently .,fact
HkZ8Gb9eG_8,A composite of transformations coupled with the LAM/RAM networks provides a highly expressive model for modelling arbitrary joint densities but retaining interpretable conditional structure .,fact
HkZ8Gb9eG_9,There is a rich variety of synthetic and real data studies which demonstrate that LAM and RAM consistently rank amongst the top models demonstrating potential utility for this class of models .,evaluation
HkZ8Gb9eG_10,"Whilst the paper provides no definitive solutions , this is not the point of the work which seeks to provide a description of a general class of potentially useful models .",evaluation
Hk6aJkmWM_0,This paper proposes to use RGANs and RCGANS to generate synthetic sequences of actual data .,fact
Hk6aJkmWM_1,"They demonstrate the quality of the sequences on sine waves , MNIST , and ICU telemetry data .",fact
Hk6aJkmWM_2,"The authors demonstrate novel approaches for generating real-valued sequences using adversarial training , a train on synthetic , test of real and vice versa method for evaluating GANS , generating synthetic medical time series data , and an empirical privacy analysis .",fact
Hk6aJkmWM_3,Major - the medical use case is not motivating .,evaluation
Hk6aJkmWM_4,de-identifying the 4 telemetry measures is extremely easy,evaluation
Hk6aJkmWM_5,and there is little evidence to show that it is even possible to reidentify individuals using these 4 measures .,evaluation
Hk6aJkmWM_6,our institutional review board would certainly allow self-certification of the data ( i.e. removing the patient identifiers and publishing the first 4 hours of sequences ) .,non-arg
Hk6aJkmWM_7,- the labels selected by the authors for the icu example are to forecast the next 15 minutes and whether a critical value is reached .,fact
Hk6aJkmWM_8,Please add information about how this critical value was generated .,request
Hk6aJkmWM_9,"Also it would be very useful to say that a physician was consulted and that the critical values were "" clinically "" useful .",request
Hk6aJkmWM_10,- the changes in performance of TSTR are large enough that I would have difficulty trusting any experiments using the synthetic data .,evaluation
Hk6aJkmWM_11,"If I optimized a method using this synthetic data , I would still need to assess the result on real data .",evaluation
Hk6aJkmWM_12,- In addition it is unclear whether this synthetic process would actually generate results that are clinically useful .,evaluation
Hk6aJkmWM_13,The authors certainly make a convincing statement about the internal validity of the method .,evaluation
Hk6aJkmWM_14,An externally valid measure would strengthen the results .,evaluation
Hk6aJkmWM_15,I 'm not quite sure how the authors could externally validate the synthetic data,evaluation
Hk6aJkmWM_16,as this would also require generating synthetic outcome measures .,fact
Hk6aJkmWM_17,I think it would be possible for the synthetic sequence to also generate an outcome measure ( i.e. death ) based on the first 4 hours of stay .,fact
Hk6aJkmWM_18,Minor - write in the description for table 1 what task the accuracies correspond .,request
Hk6aJkmWM_19,Summary The authors present methods for generating synthetic sequences .,fact
Hk6aJkmWM_20,The MNIST example is compelling .,evaluation
Hk6aJkmWM_21,However the ICU example has some pitfalls which need to be addressed .,evaluation
ryoWUP5lz_0,This work proposes an approach for transcription factor binding site prediction using a multi-label classification formulation .,fact
ryoWUP5lz_1,It is a very interesting problem,evaluation
ryoWUP5lz_2,and application and the approach is interesting .,evaluation
ryoWUP5lz_3,"Novelty : The method is quite similar to matching networks ( Vinyals , 2016 ) with a few changes in the matching approach .",evaluation
ryoWUP5lz_4,"As such , in order to establish its broader applicability there should be additional evaluation on other benchmark datasets .",request
ryoWUP5lz_5,The MNIST performance comparison is inadequate,evaluation
ryoWUP5lz_6,and there are other papers that do better on it .,fact
ryoWUP5lz_7,They should clearly list what the contributions are w.r.t to the work by Vinyals et al 2016 .,request
ryoWUP5lz_8,They should also cite works that learn embeddings in a multi-label setting such as StarSpace .,request
ryoWUP5lz_9,Impact : In its current form the paper seems to be most relevant to the computational biology / TFBS community .,evaluation
ryoWUP5lz_10,"However , there is no comparison to the exact networks used in the prior works DeepBind/DeepSea/DanQ / Basset/DeepLift or bidirectional LSTMs .",fact
ryoWUP5lz_11,Further there is no comparison to existing one-shot learning techniques either .,fact
ryoWUP5lz_12,This greatly limits the impact of the work .,evaluation
ryoWUP5lz_13,"For biological impact , a comparison to any of the motif learning approaches that are popular in the biology/comp-bio community will help ( for instance , HOMER , FIMO ) .",request
ryoWUP5lz_14,"Cons : The authors claim they can learn TF-TF interactions and it is one of the main biological contributions ,",fact
ryoWUP5lz_15,but there is no evidence of why,fact
ryoWUP5lz_16,( beyond very preliminary evaluation using the Trrust database ) .,evaluation
ryoWUP5lz_17,Their examples are 200-bp long which does not mean that all TFs binding in that window are involved in cooperative binding .,fact
ryoWUP5lz_18,The prototype loss is too simplistic to capture co-binding tendencies,evaluation
ryoWUP5lz_19,and the combinationLSTM is not well motivated .,evaluation
ryoWUP5lz_20,"One interesting source of information they could tap into for TF-TF interactions is CAP-SELEX ( Jolma et al , Nature 2015 ) .",request
ryoWUP5lz_21,One of the main drawbacks is the lack of interpretability of their model where approaches like DanQ/DeepLift etc benefit .,evaluation
ryoWUP5lz_22,The PWM-like filters in some of the prior works help understand what type of sequence properties contribute to binding events .,fact
ryoWUP5lz_23,Can their model lead to an understanding of this sort ?,non-arg
ryoWUP5lz_24,Evaluation : The empirical evaluation itself is not very strong,evaluation
ryoWUP5lz_25,as there are only modest improvements over simple baselines .,evaluation
ryoWUP5lz_26,Further there are no error-bars etc to indicate the variance in their performance numbers .,fact
ryoWUP5lz_27,It will be useful to have a TF-level performance split-up to get an idea of which TFs benefit most .,request
ryoWUP5lz_28,Clarity : The paper can benefit from more clarity in the technical aspects .,request
ryoWUP5lz_29,It is hard to follow for anyone not already familiar with matching networks .,evaluation
ryoWUP5lz_30,"The objective function , parameters need to be clearly introduced in one place .",request
ryoWUP5lz_31,"For instance , what is y_i in their multi-label framework ?",request
ryoWUP5lz_32,"Various choices are not well motivated ; for instance cosine similarity , the value of hyperparameter epsilon .",evaluation
ryoWUP5lz_33,The prototype vectors are not motif-like at all --,evaluation
ryoWUP5lz_34,can the authors motivate this aspect better ?,request
Bk3B7T5gf_0,The authors present a deep neural network that evaluates plate numbers .,fact
Bk3B7T5gf_1,"The relevance of this problem is that there are auctions for plate numbers in Hong Kong , and predicting their value is a sensible activity in that context .",evaluation
Bk3B7T5gf_2,I find that the description of the applied problem is quite interesting ;,evaluation
Bk3B7T5gf_3,in fact overall the paper is well written and very easy to follow .,evaluation
Bk3B7T5gf_4,"There are some typos and grammatical problems ( indicated below ) , but nothing really serious .",evaluation
Bk3B7T5gf_5,"So , the paper is relevant and well presented .",evaluation
Bk3B7T5gf_6,"However , I find that the proposed solution is an application of existing techniques ,",fact
Bk3B7T5gf_7,so it lacks on novelty and originality .,evaluation
Bk3B7T5gf_8,"Even though the significance of the work is apparent given the good results of the proposed neural network ,",evaluation
Bk3B7T5gf_9,I believe that such material is more appropriate to a focused applied meeting .,evaluation
Bk3B7T5gf_10,"However , even for that sort of setting I think the paper requires some additional work ,",evaluation
Bk3B7T5gf_11,as some final parts of the paper have not been tested yet,fact
Bk3B7T5gf_12,( the interesting part of explanations ) .,evaluation
Bk3B7T5gf_13,Hence I do n't think the submission is ready for publication at this moment .,evaluation
Bk3B7T5gf_14,"Concerning the text , some questions/suggestions : - Abstract , line 1 : I suppose "" In the Chinese society ... "" --- are there many Chinese societies ?",request
Bk3B7T5gf_15,- The references are not properly formatted ;,evaluation
Bk3B7T5gf_16,"they should appear at ( XXX YYY ) but appear as XXX ( YYY ) in many cases , mixed with the main text .",fact
Bk3B7T5gf_17,"- Footnote 1 , line 2 : "" an exchange "" .",quote
Bk3B7T5gf_18,"- Page 2 , line 12 : "" prices . Among "" .",quote
Bk3B7T5gf_19,- Please add commas/periods at the end of equations .,request
Bk3B7T5gf_20,- There are problems with capitalization in the references .,evaluation
HkshYX9xz_0,"In this work , discrete-weight NNs are trained using the variational Bayesian framework , achieving similar results to other state-of-the-art models .",fact
HkshYX9xz_1,Weights use 3 bits on the first layer and are ternary on the remaining layers .,fact
HkshYX9xz_2,- Pros : The paper is well-written and connections with the literature properly established .,evaluation
HkshYX9xz_3,"The approach to training discrete-weights NNs , which is variational inference , is more principled than previous works ( but see below ) .",evaluation
HkshYX9xz_4,- Cons : The authors depart from the original motivation when the central limit theorem is invoked .,fact
HkshYX9xz_5,"Once we approximate the activations with Gaussians , do we have any guarantee that the new approximate lower bound is actually a lower bound ?",request
HkshYX9xz_6,This is not discussed .,fact
HkshYX9xz_7,"If it is not a lower bound , what is the rationale behind maximizing it ?",request
HkshYX9xz_8,"This seems to place this work very close to previous works , and not in the "" more principled "" regime the authors claim to seek .",evaluation
HkshYX9xz_9,The likelihood weighting seems hacky .,evaluation
HkshYX9xz_10,"The authors claim "" there are usually many more NN weights than there are data samples "" .",fact
HkshYX9xz_11,"If that is the case , then it seems that the prior dominating is indeed the desired outcome .",evaluation
HkshYX9xz_12,"A different , more flat prior ( or parameter sharing ) , can be used ,",evaluation
HkshYX9xz_13,"but the described reweighting seems to be actually breaking a good property of Bayesian inference ,",evaluation
HkshYX9xz_14,which is defecting to the prior when evidence is lacking .,fact
HkshYX9xz_15,"In terms of performance ( Table 1 ) , the proposed method seems to be on par with existing ones .",evaluation
HkshYX9xz_16,It is unclear then what the advantage of this proposal is .,evaluation
HkshYX9xz_17,"Sparsity figures are provided for the current approach ,",fact
HkshYX9xz_18,but those are not contrasted with existing approaches .,fact
HkshYX9xz_19,"Speedup is claimed with respect to an NN with real weights , but not with respect existing NNs with binary weights ,",fact
HkshYX9xz_20,which is the appropriate baseline .,evaluation
HkshYX9xz_21,"- Minor comments : Page 3 : Subscript t and variable t is used for the targets ,",fact
HkshYX9xz_22,but I ca n't find where it is defined .,non-arg
HkshYX9xz_23,"Only the names of the datasets used in the experiments are given ,",fact
HkshYX9xz_24,"but they are not described , or even better , shown in pictures ( maybe in a supplementary ) .",fact
HkshYX9xz_25,"The title of the paper says "" discrete-valued NNs "" .",fact
HkshYX9xz_26,"The weights are discrete , but the activations and outputs are continuous ,",fact
HkshYX9xz_27,so I find it confusing .,evaluation
HkshYX9xz_28,"As a contrast , I would be less surprised to hear a sigmoid belief network called a "" discrete-valued NN "" , even though its weights are continuous .",evaluation
BkviGptxG_0,"This paper presents an alternative approach to constructing variational lower bounds on data log likelihood in deep , directed generative models with latent variables .",fact
BkviGptxG_1,"Specifically , the authors propose using approximate posteriors shared across groups of examples , rather than posteriors which treat examples independently .",fact
BkviGptxG_2,"The group-wise posteriors allow amortization of the information cost <VAR> across all examples in the group ,",fact
BkviGptxG_3,"which the authors liken to the "" KL annealing "" tricks that are sometimes used to avoid posterior collapse when training models with strong decoders <VAR> using current techniques for approximate variational inference in deep nets .",fact
BkviGptxG_4,"The presentation of the core idea is solid ,",evaluation
BkviGptxG_5,though it did take two read-throughs before the equations really clicked for me .,non-arg
BkviGptxG_6,I think the paper could be improved by spending more time on a detailed description of the model for the Omniglot experiments ( as illustrated in Figure 3 ) .,request
BkviGptxG_7,"E.g. , explicitly describing how group-wise and per-example posteriors are composed in this model , using Equations and pseudo-code for the main training loop , would have saved me some time .",request
BkviGptxG_8,"For readers less familiar with amortized variational inference in deep nets , the benefit would be larger .",evaluation
BkviGptxG_9,"I appreciate that the authors developed extensions of the core method to more complex group structures ,",evaluation
BkviGptxG_10,though I did n't find the related experiments particularly convincing .,evaluation
BkviGptxG_11,"Overall , I like this paper",evaluation
BkviGptxG_12,and think the underlying group-wise posterior construction trick is worth exploring further .,evaluation
BkviGptxG_13,"Of course , the elephant in the room is how to determine the groups across which the posteriors can be shared and their information costs amortized .",evaluation
B1MeHT3rG_0,This paper proposes a model for generating pop music melodies with a recurrent neural network conditioned on chord and part ( song section ) information .,fact
B1MeHT3rG_1,They train their model on a small dataset and compare it to a few existing models in a human evaluation .,fact
B1MeHT3rG_2,"I think this paper has many issues , which I describe below .",evaluation
B1MeHT3rG_3,"As a broad overview , the use of what the authors call "" word "" representations of notes is not novel ( appearing first in BachBot and the PerformanceRNN ) ;",fact
B1MeHT3rG_4,I suspect the model may be outputting sequences from the training set ;,fact
B1MeHT3rG_5,and the dataset is heavily constrained in a way that will make producing pleasing melodies easily but heavily limits the possible outputs of the model .,fact
B1MeHT3rG_6,The paper is also missing important references and is confusingly laid out ( e.g. introducing a GAN model in a few paragraphs in the experiments ) .,evaluation
B1MeHT3rG_7,"Specific criticism : - "" often producing works that are indistinguishable from human works ( Goodfellow et al. ( 2014 ) ; Radford et al. ( 2016 ) ; Potash et al. ( 2015 ) ) . "" I would definitely not say that any of the cited papers produce anything that could be confused as "" real "" ;",evaluation
B1MeHT3rG_8,"e.g. the early GAN papers you cite were not even close ( maybe somewhat close for images of bedrooms , which is a limited domain and certainly can not be considered a "" work "" ) .",evaluation
B1MeHT3rG_9,- There are many unsubstantiated claims in the second paragraph .,fact
B1MeHT3rG_10,"E.g. "" there is yet a certain aspect about it that makes it sound like ( or not sound like ) human-written music . """,quote
B1MeHT3rG_11,What is it ?,evaluation
B1MeHT3rG_12,What evidence do we have that this is true ?,evaluation
B1MeHT3rG_13, notes in the chorus part generally tend to be more high-pitched ,quote
B1MeHT3rG_14,Really ?,fact
B1MeHT3rG_15,Where was this measured ?,fact
B1MeHT3rG_16," music is not merely a series of notes , but entails an overall structure of its own ",quote
B1MeHT3rG_17,"Sure , but natural images are not merely a series of pixels either , and they certainly have structure , but we are making lots of good progress modeling them . Etc .",fact
B1MeHT3rG_18,- Your related work section is lacking .,evaluation
B1MeHT3rG_19,"For example , Eck & Schmidhuber in 2002 proposed using LSTMs for music composition , which is not much later than works from "" the early days "" despite having not "" employed rule or template based approach "" .",fact
B1MeHT3rG_20,Your note/time offset/duration representation is very similar to that of BachBot ( by Liang ) and Magenta 's PerformanceRNN .,evaluation
B1MeHT3rG_21,"GANs were also previously applied to piano roll generation ,",fact
B1MeHT3rG_22,"see MuseGAN ( Dong et al ) , MidiNet ( Yang et al ) , etc .",reference
B1MeHT3rG_23,Your critique of Jaques et al. is misleading ;,evaluation
B1MeHT3rG_24, they defined a number of music-theory based rules to set up the reward function ,quote
B1MeHT3rG_25,"is the whole point - this is an optional step which improves results , and there is no reason a priori to think that hand-designing regularizers is better than hand-designing RL objectives .",fact
B1MeHT3rG_26,"- The analogy to image captioning models is interesting ,",evaluation
B1MeHT3rG_27,but this type of image captioning model is not only model which is effectively a conditional language model - any sequence-to-sequence model can be looked at this way .,fact
B1MeHT3rG_28,"I do n't think that these image captioning models are even the most commonly known example ,",evaluation
B1MeHT3rG_29,so I 'm not sure why the proposed approach is being proposed in analogy to image captioning .,evaluation
B1MeHT3rG_30,"- I do n't think you need to reproduce the LSTM equations in your text , they are well-known .",evaluation
B1MeHT3rG_31,"- You should define early on what you mean by "" part "" ,",request
B1MeHT3rG_32,"I think you mean the song 's section ( verse , chorus , etc )",evaluation
B1MeHT3rG_33,but I have also heard this used to refer to the different instruments in a song .,non-arg
B1MeHT3rG_34,I do n't think you should expect this term to be known outside of musical communities ( e.g. the ICLR community ) .,evaluation
B1MeHT3rG_35,"- It seems simpler ( and more in keeping with the current zeigeist , e.g. the image captioning models you refer to ) to replace your HMM with a model that",request
B1MeHT3rG_36,"- The regularization is interesting ,",evaluation
B1MeHT3rG_37,but a simpler way to enforce this constraint would be to just only allow the model to produce notes within that predefined range .,evaluation
B1MeHT3rG_38,"Since you effectively constrain it to an octave ,",fact
B1MeHT3rG_39,it would be simple to wrap all notes in your training data into this octave .,evaluation
B1MeHT3rG_40,This baseline is probably worth comparing to,request
B1MeHT3rG_41,since it is substantially simpler than your regularizer .,evaluation
B1MeHT3rG_42,- You write that the softmax cost should have <VAR> added to it for the regularizer .,fact
B1MeHT3rG_43,"First , you do n't define E anywhere , you only introduce it in its derivative",fact
B1MeHT3rG_44,"( and of course you ca n't "" define "" the derivative of an expression , it 's an analytically computed quantity ) .",fact
B1MeHT3rG_45,"Second , are you sure you mean that the partial derivative should be added , and not the cost C itself ?",evaluation
B1MeHT3rG_46,"- Your results showing that human raters preferred your models are impressive ,",evaluation
B1MeHT3rG_47,but you have made the task easier for yourself in various ways :,fact
B1MeHT3rG_48,1 ) Constraining the training data to pop music,fact
B1MeHT3rG_49,2 ) Making all of the training data in a single ( major ) key,fact
B1MeHT3rG_50,3 ) Effectively limiting the melody range to within a single octave .,fact
B1MeHT3rG_51,"- It sounds very much like your model is repeating bars , e.g. it generates a melody of length N bars , then repeats this melody .",evaluation
B1MeHT3rG_52,Is this something you hard-coded into the model ?,non-arg
B1MeHT3rG_53,It would be very surprising if it learned to exhibit this behavior on its own .,evaluation
B1MeHT3rG_54,"If you hard-coded it into the model , I would expect it to sound better to human raters ,",evaluation
B1MeHT3rG_55,but this is a strong heuristic .,evaluation
B1MeHT3rG_56,"- I 'd suggest you provide example melodies from your model in isolation ( more like the "" varying number of bars "" examples ) rather than as part of a full music mix",request
B1MeHT3rG_57,- this makes it easier to judge the quality of the model 's output .,evaluation
B1MeHT3rG_58,- The GAN experiments are interesting but come as a big surprise and are largely orthogonal to the other model ;,evaluation
B1MeHT3rG_59,why not include this in your model description section ?,request
B1MeHT3rG_60,The model and training details are not adequately described,fact
B1MeHT3rG_61,and I do n't think it adds much to the paper to include it .,evaluation
B1MeHT3rG_62,"Furthermore it 's quite similar to the MidiNet and MuseGAN , so maybe it should be introduced as a baseline instead .",request
B1MeHT3rG_63,- How did you order the notes for chords ?,non-arg
B1MeHT3rG_64,"If three notes occur simultaneously ( in a chord ) , there 's no a priory correct way to list them sequentially ( two with an interval of length zero between notes ) .",fact
B1MeHT3rG_65,"- "" Generated instruments sound fairly in tune individually , confirming that our proposed model is applicable to other instruments as well "" Assuming you are still using C-major-only melodies , it 's not surprising that the generations sound in tune !",evaluation
B1MeHT3rG_66,- It is not surprising that your model ends up overfitting,evaluation
B1MeHT3rG_67,"because your dataset is very small ,",evaluation
B1MeHT3rG_68,"your model is very powerful ,",evaluation
B1MeHT3rG_69,and your regularizer does not really limit the model 's capacity much .,evaluation
B1MeHT3rG_70,I suspect that your model is overfitting even earlier than you think .,fact
B1MeHT3rG_71,You should check that none of the sequences output by your model appear in the training set .,request
B1MeHT3rG_72,You could easily compute n-gram overlap of the generated sequences vs. the training set .,request
B1MeHT3rG_73,At what point did you stop training before running the human evaluation ?,non-arg
B1MeHT3rG_74,"If you let your model overfit , then of course it will generate very human-sounding melodies ,",fact
B1MeHT3rG_75,but this is not a terribly interesting generative model .,evaluation
B1BFRS7ZM_0,"There may be some interesting ideas here ,",evaluation
B1BFRS7ZM_1,but I think in many places the mathematical description is very confusing and/or flawed .,evaluation
B1BFRS7ZM_2,"To give some examples : * Just before section 2.1.1 , <EQN> : it 's not clear at all clear that this defines a valid distribution over trees .",evaluation
B1BFRS7ZM_3,There is an implicit order over the paths in <VAR>,fact
B1BFRS7ZM_4,that is simply not defined,fact
B1BFRS7ZM_5,( otherwise how for <VAR> could we decide which symbols <VAR> to condition upon ? ),evaluation
B1BFRS7ZM_6, We can write <VAR> ... ,quote
B1BFRS7ZM_7,"with S , O and v defined as sets .",fact
B1BFRS7ZM_8,"This is certainly non-standard notation ,",evaluation
B1BFRS7ZM_9,more explanation is needed .,request
B1BFRS7ZM_10, The observation is generated by the sequence of left most production rules  .,quote
B1BFRS7ZM_11,This appears to be related to the idea of left-most derivations in context-free grammars .,evaluation
B1BFRS7ZM_12,"But no discussion is given ,",fact
B1BFRS7ZM_13,and the writing is again vague/imprecise .,evaluation
B1BFRS7ZM_14," Although the above grammar is not , in general , context free ",quote
B1BFRS7ZM_15,- I 'm not sure what is being referred to here .,non-arg
B1BFRS7ZM_16,"Are the authors referring to the underlying grammar , or the lack of independence assumptions in the model ?",non-arg
B1BFRS7ZM_17,The grammar is clearly context-free ;,evaluation
B1BFRS7ZM_18,the lack of independence assumptions is a separate issue .,fact
B1BFRS7ZM_19," In a probabilistic context-free grammar ( PCFG ) , all production rules are independent  :",quote
B1BFRS7ZM_20,"this is not an accurate statement ,",evaluation
B1BFRS7ZM_21,it 's not clear what is meant by production rules being independent .,evaluation
B1BFRS7ZM_22,"More accurate would be to say that the choice of rule is conditionally independent of all other information earlier in the derivation , once the non-terminal being expanded is conditioned upon .",request
HkdTXw1bM_0,"The paper takes an interesting approach to solve the existing problems of GAN training , using Coulomb potential for addressing the learning problem .",evaluation
HkdTXw1bM_1,"It is also well written with a clear presentation of the motivation of the problems it is trying to address , the background and proves the optimality of the suggested solution .",evaluation
HkdTXw1bM_2,My understanding and validity of the proof is still an educated guess .,non-arg
HkdTXw1bM_3,"I have been through section A. 2 , but I 'm unfamiliar with the earlier literature on the similar topics so I would not be able to comment on it .",non-arg
HkdTXw1bM_4,"Overall , I think this is a good paper that provides a novel way of looking at and solving problems in GANs .",evaluation
HkdTXw1bM_5,I just had a couple of points in the paper that I would like some clarification on :,evaluation
HkdTXw1bM_6,* In section 2.2.1 : The notion of the generated a_i not disappearing is something I did not follow .,evaluation
HkdTXw1bM_7,"What does it mean for a generated sample to "" not disappear "" ?",request
HkdTXw1bM_8,and this directly extends to the continuity equation in ( 2 ) .,fact
HkdTXw1bM_9,"* In section 1 : in the explanation of the 3rd problem that GANs exhibit i.e. the generator not being able to generalize the distribution of the input samples , I was hoping if you could give a bit more motivation as to why this happens .",request
HkdTXw1bM_10,"I do n't think this needs to be included in the paper ,",evaluation
HkdTXw1bM_11,but would like to have it for a personal clarification .,request
BJHwtGogM_0,"The paper proposes data augmentation as an alternative to commonly used regularisation techniques like weight decay and dropout , and shows for a few reference models / tasks that the same generalization performance can be achieved using only data augmentation .",fact
BJHwtGogM_1,I think it 's a great idea to investigate the effects of data augmentation more thoroughly .,evaluation
BJHwtGogM_2,"While it is a technique that is often used in literature ,",evaluation
BJHwtGogM_3,there has n't really been any work that provides rigorous comparisons with alternative approaches and insights into its inner workings .,evaluation
BJHwtGogM_4,Unfortunately I feel that this paper falls short of achieving this .,evaluation
BJHwtGogM_5,"Experiments are conducted on two fairly similar tasks ( image classification on CIFAR-10 and CIFAR-100 ) , with two different network architectures .",fact
BJHwtGogM_6,This is a bit meager to be able to draw general conclusions about the properties of data augmentation .,evaluation
BJHwtGogM_7,"Given that this work tries to provide insight into an existing common practice ,",fact
BJHwtGogM_8,I think it is fair to expect a much stronger experimental section .,evaluation
BJHwtGogM_9,"In section 2.1.1 it is stated that this was a conscious choice because simplicity would lead to clearer conclusions ,",fact
BJHwtGogM_10,"but I think the conclusions would be much more valuable if variety was the objective instead of simplicity , and if larger-scale tasks were also considered .",evaluation
BJHwtGogM_11,"Another concern is that the narrative of the paper pits augmentation against all other regularisation techniques , whereas more typically these will be used in conjunction .",evaluation
BJHwtGogM_12,It is however very interesting that some of the results show that augmentation alone can sometimes be enough .,evaluation
BJHwtGogM_13,"I think extending the analysis to larger datasets such as ImageNet , as is suggested at the end of section 3 , and probably also to different problems than image classification , is going to be essential to ensure that the conclusions drawn hold weight .",evaluation
BJHwtGogM_14,"Comments : - The distinction between "" explicit "" and "" implicit "" regularisation is never clearly enunciated .",evaluation
BJHwtGogM_15,"A bunch of examples are given for both ,",fact
BJHwtGogM_16,but I found it tricky to understand the difference from those .,evaluation
BJHwtGogM_17,Initially I thought it reflected the intention behind the use of a given technique ;,evaluation
BJHwtGogM_18,i.e. weight decay is explicit because clearly regularisation is its primary purpose --,fact
BJHwtGogM_19,whereas batch normalisation is implicit because its regularisation properties are actually a side effect .,fact
BJHwtGogM_20,"However , the paper then goes on to treat data augmentation as distinct from other explicit regularisation techniques ,",fact
BJHwtGogM_21,so I guess this is not the intended meaning .,evaluation
BJHwtGogM_22,"Please clarify this , as the terms crop up quite often throughout the paper .",request
BJHwtGogM_23,I suspect that the distinction is somewhat arbitrary and not that meaningful .,evaluation
BJHwtGogM_24,"- In the abstract , it is already implied that data augmentation is superior to certain other regularisation techniques because it does n't actually reduce the capacity of the model .",fact
BJHwtGogM_25,But this ignores the fact that some of the model 's excess capacity will be used to model out-of-distribution data ( w.r.t. the original training distribution ) instead .,fact
BJHwtGogM_26,Data augmentation always modifies the distribution of the training data .,fact
BJHwtGogM_27,I do n't think it makes sense to imply that this is always preferable over reducing model capacity explicitly .,evaluation
BJHwtGogM_28,This claim is referred to a few times throughout the work .,fact
BJHwtGogM_29,- It could be more clearly stated that the reason for the regularising effect of batch normalisation is the noise in the batch estimates for mean and variance .,request
BJHwtGogM_30,- Some parts of the introduction could be removed,request
BJHwtGogM_31,"because they are obvious , at least to an ICLR audience ( like "" the model would not be regularised if alpha ( the regularisation parameter ) equals 0 "" ) .",evaluation
BJHwtGogM_32,- The experiments with smaller dataset sizes would be more interesting if smaller percentages were used .,evaluation
BJHwtGogM_33,50 % / 80 % / 100 % are all on the same order of magnitude,fact
BJHwtGogM_34,and this setting is not very realistic .,evaluation
BJHwtGogM_35,"In practice , when a dataset is "" too small "" to be able to train a network that solves a problem reliably , it will generally be one or more orders of magnitude too small , not 2x too small .",fact
BJHwtGogM_36,"- The choices of hyperparameters for "" light "" and "" heavy "" motivation seem somewhat arbitrary and are not well motivated .",evaluation
BJHwtGogM_37,"Some parameters which are sampled uniformly at random should be probably be sampled log-uniformly instead ,",request
BJHwtGogM_38,because they represent scale factors .,fact
BJHwtGogM_39,"It should also be noted that much more extreme augmentation strategies have been used for this particular task in literature , in combination with padding ( for example by Graham ) .",fact
BJHwtGogM_40,It would be interesting to include this setting in the experiments as well .,request
BJHwtGogM_41,"- On page 7 it is stated that "" when combined with explicit regularization , the results are much worse than without it "" ,",fact
BJHwtGogM_42,but these results are omitted from the table .,fact
BJHwtGogM_43,This is unfortunate,evaluation
BJHwtGogM_44,"because it is a very interesting observation , that runs counter to the common practice of combining all these regularisation techniques together ( e.g. L2 + dropout + data augmentation is a common combination ) .",evaluation
BJHwtGogM_45,Delving deeper into this could make the paper a lot stronger .,evaluation
BJHwtGogM_46,- It is not entirely true that augmentation parameters depend only on the training data and not the architecture ( last paragraph of section 2.4 ) .,fact
BJHwtGogM_47,"Clearly more elaborate architectures benefit more from data augmentation , and might need heavier augmentation to perform optimally",evaluation
BJHwtGogM_48,because they are more prone to overfitting,evaluation
BJHwtGogM_49,( this is in fact stated earlier on in the paper as well ) .,fact
BJHwtGogM_50,It is of course true that these hyperparameters tend to be much more robust to architecture changes than those of other regularisation techniques such as dropout and weight decay .,fact
BJHwtGogM_51,This increased robustness is definitely useful,evaluation
BJHwtGogM_52,and I think this is also adequately demonstrated in the experiments .,evaluation
BJHwtGogM_53,"- Phrases like "" implicit regularization operates more effectively at capturing reality "" are too vague to be meaningful .",evaluation
BJHwtGogM_54,- Note that weight decay has also been found to have side effects related to optimization,fact
BJHwtGogM_55,"( e.g. in "" Imagenet classification with deep convolutional neural networks "" , Krizhevsky et al . )",reference
Hy0xrQegf_0,"This paper 's main thesis is that automatic metrics like BLEU , ROUGE , or METEOR is suitable for task-oriented natural language generation ( NLG ) .",fact
Hy0xrQegf_1,"In particular , the paper presents a counterargument to "" How NOT To Evaluate Your Dialogue System ... """,fact
Hy0xrQegf_2,where Wei et al argue that automatic metrics are not correlated or only weakly correlated with human eval on dialogue generation .,fact
Hy0xrQegf_3,The authors here show that the performance of various NN models as measured by automatic metrics like BLEU and METEOR is correlated with human eval .,fact
Hy0xrQegf_4,"Overall , this paper presents a useful conclusion : use METEOR for evaluating task oriented NLG .",evaluation
Hy0xrQegf_5,"However , there is n't enough novel contribution in this paper to warrant a publication .",evaluation
Hy0xrQegf_6,Many of the details unnecessary :,evaluation
Hy0xrQegf_7,1 ) various LSTM model descriptions are unhelpful,evaluation
Hy0xrQegf_8,given the base LSTM model does just as well on the presented tasks,fact
Hy0xrQegf_9,2 ) Many embedding based eval methods are proposed,evaluation
Hy0xrQegf_10,but no conclusions are drawn from any of these techniques .,fact
Sy4mWsOeG_0,"Many black-box optimization problems are "" multi-fidelity "" , in which it is possible to acquire data with different levels of cost and associated uncertainty .",fact
Sy4mWsOeG_1,"The training of machine learning models is a common example , in which more data and/or more training may lead to more precise measurements of the quality of a hyperparameter configuration .",fact
Sy4mWsOeG_2,"This has previously been referred to as a special case of "" multi-task "" Bayesian optimization , in which the tasks can be constructed to reflect different fidelities .",fact
Sy4mWsOeG_3,"The present paper examines this construction with three twists : using the knowledge gradient acquisition function , using batched function evaluations , and incorporating derivative observations .",fact
Sy4mWsOeG_4,"Broadly speaking , the idea is to allow fidelity to be represented as a point in a hypercube and then include this hypercube as a covariate in the Gaussian process .",fact
Sy4mWsOeG_5,"The knowledge gradient acquisition function then becomes "" knowledge gradient per unit cost "" the KG equivalent to the "" expected improvement per unit cost "" discussed in Snoek et al ( 2012 ) ,",fact
Sy4mWsOeG_6,although that paper did not consider treating fidelity separately .,fact
Sy4mWsOeG_7,"I do n't understand the claim that this is "" the first multi-fidelity algorithm that can leverage gradients "" .",evaluation
Sy4mWsOeG_8,"Ca n't any Gaussian process model use gradient observations trivially , as discussed in the Rasmussen and Williams book ?",fact
Sy4mWsOeG_9,Why ca n't any EI or entropy search method also use gradient observations ?,non-arg
Sy4mWsOeG_10,"This does n't usually come up in hyperparameter optimization ,",fact
Sy4mWsOeG_11,but it seems like a grandiose claim .,evaluation
Sy4mWsOeG_12,"Similarly , although I do n't know of a paper that explicitly does "" A + B "" for multi-fidelity BO and parallel BO ,",evaluation
Sy4mWsOeG_13,"it is an incremental contribution to combine them , not least because no other parallel BO methods get evaluated as baselines .",evaluation
Sy4mWsOeG_14,Figure 1 does not make sense to me .,evaluation
Sy4mWsOeG_15,How can the batched algorithm outperform the sequential algorithm on total cost ?,evaluation
Sy4mWsOeG_16,The sequential cfKG algorithm should always be able to make better decisions with its remaining budget than 8-cfKG .,fact
Sy4mWsOeG_17,"Is the answer that "" cost "" here means "" wall-clock time when parallelism is available "" ?",non-arg
Sy4mWsOeG_18,"If that 's the case , then it is necessary to include plots of parallelized EI , entropy search , and KG .",request
Sy4mWsOeG_19,The same is true for Figure 2 ; other parallel BO algorithms need to appear .,request
HJzYCPDlf_0,Twin Networks : Using the Future as a Regularizer,non-arg
HJzYCPDlf_1,** PAPER SUMMARY ** The authors propose to regularize RNN for sequence prediction by forcing states of the main forward RNN to match the state of a secondary backward RNN .,fact
HJzYCPDlf_2,Both RNNs are trained jointly and only the forward model is used at test time .,fact
HJzYCPDlf_3,"Experiments on conditional generation ( speech recognition , image captioning ) , and unconditional generation ( MNIST pixel RNN , language models ) show the effectiveness of the regularizer .",fact
HJzYCPDlf_4,"** REVIEW SUMMARY ** The paper reads well , has sufficient reference .",evaluation
HJzYCPDlf_5,The idea is simple and well explained .,evaluation
HJzYCPDlf_6,Positive empirial results support the proposed regularizer .,fact
HJzYCPDlf_7,"** DETAILED REVIEW ** Overall , this is a good paper .",evaluation
HJzYCPDlf_8,I have a few suggestions along the text but nothing major .,evaluation
HJzYCPDlf_9,"In related work , I would cite co-training approaches .",request
HJzYCPDlf_10,"In effect , you have two view of a point in time , its past and its future and you force these two views to agree ,",fact
HJzYCPDlf_11,"see ( Blum and Mitchell , 1998 ) or Xu , Chang , Dacheng Tao , and Chao Xu . "" A survey on multi-view learning . "" arXiv preprint arXiv : 1304.5634 ( 2013 ) .",reference
HJzYCPDlf_12,I would also relate your work to distillation/model compression which tries to get one network to behave like another .,request
HJzYCPDlf_13,"On that point , is it important to train the forward and backward network jointly or could the backward network be pre-trained ?",request
HJzYCPDlf_14,"In section 2 , it is not obvious to me that the regularizer ( 4 ) would not be ignored in absence of regularization on the output matrix .",evaluation
HJzYCPDlf_15,"I mean , the regularizer could push h ^ b to small norm , compensating with higher norm for the output word embeddings .",fact
HJzYCPDlf_16,Could you comment why this would not happen ?,request
HJzYCPDlf_17,"In Section 4.2 , you need to refer to Table 2 in the text .",request
HJzYCPDlf_18,You also need to define the evaluation metrics used .,request
HJzYCPDlf_19,"In this section , why are you not reporting the results from the original Show & Tell paper ?",request
HJzYCPDlf_20,How does your implementation compare to the original work ?,request
HJzYCPDlf_21,"On unconditional generation , your hypothesis on uncertainty is interesting and could be tested .",evaluation
HJzYCPDlf_22,"You could inject uncertainty in the captioning task for instance , e.g. consider that multiple version of each word e.g. dogA , dogB , docC which are alternatively used instead of dog with predefined substitution rates .",request
HJzYCPDlf_23,Would your regularizer still be helpful there ?,request
HJzYCPDlf_24,At which point would it break ?,request
H1--71dlz_0,"The paper proposes to improve the kernel approximation of random features by using quadratures , in particular , stochastic spherical-radial rules .",fact
H1--71dlz_1,"The quadrature rules have smaller variance given the same number of random features ,",fact
H1--71dlz_2,and experiments show its reconstruction error and classification accuracies are better than existing algorithms .,fact
H1--71dlz_3,"It is an interesting paper ,",evaluation
H1--71dlz_4,"but it seems the authors are not aware of some existing works [ 1 , 2 ] on quadrature for random features .",fact
H1--71dlz_5,"Given these previous works , the contribution and novelty of the paper is limited .",evaluation
H1--71dlz_6,[1] <CIT>,reference
H1--71dlz_7,[2] <CIT>,reference
rycISJNgz_0,"Quality The method description , particularly about reference ambiguity , I found difficult to follow .",evaluation
rycISJNgz_1,"The experiments and analysis look solid ,",evaluation
rycISJNgz_2,although it would be nice to see experiments on more challenging natural image datasets .,evaluation
rycISJNgz_3,Clarity “ In general this is not possible … “ -,quote
rycISJNgz_4,you are saying it is not possible to learn an encoder that recovers disentangled factors of variation ?,fact
rycISJNgz_5,But that seems to be one of the main goals of the paper .,fact
rycISJNgz_6,"It is not clear at all what is meant here or what the key problem is ,",evaluation
rycISJNgz_7,which detracts from the paper ’s motivation .,fact
rycISJNgz_8,What is the purpose of R_v and R_c in eq 2 ?,request
rycISJNgz_9,Why can these not be collapsed into the encoders N_v and N_c ?,request
rycISJNgz_10,What does “ different common factor ” mean ?,request
rycISJNgz_11,What is f_c in proof of proposition 1 ?,request
rycISJNgz_12,Previously f ( no subscript ) was referred to as a rendering engine .,fact
rycISJNgz_13,<VAR> and <VAR> are said to be independent .,fact
rycISJNgz_14,But <VAR> is explicitly defined in terms of c ( equation 6 ) .,fact
rycISJNgz_15,So which is correct ?,request
rycISJNgz_16,Overall the argument seems plausible -,evaluation
rycISJNgz_17,pairs of images in which a single factor of variation changes have a reference ambiguity -,fact
rycISJNgz_18,but the details are unclear .,evaluation
rycISJNgz_19,"Originality The model is very similar to Mathieu et al , although using image pairs rather than category labels directly .",evaluation
rycISJNgz_20,"The idea of weakly-supervised disentangling has also been explored in many other papers ,",fact
rycISJNgz_21,e.g. <CIT>,reference
rycISJNgz_22,"The description of reference ambiguity seems new and potentially valuable ,",evaluation
rycISJNgz_23,but I did not find it easy to follow .,evaluation
rycISJNgz_24,"Significance Disentangling factors of variation with weak supervision is an important problem ,",evaluation
rycISJNgz_25,and this paper makes a modest advance in terms of the model and potentially in terms of the theory .,evaluation
rycISJNgz_26,The analysis in figure 3 I found particularly interesting - illustrating that the encoder embedding dimension can have a drastic effect on the shortcut problem .,evaluation
rycISJNgz_27,Overall I think this can be a significant contribution if the exposition can be improved .,request
rycISJNgz_28,Pros - Proposed method allows disentangling two factors of variation given a training set of image pairs with one factor of variation matching and the other non-matching .,fact
rycISJNgz_29,- A challenge inherent to weakly supervised disentangling called reference ambiguity is described .,fact
rycISJNgz_30,"Cons - Only two factors of variation are studied ,",fact
rycISJNgz_31,and the datasets are fairly simple .,evaluation
rycISJNgz_32,- The method description and the description of reference ambiguity are unclear .,evaluation
rkZAtAaxM_0,"This manuscript is fairly well-written ,",evaluation
rkZAtAaxM_1,and discusses how the batch normalization step helps to stabilize the scale of the gradients .,fact
rkZAtAaxM_2,"Intriguingly , the analysis suggests that using a shallower but wider resnet should provide competitive performance , which is supported by empirical evidence .",fact
rkZAtAaxM_3,"This work should help elucidate the structure in the learning , and help to support efforts to improve both learning algorithms and the architecture .",evaluation
rkZAtAaxM_4,"Pros : Clean , simple analysis",evaluation
rkZAtAaxM_5,Empirical support suggests that theory captures reasonable effects behind learning,fact
rkZAtAaxM_6,Cons : The reasonableness of the assumptions used in the analysis needs a more careful analysis .,request
rkZAtAaxM_7,"In particular , the assumption that all weights are independent is valid only at the first random iteration .",fact
rkZAtAaxM_8,"Therefore , the utility of this theory during initialization seems reasonable ,",evaluation
rkZAtAaxM_9,but during learning the theory seems quite tenuous .,evaluation
rkZAtAaxM_10,"I would encourage the authors to discuss their assumptions , and talk about how the math would change as a result of relaxing the assumptions .",request
rkZAtAaxM_11,The empirical support does provide evidence that the theory is reasonable .,fact
rkZAtAaxM_12,"However , it is limited to a single dataset .",fact
rkZAtAaxM_13,It would be nice to see that the effect happens more generally .,evaluation
rkZAtAaxM_14,"Second , it is clear that shallow + wide networks may be better than deep + narrow networks ,",evaluation
rkZAtAaxM_15,but it 's not clear about how the width is evaluated and supported .,evaluation
rkZAtAaxM_16,I would encourage the authors to do more extensive experiments and evaluate the architecture further .,request
By2zFR_gz_0,Quality : Although the research problem is an interesting direction,evaluation
By2zFR_gz_1,the quality of the work is not of a high standard .,evaluation
By2zFR_gz_2,My main conservation is that the idea of perturbation in semantic latent space has not been described in an explicit way .,evaluation
By2zFR_gz_3,How different it will be compared to a perturbation in an input space ?,non-arg
By2zFR_gz_4,"Clarity : The use of the term "" adversarial "" is not quite clear in the context",evaluation
By2zFR_gz_5,"as in many of those example classification problems the perturbation completely changes the class label ( e.g. from "" church "" to "" tower "" or vice-versa )",fact
By2zFR_gz_6,Originality : The generation of adversarial examples in black-box classifiers has been looked in GAN literature as well and gradient based perturbations are studied too .,fact
By2zFR_gz_7,What is the main benefit of the proposed mechanism compared to the existing ones ?,non-arg
By2zFR_gz_8,Significance : The research problem is indeed a significant one,evaluation
By2zFR_gz_9,as it is very important to understand the robustness of the modern machine learning methods by exposing them to adversarial scenarios where they might fail .,evaluation
By2zFR_gz_10,pros : ( a ) An interesting problem to evaluate the robustness of black-box classifier systems,evaluation
By2zFR_gz_11,( b ) generating adversarial examples for image classification as well as text analysis .,fact
By2zFR_gz_12,( c ) exploiting the recent developments in GAN literature to build the framework forge generating adversarial examples .,fact
By2zFR_gz_13,cons : ( a ) The proposed search algorithm in the semantic latent space could be computationally intensive .,fact
By2zFR_gz_14,any remedy for this problem ?,non-arg
By2zFR_gz_15,( b ) Searching in the latent space z could be strongly dependent on the matching inverter <VAR> .,fact
By2zFR_gz_16,any comment on this ?,non-arg
By2zFR_gz_17,( c ) The application of the search algorithm in case of imbalanced classes could be something that require further investigation .,request
H1IrTpFxz_0,The paper addresses the problem of learning the form of the activation functions in neural networks .,fact
H1IrTpFxz_1,The authors propose to place Gaussian process ( GP ) priors on the functional form of each activation function ( each associated with a hidden layer and unit ) in the neural net .,fact
H1IrTpFxz_2,"This somehow allows to non-parametrically infer from the data the "" shape "" of the activation functions needed for a specific problem .",fact
H1IrTpFxz_3,The paper then proposes an inference framework ( to approximately marginalize out all GP functions ) based on sparse GP methods that use inducing points and variational inference .,fact
H1IrTpFxz_4,The inducing point approximation used here is very efficient since all GP functions depend on a scalar input ( as any activation function ! ),fact
H1IrTpFxz_5,and therefore by just placing the inducing points in a dense grid gives a fast and accurate representation/compression of all GPs in terms of the inducing function values ( denoted by U in the paper ) .,fact
H1IrTpFxz_6,Of course then inference involves approximating the finite posterior over inducing function values U,fact
H1IrTpFxz_7,and the paper make use of the standard Gaussian approximations .,fact
H1IrTpFxz_8,In general I like the idea,evaluation
H1IrTpFxz_9,and I believe that it can lead to a very useful model .,evaluation
H1IrTpFxz_10,"However , I have found the current paper quite preliminary and incomplete .",evaluation
H1IrTpFxz_11,The authors need to address the following :,request
H1IrTpFxz_12,First ( very important ) : You need to show experimentally how your method compares against regular neural nets ( with specific fixed forms for their activation functions such relus etc ) .,request
H1IrTpFxz_13,At the moment in the last section you mention,fact
H1IrTpFxz_14," We have validated networks of Gaussian Process Neurons in a set of experiments , the details of which we submit in a subsequent publication . In those experiments , our model shows to be significantly less prone to overfitting than a traditional feed-forward network of same size , despite having more parameters . ",quote
H1IrTpFxz_15,=== > Well all this needs to be included in the same paper .,request
H1IrTpFxz_16,Secondly : Discuss the connection with Deep GPs ( Damianou and Lawrence 2013 ) .,request
H1IrTpFxz_17,Your method seems to be connected with Deep GPs,evaluation
H1IrTpFxz_18,although there appear to be important differences as well .,evaluation
H1IrTpFxz_19,E.g. you place GPs on the scalar activation functions in an otherwise heavily parametrized neural network ( having interconnection weights between layers ) while deep GPs model the full hidden layer mapping as a single GP ( which does not require interconnection weights ) .,fact
H1IrTpFxz_20,Thirdly : You need to better explain the propagation of uncertainly in section 3.2.2 and the central limit of distribution in section 3.2.1 .,request
H1IrTpFxz_21,This is the technical part of your paper which is a non-standard approximation .,evaluation
H1IrTpFxz_22,I will suggest to give a better intuition of the whole idea and move a lot of mathematical details to the appendix .,request
HJ9LXfvlz_0,Paper studies an interesting phenomenon of overparameterised models being able to learn well-generalising solutions .,fact
HJ9LXfvlz_1,It focuses on a setting with three crucial simplifications :,fact
HJ9LXfvlz_2,- data is linearly separable,fact
HJ9LXfvlz_3,- model is 1-hidden layer feed forward network with homogenous activations,fact
HJ9LXfvlz_4,"- ** only input-hidden layer weights ** are trained , while the hidden-output layer 's weights are fixed to be <VAR> ( in particular -- <VAR> )",fact
HJ9LXfvlz_5,"While the last assumption does not limit the expressiveness of the model in any way ,",fact
HJ9LXfvlz_6,as homogenous activations have the property of <EQN> ( for positive a ),fact
HJ9LXfvlz_7,"and so for any unconstrained model in the second layer , we can "" propagate "" its weights back into first layer and obtain functionally equivalent network .",fact
HJ9LXfvlz_8,"However , learning dynamics of a model of form <EQN> and "" standard "" neural model <EQN> can be completely different .",evaluation
HJ9LXfvlz_9,"Consequently , while the results are very interesting , claiming their applicability to the deep models is ( at this point ) far fetched .",evaluation
HJ9LXfvlz_10,"In particular , abstract suggests no simplifications are being made , which does not correspond to actual result in the paper .",fact
HJ9LXfvlz_11,"The results themselves are interesting ,",evaluation
HJ9LXfvlz_12,"but due to the above restriction it is not clear whether it sheds any light on neural nets , or simply described a behaviour of very specific , non-standard shallow model .",evaluation
HJ9LXfvlz_13,"I am happy to revisit my current rating given authors rephrase the paper so that the simplifications being made are clear both in abstract and in the text , and that ( at least empirically ) it does not affect learning in practice .",evaluation
HJ9LXfvlz_14,"In other words - all the experiments in the paper follow the assumption made , if authors claim is that the restriction introduced does not matter , but make proofs too technical - at least experimental section should show this .",request
HJ9LXfvlz_15,"If the claims do not hold empirically without the assumptions made , then the assumptions are not realistic and can not be used for explaining the behaviour of models we are interested in .",evaluation
HJ9LXfvlz_16,"Pros : - tackling a hard problem of overparametrised models , without introducing common unrealistic assumptions of activations independence",evaluation
HJ9LXfvlz_17,"- very nice result of "" phase change "" dependend on the size of hidden layer in section 7",evaluation
HJ9LXfvlz_18,Cons : - simplification with non-trainable second layer is currently not well studied in the paper ;,evaluation
HJ9LXfvlz_19,and while not affecting expressive power - it is something that can change learning dynamics completely,evaluation
rkZd9y9xz_0,"The paper proposes a novel way of compressing gradient updates for distributed SGD , in order to speed up overall execution .",fact
rkZd9y9xz_1,"While the technique is novel as far as I know ( eq . ( 1 ) in particular ) ,",evaluation
rkZd9y9xz_2,many details in the paper are poorly explained ( I am unable to understand ),evaluation
rkZd9y9xz_3,and experimental results do not demonstrate that the problem targeted is actually alleviated .,fact
rkZd9y9xz_4,"More detailed remarks : 1 : Motivating with ImageNet taking over a week to train seems misplaced when we have papers claiming to train ImageNet in 1 hour , 24 mins , 15 mins ...",evaluation
rkZd9y9xz_5,"4.1 : Lemma 4.1 seems like you want B > 1 , or clarify definition of V_B .",evaluation
rkZd9y9xz_6,4.2 : This section is not fully comprehensible to me .,evaluation
rkZd9y9xz_7,- It seems you are confusingly overloading the term gradient and words derived ( also in other parts or the paper ) .,evaluation
rkZd9y9xz_8,"What is "" maximum value of gradients in a matrix "" ?",request
rkZd9y9xz_9,"Make sure to use something else , when talking about individual elements of a vector ( which is constructed as an average of gradients ) , etc .",request
rkZd9y9xz_10,- Rounding : do you use deterministic or random rounding ?,request
rkZd9y9xz_11,Do you then again store the inaccuracy ?,request
rkZd9y9xz_12,- I do n't understand definition of d.,evaluation
rkZd9y9xz_13,It seems you subtract logarithm of a gradient from a scalar .,fact
rkZd9y9xz_14,"- In total , I really do n't know what is the object that actually gets communicated ,",evaluation
rkZd9y9xz_15,"and consequently when you remark that this can be combined with QSGD and the more below it , I do n't understand it .",evaluation
rkZd9y9xz_16,"This section has to be thoroughly explained , perhaps with some illustrative examples .",request
rkZd9y9xz_17,4.3 : allgatherv remark : does that mean that this approach would not scale well to higher number of workers ?,non-arg
rkZd9y9xz_18,"4.4 : Remarks about quantization and mantissa manipulation are not clear to me again , or what is the point in doing so .",evaluation
rkZd9y9xz_19,Possible because the problems above .,evaluation
rkZd9y9xz_20,5 : I think this section is not too useful unless you can accompany it with actual efficient implementation and contrast the practical performance .,evaluation
rkZd9y9xz_21,"6 : Given that I do n't understand how you compress the information being communicated , it is hard to believe the utility of the method .",evaluation
rkZd9y9xz_22,The objective was to speed up training time because communication is bottleneck .,fact
rkZd9y9xz_23,"If you provide 12,000 x compression , is it any more practically useful than providing 120x compression ?",request
rkZd9y9xz_24,What would be the difference in runtime ?,request
rkZd9y9xz_25,Such questions are never discussed .,fact
rkZd9y9xz_26,"Further , if in the implementation you discuss masking mantissa ,",fact
rkZd9y9xz_27,"I have serious concern about whether the compression protocol is feasible to implement efficiently , without writing some extremely low-level code .",evaluation
rkZd9y9xz_28,I think the soundness of work addressing this particular problem is damaged if not implemented properly ( compared to other kinds of works in current ML related research ) .,evaluation
rkZd9y9xz_29,Therefore I highly recommend including proper time comparison with a baseline in the future .,request
rkZd9y9xz_30,"Further , I do n't understand 2 things about the Tables .",evaluation
rkZd9y9xz_31,a ) how do you combine the proposed method with Momentum in SGD ?,request
rkZd9y9xz_32,This is not discussed as far as I can see .,fact
rkZd9y9xz_33,"b ) What is "" QSGD , 2bit """,request
rkZd9y9xz_34,"If I remember QSGD protocol correctly , there 's no natural mapping of 2bit to its parameters .",evaluation
ByPQQOX1G_0,"Summary ======== The authors present a new regularization term , inspired from game theory , which encourages the discriminator 's gradient to have a norm equal to one .",fact
ByPQQOX1G_1,"This leads to reduce the number of local minima ,",fact
ByPQQOX1G_2,so that the behavior of the optimization scheme gets closer to the optimization of a zero-sum games with convex-concave functions .,fact
ByPQQOX1G_3,"Clarity ====== Overall , the paper is clear and well-written .",evaluation
ByPQQOX1G_4,"However , the authors should motivate better the regularization introduced in section 2.3 .",request
ByPQQOX1G_5,Originality ========= The idea is novel and interesting .,evaluation
ByPQQOX1G_6,"In addition , it is easy to implement it for any GANs since it requires only an additional regularization term .",evaluation
ByPQQOX1G_7,"Moreover , the numerical experiments are in favor of the proposed method .",fact
ByPQQOX1G_8,Comments ========= - Why should the norm of the gradient should to be equal to 1 and not another value ?,request
ByPQQOX1G_9,Is this possible to improve the performance if we put an additional hyper-parameter instead ?,request
ByPQQOX1G_10,- Are the performances greatly impacted by other value of lambda and c ( the suggested parameter values are lambda = c = 10 ) ?,request
ByPQQOX1G_11,"- As mentioned in the paper , the regularization affects the modeling performance .",fact
ByPQQOX1G_12,Maybe the authors should add a comparison between different regularization parameters to illustrate the real impact of lambda and c on the performance .,request
ByPQQOX1G_13,- GANs performance is usually worse on very big dataset such as Imagenet .,fact
ByPQQOX1G_14,Does this regularization trick makes their performance better ?,request
HyxmggJbM_0,This paper proposes a new way of sampling data for updates in deep-Q networks .,fact
HyxmggJbM_1,The basic principle is to update Q values starting from the end of the episode in order to facility quick propagation of rewards back along the episode .,fact
HyxmggJbM_2,"The paper is interesting ,",evaluation
HyxmggJbM_3,but it lacks the proper comparisons to previously published techniques .,evaluation
HyxmggJbM_4,The results presented by this paper shows improvement over the baseline .,fact
HyxmggJbM_5,But the Atari results is still significantly worse than the current SOTA .,fact
HyxmggJbM_6,"In the non-tabular case , the authors have actually moved away from Q learning and defined an objective that is both on and off-policy .",fact
HyxmggJbM_7,Some ( theoretical ) analysis would be nice .,request
HyxmggJbM_8,It is hard to judge whether the objective defined in the non-tabular defines a contraction operator at all in the tabular case .,evaluation
HyxmggJbM_9,There has been a number of highly relevant papers .,evaluation
HyxmggJbM_10,"Prioritized replay , for example , could have a very similar effect to proposed approach in the tabular case .",evaluation
HyxmggJbM_11,"In the non-tabular case , the Retrace algorithm , tree backup , Watkin 's Q learning all bear significant resemblance to the proposed method .",evaluation
HyxmggJbM_12,"Although the proposed algorithm is different from all 3 ,",evaluation
HyxmggJbM_13,the authors should still have compared to at least one of them as a baseline .,request
HyxmggJbM_14,"The Retrace algorithm specifically has also been shown to help significantly in the Atari case ,",fact
HyxmggJbM_15,and it defines a convergent update rule .,fact
HkCNqISxM_0,The authors try to use continuous time generalizations of normalizing flows for improving upon VAE-like models or for standard density estimation problems .,fact
HkCNqISxM_1,Clarity : the text is mathematically very sloppy / hand-wavy .,evaluation
HkCNqISxM_2,1 . I do not understand proposition ( 1 ) .,evaluation
HkCNqISxM_3,I do not think that the proof is correct,fact
HkCNqISxM_4,( e.g. the generator L needs to be applied to a function,fact
HkCNqISxM_5,-- the notation L ( x ) does not make too much sense ) :,evaluation
HkCNqISxM_6,"indeed , in the case when the volatility is zero ( or very small ) , this proposition would imply that any vector field induces a volume preserving transformation , which is indeed false .",fact
HkCNqISxM_7,2 . I do not really see how the sequence of minimization Eq ( 5 ) helps in practice .,evaluation
HkCNqISxM_8,The Wasserstein term is difficult to hand .,evaluation
HkCNqISxM_9,"3 . in Equation ( 6 ) , I do not really understand what <VAR> is if <VAR> is an empirical distribution .",evaluation
HkCNqISxM_10,One really needs <VAR> to be a probability density to make sense of that .,evaluation
ByIyxIKef_0,"In the Following , pros and cons of the paper are presented .",non-arg
ByIyxIKef_1,Pros ------- 1 . Many real-world applications .,fact
ByIyxIKef_2,2 . Simple architecture and can be reproduced ( if given enough details . ),fact
ByIyxIKef_3,Cons ---------------------- 1 . Ablation study showing whether bidirectional LSTM contributing to the similarity would be helpful .,fact
ByIyxIKef_4,2 . Baseline is not strong .,evaluation
ByIyxIKef_5,How about using just LSTM ?,request
ByIyxIKef_6,4 . It is suprising to see that only concatenation with MLP is used for optimization of capturing regularities across languages .,evaluation
ByIyxIKef_7,5 . Equation-11 looks like softplus function more than vanilla ReLU .,evaluation
ByIyxIKef_8,6 . How are the similarity assessments made in the gold standard dataset .,request
ByIyxIKef_9,The cost function used only suggest binary assessments .,fact
ByIyxIKef_10,Please refer to some SemEval tasks for cross-lingual or cross-level assessments .,request
ByIyxIKef_11,As binary assessments may not be a right measure to compare articles of two different lengths or languages .,fact
ByIyxIKef_12,Minor issues ------------ 1 . SNS is meant to be social networking sites ?,request
ByIyxIKef_13,"2 . In Section 2.2 , it denote that ' as the figure demonstrates ' .",fact
ByIyxIKef_14,No reference to the figure .,fact
ByIyxIKef_15,"3 . In Section 3 , ' discussed in detail ' pointed to Section 2.1 related work section .",fact
ByIyxIKef_16,Not clear what is discussed in detail there .,evaluation
ByIyxIKef_17,4 . Reference to Google Translate API is wrong .,fact
ByIyxIKef_18,The paper requires more experimental analysis to judge the significance of the approach presented .,request
Hyd9YyOlf_0,The paper studies the problem of DNN loss function design for reducing intra-class variance in the output feature space .,fact
Hyd9YyOlf_1,The key contribution is proposing an isotropic variant of the softmax loss that can balance the accuracy of classification and compactness of individual class .,fact
Hyd9YyOlf_2,The proposed loss has been compared extensively against a number of closely related approaches in methodology .,fact
Hyd9YyOlf_3,"Numerical results on benchmark datasets show some improvement of the proposed loss over softmax loss and center loss ( Wen et al. , 2016 ) , when applied to distance-based classifiers such as k-NN and k-means .",fact
Hyd9YyOlf_4,Pros : - The idea of isotropic normalization for enhancing compactness of class is well motivated,evaluation
Hyd9YyOlf_5,- The paper is mostly clearly organized and presented .,evaluation
Hyd9YyOlf_6,- Numerical study shows some promise of the proposed method .,evaluation
Hyd9YyOlf_7,"Cons : - The novelty of method is mostly incremental given the prior work of ( Wen et al. , 2016 ) which has provided a slightly different isotropic variant of softmax loss .",evaluation
Hyd9YyOlf_8,- The training procedure of the proposed method remains unclear in this paper .,evaluation
Sy4HaTtlz_0,Quality : The work focuses on a novel problem of generating text sample using GAN and a novel in-filling mechanism of words .,fact
Sy4HaTtlz_1,Using GAN to generate samples in adversarial setup in texts has been limited due to the mode collapse and training instability issues .,fact
Sy4HaTtlz_2,As a remedy to these problems an in-filling-task conditioning on the surrounding text has been proposed .,fact
Sy4HaTtlz_3,"But , the use of the rewards at every time step ( RL mechanism ) to employ the actor-critic training procedure could be challenging computationally challenging .",evaluation
Sy4HaTtlz_4,Clarity : The mechanism of generating the text samples using the proposed methodology has been described clearly .,evaluation
Sy4HaTtlz_5,However the description of the reinforcement learning step could have been made a bit more clear .,request
Sy4HaTtlz_6,Originality : The work indeed use a novel mechanism of in-filling via a conditioning approach to overcome the difficulties of GAN training in text settings .,evaluation
Sy4HaTtlz_7,There has been some work using GAN to generate adversarial examples in textual context too to check the robustness of classifiers .,fact
Sy4HaTtlz_8,How this current work compares with the existing such literature ?,request
Sy4HaTtlz_9,Significance : The research problem is indeed significant,evaluation
Sy4HaTtlz_10,since the use of GAN in generating adversarial examples in image analysis has been more prevalent compared to text settings .,evaluation
Sy4HaTtlz_11,"Also , the proposed actor-critic training procedure via RL methodology is indeed significant from its application in natural language processing .",evaluation
Sy4HaTtlz_12,pros : ( a ) Human evaluations applications to several datasets show the usefulness of MaskGen over the maximum likelihood trained model in generating more realistic text samples .,fact
Sy4HaTtlz_13,( b ) Using a novel in-filling procedure to overcome the complexities in GAN training .,evaluation
Sy4HaTtlz_14,( c ) generation of high quality samples even with higher perplexity on ground truth set .,evaluation
Sy4HaTtlz_15,cons : ( a ) Use of rewards at every time step to the actor-critic training procure could be computationally expensive .,evaluation
Sy4HaTtlz_16,( b ) How to overcome the situation where in-filling might introduce implausible text sequences with respect to the surrounding words ?,request
Sy4HaTtlz_17,( c ) Depending on the Mask quality GAN can produce low quality samples .,fact
Sy4HaTtlz_18,Any practical way of choosing the mask ?,request
Hy2FsQKef_0,This paper addresses the problem of one class classification .,fact
Hy2FsQKef_1,The authors suggest a few techniques to learn how to classify samples as negative ( out of class ) based on tweaking the GAN learning process to explore large areas of the input space which are out of the objective class .,fact
Hy2FsQKef_2,The suggested techniques are nice and show promising results .,evaluation
Hy2FsQKef_3,"But I feel a lot can still be done to justify them , even just one of them .",evaluation
Hy2FsQKef_4,"For instance , the authors manipulate the objective of G using a new parameter alpha_new and divide heuristically the range of its values .",fact
Hy2FsQKef_5,"But , in the experimental section results are shown only for a single value , alpha_new = 0.9",fact
Hy2FsQKef_6,The authors also suggest early stopping,fact
Hy2FsQKef_7,but again ( as far as I understand ) only a single value for the number of iterations was tested .,fact
Hy2FsQKef_8,"The writing of the paper is also very unclear , with several repetitions and many typos e.g. :",evaluation
Hy2FsQKef_9,' we first introduce you a ',quote
Hy2FsQKef_10,' architexture ',quote
Hy2FsQKef_11,' future work remain to ',quote
Hy2FsQKef_12,' it self ',quote
Hy2FsQKef_13,I believe there is a lot of potential in the approach ( es ) presented in the paper .,evaluation
Hy2FsQKef_14,In my view a much stronger experimental section together with a clearer presentation and discussion could overcome the lack of theoretical discussion .,request
ByjrTO5ef_0,Recently some interesting work on a role of prior in deep generative models has been presented .,evaluation
ByjrTO5ef_1,The choice of prior may have an impact on the expressiveness of the model,fact
ByjrTO5ef_2,"[ Hoffman and Johnson , 2016 ] .",reference
ByjrTO5ef_3,A few existing work presents methods for learning priors from data for variational autoencoders,fact
ByjrTO5ef_4,"[ Goyal et al. , 2017 ]",reference
ByjrTO5ef_5,"[ Tomczak and Welling , 2017 ] .",reference
ByjrTO5ef_6,"The work , "" VAE with a VampPrior , "" [ Tomczak and Welling , 2017 ] is missing in references .",fact
ByjrTO5ef_7,The current work focuses on adversarial autoencoder ( AAE ) and introduces a code generator network to transform a simple prior into one that together with the generator can better fit the data distribution .,fact
ByjrTO5ef_8,"Adversarial loss is used to train the code generator network , allowing the output of the network could be any distribution .",fact
ByjrTO5ef_9,I think the method is quite simple but interesting approach to improve AAEs without hurting the reconstruction .,evaluation
ByjrTO5ef_10,The paper is well written and is easy to read .,evaluation
ByjrTO5ef_11,The method is well described .,evaluation
ByjrTO5ef_12,"However , what is missing in this paper is an analysis of learned priors ,",request
ByjrTO5ef_13,which help us to better understand its behavior .,evaluation
ByjrTO5ef_14,The model is evaluated qualitatively only .,fact
ByjrTO5ef_15,What about quantitative evaluation ?,request
ByRWmWAxM_0,This paper proposes an extremely simple methodology to improve the network 's performance by adding extra random perturbations ( resizing/padding ) at evaluation time .,evaluation
ByRWmWAxM_1,"Although the paper is very basic ,",evaluation
ByRWmWAxM_2,it creates a good baseline for defending about various types of attacks and got good results in kaggle competition .,evaluation
ByRWmWAxM_3,The main merit of the paper is to study this simple but efficient baseline method extensively and shows how adversarial attacks can be mitigated by some extent .,evaluation
ByRWmWAxM_4,Cons of the paper : there is not much novel insight or really exciting new ideas presented .,evaluation
ByRWmWAxM_5,Pros : It gives a convincing very simple baseline,evaluation
ByRWmWAxM_6,"and the evaluation of all subsequent results on defending against adversaries will need to incorporate this simple defense method in addition to any future proposed defenses ,",fact
ByRWmWAxM_7,since it is very easy to implement and evaluate and seems to improve the defense capabilities of the network to a significant degree .,evaluation
ByRWmWAxM_8,So I assume that this paper will be influential in the future just by the virtue of its easy applicability and effectiveness .,evaluation
SJ2P_-YgG_0,The main idea of this paper is to replace the feedforward summation,evaluation
SJ2P_-YgG_1,"<EQN> where x , y , b are vectors , W is a matrix by an integral <EQN> where x , y , b are functions , and W is a kernel .",fact
SJ2P_-YgG_2,A deep neural network with this integral feedforward is called a deep function machine .,fact
SJ2P_-YgG_3,The motivation is along the lines of functional PCA :,fact
SJ2P_-YgG_4,"if the vector x was obtained by discretization of some function x , then one encounters the curse of dimensionality as one obtains finer and finer discretization .",evaluation
SJ2P_-YgG_5,"The idea of functional PCA is to view x as a function is some appropriate Hilbert space , and expands it in some appropriate basis .",fact
SJ2P_-YgG_6,"This way , finer discretization does not increase the dimension of x ( nor its approximation ) , but rather improves the resolution .",fact
SJ2P_-YgG_7,This paper takes this idea and applies it to deep neural networks .,fact
SJ2P_-YgG_8,"Unfortunately , beyond rather obvious approximation results , the paper does not get major mileage out of this idea .",evaluation
SJ2P_-YgG_9,This approach amounts to a change of basis -,fact
SJ2P_-YgG_10,and therefore the resolution invariance is not surprising .,evaluation
SJ2P_-YgG_11,"In the experiments , results of this method should be compared not against NNs trained on the data directly , but against NNs trained on dimension reduced version of the data ( eg : first fixed number of PCA components ) .",request
SJ2P_-YgG_12,"Unfortunately , this was not done .",evaluation
SJ2P_-YgG_13,"I suspect that in this case , the results would be very similar .",evaluation
H1WORsdlG_0,This paper addresses the important problem of understanding mathematically how GANs work .,fact
H1WORsdlG_1,The approach taken here is to look at GAN through the lense of the scattering transform .,fact
H1WORsdlG_2,Unfortunately the manuscrit submitted is very poorly written .,evaluation
H1WORsdlG_3,Introduction and flow of thoughts is really hard to follow .,evaluation
H1WORsdlG_4,"In method sections , the text jumps from one concept to the next without proper definitions .",evaluation
H1WORsdlG_5,Sorry I stopped reading on page 3 .,evaluation
H1WORsdlG_6,I suggest to rewrite this work before sending it to review .,request
H1WORsdlG_7,Among many things : - For citations use citep and not citet to have ( ) at the right places .,request
H1WORsdlG_8,- Why does it seems - > Why does it seem etc .,request
BJ1X3tYgf_0,The paper treats the interesting problem of long term video prediction in complex video streams .,fact
BJ1X3tYgf_1,I think the approach of adding more structure to their representation before making longer term prediction is also a reasonable one .,evaluation
BJ1X3tYgf_2,Their approach combines an RNN that predicts an encoding of scene and then generating an image prediction using a VAN ( Reed et al. ) .,fact
BJ1X3tYgf_3,They show some results on the Human3 .6 M and the Robot Push dataset .,fact
BJ1X3tYgf_4,I find the submission lacking clarity in many places .,evaluation
BJ1X3tYgf_5,The main lack of clarity source I think is about what the contribution is .,evaluation
BJ1X3tYgf_6,There are sparse mentions in the introduction,fact
BJ1X3tYgf_7,but I think it would be much more forceful and clear if they would present VAN or Villegas et al method separately and then put the pieces together for their method in a separate section .,request
BJ1X3tYgf_8,This would allow the author to clearly delineate their contribution and maybe why those choices were made .,evaluation
BJ1X3tYgf_9,"Also the use of hierarchical is non-standard and leads to confusion I recommend maybe "" semantical "" or better "" latent structured "" instead .",request
BJ1X3tYgf_10,Smaller ambiguities in wording are also in the paper :,evaluation
BJ1X3tYgf_11,"e.g. related work - > long term prediction "" in this work "" refers to the work mentioned but could as well be the work that they are presenting .",evaluation
BJ1X3tYgf_12,I find some of the claims not clearly backed by a thorough evaluation and analysis .,evaluation
BJ1X3tYgf_13,Claiming to be able to produce encodings of scenes that work well at predicting many steps into the future is a very strong claim .,evaluation
BJ1X3tYgf_14,I find the few images provided very little evidence for that fact .,evaluation
BJ1X3tYgf_15,I think a toy example where this is clearly the case,evaluation
BJ1X3tYgf_16,"because we know exactly the factors of variations and they are inferred by the algorithm automatically or some better ones are discovered by the algorithm ,",evaluation
BJ1X3tYgf_17,that would make it a very strong submission .,evaluation
BJ1X3tYgf_18,"Reed et al. have a few examples that could be adapted to this setting and the resulting representation , analyzed appropriately , would shed some light into whether this is the right approach for long term video prediction and what are the nobs that should be tweaked in this system .",evaluation
BJ1X3tYgf_19,"In the current format , I think that the authors are on a good path",evaluation
BJ1X3tYgf_20,"and I hope my suggestions will help them improve their submission ,",evaluation
BJ1X3tYgf_21,but as it stands I recommend rejection from this conference .,evaluation
BJggQbceG_0,Summary : This paper proposes an adversarial learning framework for machine comprehension task .,fact
BJggQbceG_1,"Specifically , authors consider a reader network which learns to answer the question by reading the passage and a narrator network which learns to obfuscate the passage so that the reader can fail in its task .",fact
BJggQbceG_2,Authors report results in 3 different reading comprehension datasets,fact
BJggQbceG_3,and the proposed learning framework results in improving the performance of GMemN2N .,fact
BJggQbceG_4,My Comments : This paper is a direct application of adversarial learning to the task of reading comprehension .,fact
BJggQbceG_5,It is a reasonable idea,evaluation
BJggQbceG_6,and authors indeed show that it works .,fact
BJggQbceG_7,1 . The paper needs a lot of editing .,evaluation
BJggQbceG_8,Please check the minor comments .,request
BJggQbceG_9,2 . Why is the adversary called narrator network ?,request
BJggQbceG_10,It is bit confusing,evaluation
BJggQbceG_11,because the job of that network is to obfuscate the passage .,fact
BJggQbceG_12,3 . Why do you motivate the learning method using self-play ?,request
BJggQbceG_13,This is just using the idea of adversarial learning ( like GAN ) and it is not related to self-play .,fact
BJggQbceG_14,"4 . In section 2 , first paragraph , authors mention that the narrator prevents catastrophic forgetting .",fact
BJggQbceG_15,How is this happening ?,request
BJggQbceG_16,Can you elaborate more ?,request
BJggQbceG_17,5 . The learning framework is not explained in a precise way .,evaluation
BJggQbceG_18,What do you mean by re-initializing and retraining the narrator ?,request
BJggQbceG_19,Is n’t it costly to reinitialize the network and retrain it for every turn ?,evaluation
BJggQbceG_20,How many such epochs are done ?,request
BJggQbceG_21,You say that test set also contains obfuscated documents .,fact
BJggQbceG_22,Is it only for the validation set ?,request
BJggQbceG_23,Can you please explain if you use obfuscation when you report the final test performance too ?,request
BJggQbceG_24,It would be more clear if you can provide a complete pseudo-code of the learning procedure .,request
BJggQbceG_25,6 . How does the narrator choose which word to obfuscate ?,request
BJggQbceG_26,Do you run the narrator model with all possible obfuscations and pick the best choice ?,request
BJggQbceG_27,7 . Why do n’t you treat number of hops as a hyper-parameter and choose it based on validation set ?,request
BJggQbceG_28,I would like to see the results in Table 1 where you choose number of hops for each of the three models based on validation set .,request
BJggQbceG_29,"8 . In figure 2 , how are rounds constructed ?",request
BJggQbceG_30,Does the model sees the same document again and again for 100 times or each time it sees a random document and you sample documents with replacement ?,request
BJggQbceG_31,This will be clear if you provide the pseudo-code for learning .,request
BJggQbceG_32,9 . I do not understand author 's ’ justification for figure-3 .,evaluation
BJggQbceG_33,Is it the case that the model learns to attend to last sentences for all the questions ?,request
BJggQbceG_34,Or where it attends varies across examples ?,request
BJggQbceG_35,10 . Are you willing to release the code for reproducing the results ?,non-arg
BJggQbceG_36,"Minor comments : Page 1 , “ exploit his own decision ” should be “ exploit its own decision ”",request
BJggQbceG_37,"In page 2 , section 2.1 , sentence starting with “ Indeed , a too low percentage … ” needs to be fixed .",request
BJggQbceG_38,"Page 3 , “ forgetting is compensate ” should be “ forgetting is compensated ” .",request
BJggQbceG_39,"Page 4 , “ for one sentences ” needs to be fixed .",request
BJggQbceG_40,"Page 4 , “ unknow ” should be “ unknown ” .",request
BJggQbceG_41,"Page 4 , “ ?? ” needs to be fixed .",request
BJggQbceG_42,"Page 5 , “ for the two first datasets ” needs to be fixed .",request
BJggQbceG_43,"Table 1 , “ GMenN2N ” should be “ GMemN2N ” .",request
BJggQbceG_44,"In caption , is it mean accuracy or maximum accuracy ?",request
BJggQbceG_45,"Page 6 , “ dataset was achieves ” needs to be fixed .",request
BJggQbceG_46,"Page 7 , “ document by obfuscated this word ” needs to be fixed .",request
BJggQbceG_47,"Page 7 , “ overall aspect of the two first readers ” needs to be fixed .",request
BJggQbceG_48,"Page 8 , last para , references needs to be fixed .",request
BJggQbceG_49,"Page 9 , first sentence , please check grammar .",request
BJggQbceG_50,"Section 6.2 , last sentence is irrelevant .",evaluation
rJGK3urgz_0,"In this paper , the authors trains a large number of MNIST classifier networks with differing attributes ( batch-size , activation function , no . layers etc. )",fact
rJGK3urgz_1,and then utilises the inputs and outputs of these networks to predict said attributes successfully .,fact
rJGK3urgz_2,They then show that they are able to use the methods developed to predict the family of Imagenet-trained networks and use this information to improve adversarial attack .,fact
rJGK3urgz_3,I enjoyed reading this paper .,evaluation
rJGK3urgz_4,"It is a very interesting set up , and a novel idea .",evaluation
rJGK3urgz_5,"A few comments : The paper is easy to read , and largely written well .",evaluation
rJGK3urgz_6,The article is missing from the nouns quite often though,evaluation
rJGK3urgz_7,so this is something that should be amended .,request
rJGK3urgz_8,There are a few spelling slip ups,fact
rJGK3urgz_9,"( "" to a certain extend "" -- > "" to a certain extent "" ,",request
rJGK3urgz_10," as will see  -- > "" as we will see "" )",request
rJGK3urgz_11,"It appears that the output for kennen-o is a discrete probability vector for each attribute , where each entry corresponds to a possibility",fact
rJGK3urgz_12,"( for example , for "" batch-size "" it is a length 3 vector where the first entry corresponds to 64 , the second 128 , and the third 256 ) .",fact
rJGK3urgz_13,"What happens if you instead treat it as a regression task , would it then be able to hint at intermediates ( a batch size of 96 ) or extremes ( say , 512 ) .",request
rJGK3urgz_14,"A flaw of this paper is that kennen-i and io appear to require gradients from the network being probed ( you do mention this in passing ) , which realistically you would never have access to .",fact
rJGK3urgz_15,( Please do correct me if I have misunderstood this ),non-arg
rJGK3urgz_16,It would be helpful if Section 4 had a paragraph as to your thoughts regarding why certain attributes are easier/harder to predict .,request
rJGK3urgz_17,"Also , the caption for Table 2 could contain more information regarding the network outputs .",request
rJGK3urgz_18,You have jumped from predicting 12 attributes on MNIST to 1 attribute on Imagenet .,fact
rJGK3urgz_19,It could be beneficial to do an intermediate experiment ( a handful of attributes on a middling task ) .,request
rJGK3urgz_20,I think this paper should be accepted,evaluation
rJGK3urgz_21,as it is interesting and novel .,evaluation
rJGK3urgz_22,Pros ------- Interesting idea,evaluation
rJGK3urgz_23,- Reads well,evaluation
rJGK3urgz_24,- Fairly good experimental results,evaluation
rJGK3urgz_25,Cons ------- kennen-i seems like it could n't be realistically deployed,evaluation
rJGK3urgz_26,- lack of an intermediate difficulty task,fact
rJAxUSLSM_0,"The paper consider a method for "" weight normalization "" of layers of a neural network .",fact
rJAxUSLSM_1,"The weight matrix is maintained normalized , which helps accuracy .",fact
rJAxUSLSM_2,"However , the simplest way to normalize on a fully connected layer is quadratic ( adding squares of weights and taking square root ) .",fact
rJAxUSLSM_3,"The paper proposes "" FastNorm "" , which is a way to implicitly maintain the normalized weight matrix using much less computation .",fact
rJAxUSLSM_4,"Essentially , a normalization vector is maintained an updated separately .",fact
rJAxUSLSM_5,Pros : Natural method to do weight normalization efficeintly,evaluation
rJAxUSLSM_6,Cons : A very natural and simple solution that is fairly obvious .,evaluation
rJAxUSLSM_7,Limited experiments,evaluation
rkCp66Tef_0,The paper proposes a deep learning framework called DeePa that supports multiple dimensions of parallelism in computation to accelerate training of convolutional neural networks .,fact
rkCp66Tef_1,"Whereas the majority of work on parallel or distributed deep learning partitions training over bootstrap samples of training data ( called image parallelism in the paper ) ,",fact
rkCp66Tef_2,"DeePa is able to additionally partition the operations over image height , width and channel .",fact
rkCp66Tef_3,This gives more options to parallelize different parts of the neural network .,fact
rkCp66Tef_4,"For example , the best DeePa configurations studied in the paper for AlexNet , VGG-16 , and Inception-v3 typically use image parallelism for the initial layers , reduce GPU utilization for the deeper layers to reduce data transfer overhead , and use model parallelism on a smaller number of GPUs for fully connected layers .",fact
rkCp66Tef_5,The net is that DeePa allows such configurations to be created that provide an increase in training throughput and lower data transfer in practice for training these networks .,fact
rkCp66Tef_6,These configurations for parellism are not easily programmed in other frameworks like TensorFlow and PyTorch .,evaluation
rkCp66Tef_7,The paper can potentially be improved in a few ways .,evaluation
rkCp66Tef_8,One is to explore more demanding training workloads that require larger-scale distribution and parallelism .,request
rkCp66Tef_9,The ImageNet 22-K would be a good example and would really highlight the benefits of the DeePa in practice .,evaluation
rkCp66Tef_10,"Beyond that , more complex workloads like 3D CNNs for video modeling would also provide a strong motivation for having multiple dimensions of the data for partitioning operations .",request
S1QsSa1-M_0,Authors provide an interesting loss function approach for clustering using a deep neural network .,fact
S1QsSa1-M_1,They optimize Kuiper-based nonparametric loss and apply the approach on a large social network data-set .,fact
S1QsSa1-M_2,"However , the details of the deep learning approach are not well described .",evaluation
S1QsSa1-M_3,Some specific comments are given below .,non-arg
S1QsSa1-M_4,1 . Further details on use of 10-fold cross validation need to be discussed including over-fitting aspect .,request
S1QsSa1-M_5,"2 . Details on deep learning , number of hidden layers , number of hidden units , activation functions , weight adjustment details on each learning methods should be included .",request
S1QsSa1-M_6,3 . Conclusion section is very brief,evaluation
S1QsSa1-M_7,and can be expanded by including a discussion on results comparison and over fitting aspects in cross validation .,request
S1QsSa1-M_8,Use of Kuiper-based nonparametric loss should also be justified as there are other loss functions can be used under these settings .,request
BJ9DfkxWM_0,The paper is clear and well written .,evaluation
BJ9DfkxWM_1,It is an incremental modification of prior work ( ResNeXt ) that performs better on several experiments selected by the author ;,evaluation
BJ9DfkxWM_2,comparisons are only included relative to ResNeXt .,fact
BJ9DfkxWM_3,"This paper is not about gating ( c.f. , gates in LSTMs , mixture of experts , etc ) but rather about masking or perhaps a kind of block sparsity ,",fact
BJ9DfkxWM_4,"as the "" gates "" of the paper do not depend upon the input :",fact
BJ9DfkxWM_5,they are just fixed masking matrices ( see eq ( 2 ) ) .,fact
BJ9DfkxWM_6,The main contribution appears to be the optimisation procedure for the binary masking tensor g.,fact
BJ9DfkxWM_7,But this procedure is not justified :,fact
BJ9DfkxWM_8,does each step minimise the loss ?,request
BJ9DfkxWM_9,This seems unlikely due to the sampling .,evaluation
BJ9DfkxWM_10,Can the authors show that the procedure will always converge ?,request
BJ9DfkxWM_11,It would be good to contrast this with other attempts to learn discrete random variables,request
BJ9DfkxWM_12,"( for example , <CIT> ) .",reference
S1SG_l5gz_0,This paper proposes to automatically recognize domain names as malicious or benign by deep networks ( convnets and RNNs ) trained to directly classify the character sequence as such .,fact
S1SG_l5gz_1,"Pros The paper addresses an important application of deep networks , comparing the performance of a variety of different types of model architectures .",evaluation
S1SG_l5gz_2,The tested networks seem to perform reasonably well on the task .,evaluation
S1SG_l5gz_3,Cons There is little novelty in the proposed method/models,evaluation
S1SG_l5gz_4,-- the paper is primarily focused on comparing existing models on a new task .,fact
S1SG_l5gz_5,The descriptions of the different architectures compared are overly verbose,evaluation
S1SG_l5gz_6,-- they are all simple standard convnet / RNN architectures .,evaluation
S1SG_l5gz_7,The code specifying the models is also excessive for the main text,evaluation
S1SG_l5gz_8,-- it should be moved to an appendix or even left for a code release .,request
S1SG_l5gz_9,The comparisons between various architectures are not very enlightening,evaluation
S1SG_l5gz_10,as they are n’t done in a controlled way,fact
S1SG_l5gz_11,-- there are a large number of differences between any pair of models,evaluation
S1SG_l5gz_12,so it ’s hard to tell where the performance differences come from .,evaluation
S1SG_l5gz_13,It ’s also difficult to compare the learning curves among the different models ( Fig 1 ),evaluation
S1SG_l5gz_14,as they are in separate plots with differently scaled axes .,fact
S1SG_l5gz_15,The proposed problem is an explicitly adversarial setting,fact
S1SG_l5gz_16,"and adversarial examples are a well-known issue with deep networks and other models ,",evaluation
S1SG_l5gz_17,but this issue is not addressed or analyzed in the paper .,fact
S1SG_l5gz_18,"( In fact , the intro claims this is an advantage of not using hand-engineered features for malicious domain detection , seemingly ignoring the literature on adversarial examples for deep nets . )",fact
S1SG_l5gz_19,"For example , in this case an attacker could start with a legitimate domain name and use black box adversarial attacks ( or white box attacks , given access to the model weights ) to derive a similar domain name that the models proposed here would classify as benign .",fact
S1SG_l5gz_20,"While this paper addresses an important problem ,",evaluation
S1SG_l5gz_21,in its current form the novelty and analysis are limited,evaluation
S1SG_l5gz_22,and the paper has some presentation issues .,evaluation
Bk8udEEeM_0,Quick summary : This paper proposes an energy based formulation to the BEGAN model and modifies it to include an image quality assessment based term .,fact
Bk8udEEeM_1,The model is then trained with CelebA under different parameters settings and results are analyzed .,fact
Bk8udEEeM_2,"Quality and significance : This is quite a technical paper , written in a very compressed form and is a bit hard to follow .",evaluation
Bk8udEEeM_3,Mostly it is hard to estimate what is the contribution of the model and how the results differ from baseline models .,evaluation
Bk8udEEeM_4,Clarity : I would say this is one of the weak points of the paper - the paper is not well motivated and the results are not clearly presented .,evaluation
Bk8udEEeM_5,Originality : Seems original .,evaluation
Bk8udEEeM_6,Pros : * Interesting energy formulation and variation over BEGAN,evaluation
Bk8udEEeM_7,Cons : * Not a clear paper,evaluation
Bk8udEEeM_8,* results are only partially motivated and analyzed,evaluation
BkC87_Cgz_0,The paper proposes to augment ( traditional ) text-based sentence generation/dialogue approaches by incorporating visual information .,fact
BkC87_Cgz_1,"The idea is that associating visual information with input text , and using that associated visual information as additional input will produce better output text than using only the original input text .",fact
BkC87_Cgz_2,The basic idea is to collect a bunch of data consisting of both text and associated images or video .,fact
BkC87_Cgz_3,"Here , this was done using Japanese news programs .",fact
BkC87_Cgz_4,"The text + image/video is used to train a model that requires both as input and that encodes both as context vectors , which are then combined and decoded into output text .",fact
BkC87_Cgz_5,"Next , the image inputs are eliminated , with the encoded image context vector being instead associatively predicted directly from the encoded text context vector ( why not also use the input text to help predict the visual context ? ) , which is still obtained from the text input , as before .",fact
BkC87_Cgz_6,The result is a model that can make use of the text-visual associations without needing visual stimuli .,fact
BkC87_Cgz_7,This is a nice idea .,evaluation
BkC87_Cgz_8,"Actually , based on the brief discussion in Section 2.2.2 , it occurs to me that the model might not really be learning visual context vectors associatively , or , that this does n't really have meaning in some sense .",evaluation
BkC87_Cgz_9,"Does it make sense to say that what it is really doing is just learning to associate other concepts/words with the input text , and that it is using the augmenting visual information in the training data to provide those associations ?",fact
BkC87_Cgz_10,Is this worth talking about ?,non-arg
BkC87_Cgz_11,"Unfortunately , while the idea has merit , and I 'd like to see it pursued ,",evaluation
BkC87_Cgz_12,"the paper suffers from a fatal lack of validation/evaluation ,",evaluation
BkC87_Cgz_13,"which is very curious , given the amount of data that was collected , the fact that the authors have both a training and a test set , and that there are several natural ways such an evaluation might be performed .",evaluation
BkC87_Cgz_14,"The two examples of Fig 3 and the additional four examples in the appendix are nice for demonstrating some specific successes or weaknesses of the model ,",evaluation
BkC87_Cgz_15,"but they are in no way sufficient for evaluation of the system , to demonstrate its accuracy or value in general .",evaluation
BkC87_Cgz_16,"Perhaps the most obvious thing that should be done is to report the model 's accuracy for reproducing the news dialogue , that is , how accurately is the next sentence predicted by the baseline and ACM models over the training instances and over the test data ?",request
BkC87_Cgz_17,How does this compare with other state-of-the-art models for dialogue generation trained on this data ( perhaps trained only on the textual part of the data in some cases ) ?,request
BkC87_Cgz_18,"Second , some measure of accuracy for recall of the associative image context vector should be reported ; for example , on average , how close ( cosine similarity or some other appropriate measure ) is the associatively recalled image context vector to the target image context vector ?",request
BkC87_Cgz_19,On average ?,request
BkC87_Cgz_20,Best case ?,request
BkC87_Cgz_21,Worst case ?,request
BkC87_Cgz_22,How often is this associative vector closer to a confounding image vector than an appropriate one ?,request
BkC87_Cgz_23,A third natural kind of validation would be some form of study employing human subjects to test it 's quality as a generator of dialogue .,request
BkC87_Cgz_24,"One thing to note , the example of learning to associate the snowy image with the text about university entrance exams demonstrates that the model is memorizing rather than generalizing .",fact
BkC87_Cgz_25,"In general , this is a false association",fact
BkC87_Cgz_26,"( that is , in general , there is no reason that snow should be associated with exams on the 14th and 15th — the month is not mentioned , which might justify such an association . )",fact
BkC87_Cgz_27,Another thought : did you try not retraining the decoder and attention mechanisms for step 3 ?,non-arg
BkC87_Cgz_28,"In theory , if step 2 is successful , the retraining should not be necessary .",fact
BkC87_Cgz_29,"To the extent that it is necessary , step 2 has failed to accurately predict visual context from text .",evaluation
BkC87_Cgz_30,This seems like an interesting avenue to explore ( and is obviously related to the second type of validation suggested above ) .,request
BkC87_Cgz_31,"Also , in addition to the baseline model , it seems like it would be good to compare a model that uses actual visual input and the model of step 1 against the model of step 3 ( possibly bot retrained and not retrained ) to see the effect on the outputs generated — how well do each of these do at predicting the next sentence on both training and test sets ?",request
BkC87_Cgz_32,Other concerns : 1 . The paper is too long by almost a page in main content .,evaluation
BkC87_Cgz_33,2 . The paper exhibits significant English grammar and usage issues,evaluation
BkC87_Cgz_34,and should be carefully proofed by a native speaker .,request
BkC87_Cgz_35,"3 . There are lots of undefined variables in the Eqs . ( s , W_s , W_c , b_s , e_t , i , etc . )",fact
BkC87_Cgz_36,"Given the context and associated discussion , it is almost possible to sort out what all of them mean ,",evaluation
BkC87_Cgz_37,but brief careful definitions should be given for clarity .,request
BkC87_Cgz_38,"4 . Using news broadcasts as a substitute for true dialogue data seems kind of problematic ,",evaluation
BkC87_Cgz_39,though I see why it was done .,evaluation
rkFUZ2uxf_0,The authors introduce a set of very simple tasks that are meant to illustrate the challenges of learning visual relations .,fact
rkFUZ2uxf_1,"They then evaluate several existing network architectures on these tasks ,",fact
rkFUZ2uxf_2,and show that results are not as impressive as others might have assumed they would be .,evaluation
rkFUZ2uxf_3,"They show that while recent approaches ( e.g. relational networks ) can generalize reasonably well on some tasks , these results do not generalize as well to held-out-object scenarios as might have been assumed .",fact
rkFUZ2uxf_4,Clarity : The paper is fairly clearly written .,evaluation
rkFUZ2uxf_5,I think I mostly followed it .,evaluation
rkFUZ2uxf_6,Quality : I 'm intrigued by but a little uncomfortable with the generalization metrics that the authors use .,evaluation
rkFUZ2uxf_7,The authors estimate the performance of algorithms by how well they generalize to new image scenarios when trained on other image conditions .,fact
rkFUZ2uxf_8,"The authors state that "" . . . the effectiveness of an architecture to learn visual-relation problems should be measured in terms of generalization over multiple variants of the same problem , not over multiple splits of the same dataset . """,fact
rkFUZ2uxf_9,"Taken literally , this would rule out a lot of modern machine learning , even obviously very good work .",fact
rkFUZ2uxf_10,"On the other hand , it 's clear that at some point , generalization needs to occur in testing ability to understand relationships .",fact
rkFUZ2uxf_11,"I 'm a little worried that it 's "" in the eye of the beholder "" whether a given generalization should be expected to work or not .",evaluation
rkFUZ2uxf_12,"There are essentially three scenarios of generalization discussed in the paper : ( a ) various generalizations of image parameters in the PSVRT dataset ( b ) various hold-outs of the image parameters in the sort-of-CLEVR dataset ( c ) from sort-of-CLEVR "" objects "" to PSVRT bit patterns",fact
rkFUZ2uxf_13,The result that existing architectures did n't do very well at these generalizations ( especially b and c ) * may * be important -- or it may not .,evaluation
rkFUZ2uxf_14,"Perhaps if CNN+RN were trained on a quite rich real-world training set with a variety of real-world three-D objects beyond those shown in sort-of-CLEVR , it would generalize to most other situations that might be encountered .",evaluation
rkFUZ2uxf_15,"After all , when we humans generalize to understanding relationships , exactly what variability is present in our "" training sets "" as compared to our "" testing "" situations ?",non-arg
rkFUZ2uxf_16,"How do the authors know that humans are effectively generalizing rather than just "" interpolating "" within their ( very rich ) training set ?",evaluation
rkFUZ2uxf_17,"It 's not totally clear to me that if totally naive humans ( who had never seen spatial relationships before ) were evaluated on exactly the training/testing scenarios described above , that they would generalize particularly well either .",evaluation
rkFUZ2uxf_18,I do n't think it can just be assumed a priori that humans would be super good this form of generalization .,evaluation
rkFUZ2uxf_19,So how should authors handle this criticism ?,non-arg
rkFUZ2uxf_20,What would be useful would either be some form of positive control .,request
rkFUZ2uxf_21,"Either human training data showing very effective generalization ( if one could somehow make "" novel "" relationships unfamiliar to humans ) , or a different network architecture that was obviously superior in generalization to CNN+RN .",request
rkFUZ2uxf_22,"If such were present , I 'd rate this paper significantly higher .",evaluation
rkFUZ2uxf_23,"Also , I ca n't tell if I really fully believe the results of this paper .",evaluation
rkFUZ2uxf_24,I do n't doubt that the authors saw the results they report .,evaluation
rkFUZ2uxf_25,"However , I think there 's some chance that if the same tasks were in the hands of people who * wanted * CNNs or CNN+RN to work well , the results might have been different .",evaluation
rkFUZ2uxf_26,"I ca n't point to exactly what would have to be different to make things "" work "" ,",evaluation
rkFUZ2uxf_27,because it 's really hard to do that ahead of actually trying to do the work .,evaluation
rkFUZ2uxf_28,"However , this suspicion on my part is actually a reason I think it might be * good * for this paper to be published at ICLR .",evaluation
rkFUZ2uxf_29,This will give the people working on ( e.g. ) CNN+RN somewhat more incentive to try out the current paper 's benchmarks and either improve their architecture or show that the the existing one would have totally worked if only tried correctly .,evaluation
rkFUZ2uxf_30,I myself am very curious about what would happen and would love to see this exchange catalyzed .,evaluation
rkFUZ2uxf_31,Originality and Significance : The area of relation extraction seems to me to be very important and probably a bit less intensively worked on that it should be .,evaluation
rkFUZ2uxf_32,"However , as the authors here note , there 's been some recent work ( e.g. Santoro 2017 ) in the area .",fact
rkFUZ2uxf_33,"I think that the introduction of baselines benchmark challenge datasets such as the ones the authors describe here is very useful , and is a somewhat novel contribution .",evaluation
Hy4_ANE-f_0,This paper studies new off-policy policy optimization algorithm using relative entropy objective and use EM algorithm to solve it .,fact
Hy4_ANE-f_1,"The general idea is not new , aka , formulating the MDP problem as a probabilistic inference problem .",fact
Hy4_ANE-f_2,"There are some technical questions : 1 . For parametric EM case , there is asymptotic convergence guarantee to local optima case ;",fact
Hy4_ANE-f_3,"However , for nonparametric EM case , there is no guarantee for that .",fact
Hy4_ANE-f_4,This is the biggest concern I have for the theoretical justification of the paper .,evaluation
Hy4_ANE-f_5,"2 . In section 4 , it is said that Retrace algorithm from Munos et al. ( 2016 ) is used for policy evaluation .",fact
Hy4_ANE-f_6,This is not true .,fact
Hy4_ANE-f_7,"The Retrace algorithm , is per se , a value iteration algorithm .",fact
Hy4_ANE-f_8,"I think the author could say using the policy evaluation version of Retrace , or use the truncated importance weights technique as used in Retrace algorithm , which is more accurate .",request
Hy4_ANE-f_9,"Besides , a minor point : Retrace algorithm is not off-policy stable with function approximation , as shown in several recent papers , such as “ Convergent Tree-Backup and Retrace with Function Approximation ” .",fact
Hy4_ANE-f_10,But this is a minor point if the author does n’t emphasize too much about off-policy stability .,evaluation
Hy4_ANE-f_11,3 . The shifting between the unconstrained multiplier formulation in Eq .9 to the constrained optimization formulation in Eq .10 should be clarified .,request
Hy4_ANE-f_12,"Usually , an in-depth analysis between the choice of \ lambda in multiplier formulation and the \ epsilon in the constraint should be discussed ,",request
Hy4_ANE-f_13,which is necessary for further theoretical analysis .,fact
Hy4_ANE-f_14,4 . The experimental conclusions are conducted without sound evidence .,evaluation
Hy4_ANE-f_15,"For example , the author claims the method to be ' highly data efficient ' compared with existing approaches ,",fact
Hy4_ANE-f_16,"however , there is no strong evidence supporting this claim .",evaluation
Hy4_ANE-f_17,"Overall , although the motivation of this paper is interesting ,",evaluation
Hy4_ANE-f_18,I think there is still a lot of details to improve .,evaluation
SyrOMN9eM_0,"The authors propose WAGE , which discretized weights , activations , gradients , and errors at both training and testing time .",fact
SyrOMN9eM_1,"By quantization and shifting , SGD training without momentum , and removing the softmax at output layer as well , the model managed to remove all cumbersome computations from every aspect of the model ,",fact
SyrOMN9eM_2,thus eliminating the need for a floating point unit completely .,fact
SyrOMN9eM_3,"Moreover , by keeping up to 8-bit accuracy , the model performs even better than previously proposed models .",fact
SyrOMN9eM_4,I am eager to see a hardware realization for this method because of its promising results .,evaluation
SyrOMN9eM_5,"The model makes a unified discretization scheme for 4 different kinds of components ,",fact
SyrOMN9eM_6,and the accuracy for each of the kind becomes independently adjustable .,fact
SyrOMN9eM_7,"This makes the method quite flexible and has the potential to extend to more complicated networks , such as attention or memory .",evaluation
SyrOMN9eM_8,"One caveat is that there seem to be some conflictions in the results shown in Table 1 , especially ImageNet .",evaluation
SyrOMN9eM_9,"Given the number of bits each of the WAGE components asked for , a 28.5 % top 5 error rate seems even lower than XNOR .",evaluation
SyrOMN9eM_10,"I suspect it is due to the fact that gradients and errors need higher accuracy for real-valued input ,",evaluation
SyrOMN9eM_11,"but if that is the case , accuracies on SVHN and CIFAR-10 should also reflect that .",evaluation
SyrOMN9eM_12,"Or , maybe it is due to hyperparameter setting or insufficient training time ?",evaluation
SyrOMN9eM_13,"Also , dropout seems not conflicting with the discretization .",evaluation
SyrOMN9eM_14,"If there are no other reasons , it would make sense to preserve the dropout in the network as well .",evaluation
SyrOMN9eM_15,"In general , the paper was written in good quality and in detail ,",evaluation
SyrOMN9eM_16,I would recommend a clear accept .,evaluation
BkQD60b-f_0,"The paper proposes the use of a GAN to learn the distribution of image classes from an existing classifier ,",fact
BkQD60b-f_1,that is a nice and straightforward idea .,evaluation
BkQD60b-f_2,"From the point of view of forensic analysis of a classifier , it supposes a more principled strategy than a brute force attack based on the classification of a database and some conditional density estimation of some intermediate image features .",fact
BkQD60b-f_3,"Unfortunately , the experiments are inconclusive .",evaluation
BkQD60b-f_4,Quality : The key question of the proposed scheme is the role of the auxiliary dataset .,evaluation
BkQD60b-f_5,"In the EMNIST experiment , the results for the “ exact same ” and “ partly same ” situations are good ,",evaluation
BkQD60b-f_6,"but it seems that for the “ mutually exclusive ” situation the generated samples look like letters , not numbers ,",evaluation
BkQD60b-f_7,and raises the question on the interpolation ability of the generator .,evaluation
BkQD60b-f_8,"In the FaceScrub experiment is even more difficult to interpret the results ,",evaluation
BkQD60b-f_9,basically because we do not even know the full list of person identities .,fact
BkQD60b-f_10,It seems that generated images contain only parts of the auxiliary images related to the most discriminative features of the given classifier .,evaluation
BkQD60b-f_11,Does this imply that the GAN models a biased probability distribution of the image class ?,request
BkQD60b-f_12,What is the result when the auxiliary dataset comes from a different kind of images ?,request
BkQD60b-f_13,"Due to the difficulty of evaluating GAN results , more experiments are needed to determine the quality and significance of this work .",request
BkQD60b-f_14,"Clarity : The paper is well structured and written ,",evaluation
BkQD60b-f_15,but Sections 1-4 could be significantly shorter to leave more space to additional and more conclusive experiments .,request
BkQD60b-f_16,Some typos on Appendix A should be corrected .,request
BkQD60b-f_17,Originality : the paper is based on a very smart and interesting idea and a straightforward use of GANs .,evaluation
BkQD60b-f_18,"Significance : If additional simulations confirm the author ’s claims , this work can represent a significant contribution to the forensic analysis of discriminative classifiers .",evaluation
SJdWxzoxz_0,Summary : The paper presents a novel method for answering “ How many … ? ” questions in the VQA datasets .,fact
SJdWxzoxz_1,"Unlike previously proposed approaches , the proposed method uses an iterative sequential decision process for counting the relevant entity .",fact
SJdWxzoxz_2,The proposed model makes discrete choices about what to count at each time step .,fact
SJdWxzoxz_3,Another qualitative difference compared to existing approaches is that the proposed method returns bounding boxes for the counted object .,fact
SJdWxzoxz_4,The training and evaluation of the proposed model and baselines is done on a subset of the existing VQA dataset that consists of “ How many … ? ” questions .,fact
SJdWxzoxz_5,The experimental results show that the proposed model outperforms the baselines discussed in the paper .,fact
SJdWxzoxz_6,Strengths : 1 . The idea of sequential counting is novel and interesting .,evaluation
SJdWxzoxz_7,2 . The analysis of model performance by grouping the questions as per frequency with which the counting object appeared in the training data is insightful .,evaluation
SJdWxzoxz_8,"Weaknesses : 1 . The proposed dataset consists of 17,714 QA pairs in the dev set , whereas only 5,000 QA pairs in the test set .",fact
SJdWxzoxz_9,Such a 3.5:1 split of dev and test seems unconventional .,evaluation
SJdWxzoxz_10,"Also , the size of the test set seems pretty small given the diversity of the questions in the VQA dataset .",evaluation
SJdWxzoxz_11,2 . The paper lacks quantitative comparison with existing models for counting such as with Chattopadhyay et al .,fact
SJdWxzoxz_12,This would require the authors to report the accuracies of existing models by training and evaluating on the same subset as that used for the proposed model .,request
SJdWxzoxz_13,Absence of such a comparison makes it difficult to judge how well the proposed model is performing compared to existing models .,evaluation
SJdWxzoxz_14,3 . The paper lacks analysis on how much of performance improvement is due to visual genome data augmentation and pre-training ?,fact
SJdWxzoxz_15,"When comparing with existing models ( as suggested in above ) , this analysis should be done , so as to identify the improvements coming from the proposed model alone .",request
SJdWxzoxz_16,4 . The paper does not report the variation in model performance when changing the weights of the various terms involved in the loss function ( equations 15 and 16 ) .,fact
SJdWxzoxz_17,"5 . Regarding Chattopadhyay et al. the paper says that “ However , their analysis was limited to the specific subset of examples where their approach was applicable . ”",fact
SJdWxzoxz_18,It would be good it authors could elaborate on this a bit more .,request
SJdWxzoxz_19,"6 . The relation prediction part of the vision module in the proposed model seems quite similar to the Relation Networks ,",evaluation
SJdWxzoxz_20,but the paper does not mention Relation Networks .,fact
SJdWxzoxz_21,It would be good to cite the Relation Networks paper and state clearly if the motivation is drawn from Relation Networks .,request
SJdWxzoxz_22,7 . It is not clear what are the 6 common relationships that are being considered in equation 1 .,evaluation
SJdWxzoxz_23,Could authors please specify these ?,request
SJdWxzoxz_24,"8 . In equation 1 , if only 6 relationships are being considered , then why does <VAR> map to <VAR> instead of <VAR> ?",evaluation
SJdWxzoxz_25,"9 . In equations 4 and 5 , it is not clarified what each symbol represents , making it difficult to understand .",evaluation
SJdWxzoxz_26,10 . What is R in equation 15 ?,request
SJdWxzoxz_27,Is it reward ?,non-arg
SJdWxzoxz_28,Overall : The paper proposes a novel and interesting idea for solving counting questions in the Visual Question Answering tasks .,evaluation
SJdWxzoxz_29,"However , the writing of the paper needs to be improved to make is easier to follow .",request
SJdWxzoxz_30,The experimental set-up – the size of the test dataset seems too small .,evaluation
SJdWxzoxz_31,"And lastly , the paper needs to add comparisons with existing models on the same datasets as used for the proposed model .",request
SJdWxzoxz_32,"So , the paper seems to be not ready for the publication yet .",evaluation
SJUEXlDxf_0,"Summary This paper presents Neural Process Networks , an architecture for capturing procedural knowledge stated in texts that makes use of a differentiable memory , a sentence and word attention mechanism , as well as learning action representations and their effect on entity representations .",fact
SJUEXlDxf_1,"The architecture is tested for tracking entities in recipes , as well as generating the natural language description for the next step in a recipe .",fact
SJUEXlDxf_2,"It is compared against a suit of baselines , such as GRUs , Recurrent Entity Networks , Seq2Seq and the Neural Checklist Model .",fact
SJUEXlDxf_3,"While I liked the overall paper ,",evaluation
SJUEXlDxf_4,"I am worried about the generality of the model , the qualitative analysis , as well as a fair comparison to Recurrent Entity Networks and non-neural baselines .",evaluation
SJUEXlDxf_5,"Strengths I believe the authors made a good effort in comparing against existing neural baselines ( Recurrent Entity Networks , Neural Checklist Model ) * for their task * .",evaluation
SJUEXlDxf_6,"That said , it is unclear to me how generally applicable the method is and whether the comparison against Recurrent Entity Networks is fair ( see Weaknesses ) .",evaluation
SJUEXlDxf_7,I like the ablation study .,evaluation
SJUEXlDxf_8,Weaknesses While I find the Neural Process Networks architecture interesting,evaluation
SJUEXlDxf_9,"and I acknowledge that it outperforms Recurrent Entity Networks for the presented tasks ,",evaluation
SJUEXlDxf_10,after reading the paper it is not clear to me how generally applicable the architecture is .,evaluation
SJUEXlDxf_11,Some design choices seem rather tailored to the task at hand ( manual collection of actions MTurk annotation in section 3.1 ),evaluation
SJUEXlDxf_12,and I am wondering where else the authors see their method being applied given that the architecture relies on all entities and actions being known in advance .,evaluation
SJUEXlDxf_13,My understanding is that the architecture could be applied to bAbI and CBT ( the two tasks used in the Recurrent Entity Networks paper ) .,fact
SJUEXlDxf_14,"If that is the case , a fair comparison to Recurrent Entity Networks would have been to test against Recurrent Entity Networks on these tasks too .",evaluation
SJUEXlDxf_15,"If they the architecture can not be applied in these tasks , the authors should explain why .",request
SJUEXlDxf_16,I am not convinced by the qualitative analysis .,evaluation
SJUEXlDxf_17,"Table 2 tells me that even for the best model the entity selection performance is rather unreliable ( only 55.39 % F1 ) ,",fact
SJUEXlDxf_18,"yet all examples shown in Table 3 look really good , missing only the two entities oil ( 1 ) and sprinkles ( 3 ) .",evaluation
SJUEXlDxf_19,This suggests that these examples were cherry-picked,evaluation
SJUEXlDxf_20,and I would like to see examples that are sampled randomly from the dev set .,request
SJUEXlDxf_21,I have a similar concern regarding the generation task .,evaluation
SJUEXlDxf_22,"First , it is not mentioned where the examples in Table 6 are taken from – is it the train , dev or test set ?",fact
SJUEXlDxf_23,"Second , the overall BLEU score seems quite low even for the best model ,",evaluation
SJUEXlDxf_24,yet the examples in Table 6 look really good .,evaluation
SJUEXlDxf_25,"In my opinion , a good qualitative analysis should also discuss failure cases .",request
SJUEXlDxf_26,"Since the BLEU score is so low here ,",evaluation
SJUEXlDxf_27,you might also want to compare perplexity of the models .,request
SJUEXlDxf_28,The qualitative analysis in Table 5 is not convincing either .,evaluation
SJUEXlDxf_29,In Appendix A. 1 it is mentioned that word embeddings are initialized from word2vec trained on the training set .,fact
SJUEXlDxf_30,"My suspicion is that one would get the clustering in Table 4 already from those pretrained vectors , maybe even when pretrained on the Google news corpus .",fact
SJUEXlDxf_31,"Hence , it is not clear what propagating gradients through the Neural Process Networks into the action embeddings adds , or put differently , why does it have to be a differentiable architecture when an NLP pipeline might be enough ?",evaluation
SJUEXlDxf_32,This could easily be tested by another ablation where action embeddings are pretrained using word2vec and then fixed during training of the Neural Process Network .,fact
SJUEXlDxf_33,"Moreover , in 3.3 it is mentioned that even the Action Selection is pretrained ,",fact
SJUEXlDxf_34,which makes me wonder what is actually trained jointly in the architecture and what is not .,evaluation
SJUEXlDxf_35,"I think the difficulty of the task at hand needs to be discussed at some point , ideally early in the paper .",request
SJUEXlDxf_36,"Until examples on page 7 are shown , I did not have a sense for why a neural architecture is chosen .",evaluation
SJUEXlDxf_37,"For example , in 2.3 it is mentioned that for "" wash and cut "" the two functions fwash and fcut need to be selected .",fact
SJUEXlDxf_38,"For this example , this seems trivial",evaluation
SJUEXlDxf_39,as the functions have the same name,fact
SJUEXlDxf_40,( and you could even have a function per name ! ) .,fact
SJUEXlDxf_41,"As far as I understand , the point of the action selector is to only have a fixed number of learned actions and multiple words ( cut , slice etc. ) should select the same action fcut .",fact
SJUEXlDxf_42,Otherwise ( if there is little language ambiguity ) I would not see the need for a complex neural architecture .,evaluation
SJUEXlDxf_43,"Related to that , a non-neural baseline for the entity selection task that in my opinion definitely needs to be added is extracting entities using a pretrained NER system and returning all of them as the selection .",request
SJUEXlDxf_44,"p2 Footnote 1 : So if I understand this correctly , this work builds upon a dataset of over 65k recipes from Kiddon et al. ( 2016 ) , but only for 875 of those detailed annotations were created ?",non-arg
SJUEXlDxf_45,"Minor Comments p1 : The statement "" most natural language understanding algorithms do not have the capacity … "" should be backed by reference .",request
SJUEXlDxf_46,"p2 : "" context representation ht "" – I would directly mention that this is a sentence encoding .",request
SJUEXlDxf_47,p3 : 2.4 : I have the impression what you are describing here is known in the literature as entity linking .,evaluation
SJUEXlDxf_48,p3 Eq .3 : Is n't c3 * 0 always a vector of zeros ?,evaluation
SJUEXlDxf_49,"p4 Eq .6 : W4 is an order-3 tensor , correct ?",evaluation
SJUEXlDxf_50,p4 Eq .8 : What is YC and WC here and what are their dimensions ?,request
SJUEXlDxf_51,"I am confused by the softmax ,",evaluation
SJUEXlDxf_52,"as my understanding ( from reading the paragraph on the Action Selection Loss on p. 5 ) was that the expression in the softmax here is a scalar ( as it is done for every possible action ) ,",fact
SJUEXlDxf_53,so this should be a sigmoid to allow for multiple actions to attain a probability of 1 ?,fact
SJUEXlDxf_54,"p5 : "" See Appendix for details "" - > "" see Appendix C for details """,request
SJUEXlDxf_55,p5 3.3 : Could you elaborate on the heuristic for extracting verb mentions ?,request
SJUEXlDxf_56,Is only one verb mention per sentence extracted ?,request
SJUEXlDxf_57,"p5 : "" trained to minimize cross-entropy loss "" - > "" trained to minimize the cross-entropy loss """,request
SJUEXlDxf_58,p5 3.3 : What is the global loss ?,request
SJUEXlDxf_59,"p6 : "" been read ( § 2.5 . "" - > "" been read ( § 2.5 ) . """,request
SJUEXlDxf_60,"p6 : "" We encode these vectors using a bidirectional GRU "" – I think you composing a fixed-dimensional vector from the entity vectors ?",request
SJUEXlDxf_61,What 's eI ?,request
SJUEXlDxf_62,p7 : For which statement is ( Kim et al. 2016 ) the reference ?,request
SJUEXlDxf_63,"Surely , they did not invent the Hadamard product .",fact
SJUEXlDxf_64,"p8 : "" Our model , in contrast "" use "" - > "" Our model , in contrast , uses "" .",request
SJUEXlDxf_65,"p8 Related Work : I think it is important to mention that existing architectures such as Memory Netwroks could , in principle , learn to track entities and devote part of their parameters to learn the effect of actions .",request
SJUEXlDxf_66,What Neural Process Networks are providing is a strong inductive bias for tracking entities and learning the effect of actions that is useful for the task considered in this paper .,evaluation
SJUEXlDxf_67,"As mentioned in the weaknesses , this might however come at the price of a less general model ,",fact
SJUEXlDxf_68,which should be discussed .,request
B1RZJ1cxG_0,The paper explores momentum SGD and an adaptive version of momentum SGD which the authors name YF ( Yellow Fin ) .,fact
B1RZJ1cxG_1,They compare YF to hand tuned momentumSGD and to Adam in several deep learning applications .,fact
B1RZJ1cxG_2,I found the first part which discusses the theoretical motivation behind YF to be very confusing and misleading :,evaluation
B1RZJ1cxG_3,"Based on the analysis of 1-dimensional problems , the authors design a framework and an algorithm that supposedly ensures accelerated convergence .",evaluation
B1RZJ1cxG_4,There are two major problems with this approach : - First : Exploring 1-dim functions is indeed a nice way to get some intuition .,evaluation
B1RZJ1cxG_5,"Yet , algorithms that work in the 1-dim case do not trivially generalize to high dimensions ,",evaluation
B1RZJ1cxG_6,and such reasoning might lead to very bad solutions .,evaluation
B1RZJ1cxG_7,- Second : Accelerated GD does not benefit over GD in the 1-dim case .,fact
B1RZJ1cxG_8,"And therefore , this is not an appropriate setting to explore acceleration .",evaluation
B1RZJ1cxG_9,"Concretely , the definition of the generalized condition number $ \ nu $ , and relating it to the standard definition of the condition number $ \ kappa $ , is very misleading .",evaluation
B1RZJ1cxG_10,"This is since $ \ kappa = 1 $ for 1-dim problems ,",fact
B1RZJ1cxG_11,and therefore accelerated GD does not have any benefits over non accelerated GD in this case .,evaluation
B1RZJ1cxG_12,"However , $ \ nu $ might be much larger than 1 even in the 1-dim case .",evaluation
B1RZJ1cxG_13,Regarding the algorithm itself : there are too many hyper-parameters ( which depend on each other ) that are tuned ( per-dimension ) .,evaluation
B1RZJ1cxG_14,"And as I have mentioned , the design of the algorithm is inspired by the analysis of 1-dim quadratic functions .",evaluation
B1RZJ1cxG_15,"Thus , it is very hard for me to believe that this algorithm works in practice unless very careful fine tuning is employed .",evaluation
B1RZJ1cxG_16,"The authors mention that their experiments were done without tuning or with very little tuning , which is very mysterious for me .",evaluation
B1RZJ1cxG_17,"In contrast to the theoretical part , the experiments seems very encouraging .",evaluation
B1RZJ1cxG_18,Showing YF to perform very well on several deep learning tasks without ( or with very little ) tuning .,evaluation
B1RZJ1cxG_19,"Again , this seems a bit magical or even too good to be truth .",evaluation
B1RZJ1cxG_20,"I suggest the authors to perform a experiment with say a qaudratic high dimensional function , which is not aligned with the axes in order to illustrate how their method behaves and try to give intuition .",request
BJAg3e7ZM_0,1 ) Summary This paper proposes a flow-based neural network architecture and adversarial training for multi-step video prediction .,fact
BJAg3e7ZM_1,The neural network in charge of predicting the next frame in a video implicitly generates flow that is used to transform the previously observed frame into the next .,fact
BJAg3e7ZM_2,"Additionally , this paper proposes a new quantitative evaluation criteria based on the observed flow in the prediction in comparison to the groundtruth .",fact
BJAg3e7ZM_3,Experiments are performed on a new robot arm dataset proposed in the paper where they outperform the used baselines .,fact
BJAg3e7ZM_4,2 ) Pros : + New quantitative evaluation criteria based on motion accuracy .,fact
BJAg3e7ZM_5,+ New dataset for robot arm pushing objects .,fact
BJAg3e7ZM_6,3 ) Cons : Overall architectural prediction network differences with baseline are unclear :,evaluation
BJAg3e7ZM_7,The differences between the proposed prediction network and [ 1 ] seem very minimal .,evaluation
BJAg3e7ZM_8,"In Figure 3 , it is mentioned that the network uses a U-Net with recurrent connections .",fact
BJAg3e7ZM_9,This seems like a very minimal change in the overall architecture proposed .,evaluation
BJAg3e7ZM_10,"Additionally , there is a paragraph of “ architecture improvements ” which also are minimal changes .",evaluation
BJAg3e7ZM_11,"Based on the title of section 3 , it seems that there is a novelty on the “ prediction with flow ” part of this method .",fact
BJAg3e7ZM_12,"If this is a fact , there is no equation describing how this flow is computed .",fact
BJAg3e7ZM_13,"However , if this “ flow ” is computed the same way [ 1 ] does it , then the title is misleading .",evaluation
BJAg3e7ZM_14,Adversarial training objective alone is not new as claimed by the authors :,fact
BJAg3e7ZM_15,The adversarial objective used in this paper is not new .,evaluation
BJAg3e7ZM_16,"Works such as [ 2,3 ] have used this objective function for single step and multi-step frame prediction training , respectively .",fact
BJAg3e7ZM_17,"If the authors refer to the objective being new in the sense of using it with an action conditioned video prediction network , then this is again an extremely minimal contribution .",evaluation
BJAg3e7ZM_18,"Essentially , the authors just took the previously used objective function and used it with a different network .",fact
BJAg3e7ZM_19,"If the authors feel otherwise , please comment on why this is the case .",request
BJAg3e7ZM_20,"Incomplete experiments : The authors only show experiments on videos containing objects that have already been seen ,",fact
BJAg3e7ZM_21,but no experiments with objects never seen before .,fact
BJAg3e7ZM_22,The missing experiment concerns me in the sense that the network could just be memorizing previously seen objects .,evaluation
BJAg3e7ZM_23,"Additionally , the authors present evaluation based on PSNR and SSIM on the overall predicted video , but not in a per-step paradigm .",fact
BJAg3e7ZM_24,"However , the authors show this per-step evaluation in the Amazon Mechanical Turk , and predicted object position evaluations .",fact
BJAg3e7ZM_25,Unclear evaluation : The way the Amazon Mechanical Turk experiments are performed are unclear and/or not suited for the task at hand .,evaluation
BJAg3e7ZM_26,"Based on the explanation of how these experiments are performed , the authors show individual images to mechanical turkers .",fact
BJAg3e7ZM_27,"If we are evaluating the video prediction task for having real or fake looking videos , the turkers need to observe the full video and judge based on that .",request
BJAg3e7ZM_28,"If we are just showing images , then they are evaluating image synthesis , which do not necessarily contain the desired properties in videos such as temporal coherence .",fact
BJAg3e7ZM_29,Additional comments : The paper needs a considerable amount of polishing .,request
BJAg3e7ZM_30,4 ) Conclusion : This paper seems to contain very minimal changes in comparison to the baseline by [ 1 ] .,evaluation
BJAg3e7ZM_31,"The adversarial objective is not novel as mentioned by the authors and has been used in [ 2,3 ] .",evaluation
BJAg3e7ZM_32,Evaluation is unclear and incomplete .,evaluation
BJAg3e7ZM_33,References : [1] <CIT>,reference
BJAg3e7ZM_34,[2] <CIT>,reference
BJAg3e7ZM_35,[3] <CIT>,reference
B129GzFxf_0,This paper proposes a new method for reverse curriculum generation by gradually reseting the environment in phases and classifying states that tend to lead to success .,fact
B129GzFxf_1,"It additionally proposes a mechanism for learning from human-provided "" key states "" .",fact
B129GzFxf_2,"The ideas in this paper are quite nice ,",evaluation
B129GzFxf_3,but the paper has significant issues with regard to clarity and applicability to real-world problems :,evaluation
B129GzFxf_4,"First , it is unclear is the proposed method requires access only high-dimensional observations ( e.g. images ) during training or if it additionally requires low-dimensional states ( e.g. sufficient information to reset the environment ) .",evaluation
B129GzFxf_5,"In most compelling problems settings where a low-dimensional representation that sufficiently explains the current state of the world is available during training , then it is also likely that one can write down a nicely shaped reward function using that state information during training , in which case , it makes sense to use such a reward function .",evaluation
B129GzFxf_6,"This paper seems to require access to low-dimensional states , and specifically considers the sparse-reward setting ,",fact
B129GzFxf_7,which seems contrived .,evaluation
B129GzFxf_8,"Second , the paper states that the assumption "" when resetting , the agent can be reset to any state "" can be satisfied in problems such as real-world robotic manipulation .",fact
B129GzFxf_9,This is not correct .,fact
B129GzFxf_10,"If the robot could autonomously reset to any state , then we would have largely solved robotic manipulation .",fact
B129GzFxf_11,"Further , it is not always realistic to assume access to low-dimensional state information during training on a real robotic system ( e.g. knowing the poses of all of the objects in the world ) .",evaluation
B129GzFxf_12,"Third , the experiments section lacks crucial information needed to understand the experiments .",evaluation
B129GzFxf_13,"What is the state , observation , and action space for each problem setting ?",request
B129GzFxf_14,What is the reward function for each problem setting ?,request
B129GzFxf_15,What reinforcement learning algorithm is used in combination with the curriculum and tendency rewards ?,request
B129GzFxf_16,Are the states and actions continuous or discrete ?,request
B129GzFxf_17,"Without this information , it is difficult to judge the merit of the experimental setting .",evaluation
B129GzFxf_18,"Fourth , the proposed method seems to lack motivation , making the proposed scheme seem a bit ad hoc .",evaluation
B129GzFxf_19,Could each of the components be motivated further through more discussion and/or ablative studies ?,request
B129GzFxf_20,"Finally , the main text of the paper is substantially longer than the recommended page limit .",fact
B129GzFxf_21,It should be shortened by making the writing more concise .,request
B129GzFxf_22,"Beyond my feedback on clarity and significance , here are further pieces of feedback with regard to the technical content , experiments , and related work : I 'm wondering -- can the reward shaping in Equation 2 be made to satisfy the property of not affecting the final policy ?",request
B129GzFxf_23,( see Ng et al. ' 09 ),reference
B129GzFxf_24,"If so , such a reward shaping would make the method even more appealing .",evaluation
B129GzFxf_25,How do the experiments in section 5.4 compare to prior methods and ablations ?,request
B129GzFxf_26,"Without such a comparison , it is impossible to judge the performance of the proposed method and the level of difficulty of these tasks .",fact
B129GzFxf_27,"At the very least , the paper should compare the performance of the proposed method to the performance a random policy .",request
B129GzFxf_28,The paper is missing some highly relevant references .,evaluation
B129GzFxf_29,"First , how does the proposed method compare to hindsight experience replay ?",request
B129GzFxf_30,"[ 1 ] Second , learning from keyframes ( rather than demonstrations ) has been explored in the past [ 1 ] .",fact
B129GzFxf_31,"It would be preferable to use the standard terminology of "" keyframe "" .",request
B129GzFxf_32,[1] <CIT>,reference
B129GzFxf_33,[2] <CIT>,reference
B129GzFxf_34,"In summary , I think this paper has a number of promising ideas and experimental results ,",evaluation
B129GzFxf_35,"but given the significant issues in clarity and significance to real world problems , I do n't think that the current version of this paper is suitable for publication in ICLR .",evaluation
B129GzFxf_36,"More minor feedback on clarity and correctness : - Abstract : "" Deep RL algorithms have proven successful in a vast variety of domains "" -- This is an overstatement .",evaluation
B129GzFxf_37,- The introduction should be more clear with regard to the assumptions .,request
B129GzFxf_38,"In particular , it would be helpful to see discussion of requiring human-provided keyframes .",request
B129GzFxf_39,"As is , it is unclear what is meant by "" checkpoint scheme "" ,",evaluation
B129GzFxf_40,which is not commonly used terminology .,evaluation
B129GzFxf_41,"- "" This kind of spare reward , goal-oriented tasks are considered the most difficult challenges "" -- This is also an overstatement .",evaluation
B129GzFxf_42,Long-horizon tasks and high-dimensional observations are also very difficult .,evaluation
B129GzFxf_43,"Also , the sentence is not grammatically correct .",fact
B129GzFxf_44,"- "" That is , environment "" - > "" That is , the environment """,request
B129GzFxf_45,"- In the last paragraph of the intro , it would be helpful to more clearly state what the experiments can accomplish .",request
B129GzFxf_46,Can they handle raw pixel inputs ?,request
B129GzFxf_47,"- "" diverse domains "" - > "" diverse simulated domains """,request
B129GzFxf_48,"- "" a robotic grasping task "" - > "" a simulated robotic grasping task """,request
B129GzFxf_49,"- There are a number of issues and errors in citations , e.g. missing the year , including the first name , incorrect reference",fact
B129GzFxf_50,- Assumption 1 : <VAR> has not yet been defined .,fact
B129GzFxf_51,- The last two paragraphs of section 3.2 are very difficult to understand without reading the method yet,evaluation
B129GzFxf_52,"- "" conventional RL solver tend "" - > "" conventional RL tend "" ,",request
B129GzFxf_53,also should mention sparse reward in this sentence .,request
B129GzFxf_54,"- Algorithm 1 and Figure 1 are not referenced in the text anywhere , and should be",request
B129GzFxf_55,- The text in Figure 1 and Figure 3 is extremely small,request
B129GzFxf_56,- The text in Figure 3 is extremely small,request
H1caL6tXz_0,The paper proposes to use a regularizer for tensor completion problems that can be written in a similar fashion as the variational factorization formulation of the trace norm aka nuclear norm for matrices .,fact
H1caL6tXz_1,The paper introduces the regularizer,fact
H1caL6tXz_2,"with the nice argument that the gradient of the L3 norm to the power of 3rd will be easy to compute ,",fact
H1caL6tXz_3,but if we were raising the L2 norm to the 3rd power it would not be the case .,fact
H1caL6tXz_4,They mention that their argument can generalize from D = 3 to higher order tensors .,fact
H1caL6tXz_5,Authors mention the paper by Friedland and Lim that introduces this norm and provides first theoretical results on it .,fact
H1caL6tXz_6,Authors develop on the tensor equivalent of the matrix max norm,fact
H1caL6tXz_7,which is built with the motivation of bringing robustness to heavy nodes in the graph ( very popular content ) .,fact
H1caL6tXz_8,This is again straightforward on the technical side .,evaluation
H1caL6tXz_9,Empirical results are fine but do not show huge improvements compared to baselines,evaluation
H1caL6tXz_10,so I do not think this is a strong argument for accepting the paper .,evaluation
H1caL6tXz_11,"On the scalability , authors do not show that their approach is better suited than baselines .",fact
ry6owJ9lM_0,This paper introduces a simple extension to parallelize Hyperband .,fact
ry6owJ9lM_1,Points in favor of the paper : * Addresses an important problem,evaluation
ry6owJ9lM_2,"Points against : * Only 5-fold speedup by parallelization with 5 x 25 workers , and worse performance in the same budget than Google Vizier ( even though that treats the problem as a black box )",fact
ry6owJ9lM_3,* Limited methodological contribution/novelty,evaluation
ry6owJ9lM_4,The paper 's methodological contribution is quite limited :,evaluation
ry6owJ9lM_5,it amounts to a straight-forward parallelization of successive halving ( SHA ) .,fact
ry6owJ9lM_6,"Specifically , whenever a worker frees up , do a new run on it , at the highest rung possible while making sure to not run too many runs for too high rungs .",fact
ry6owJ9lM_7,"( I am pretty sure that is the idea , even though Algorithm 1 , which is supposed to give the details , appears to have a bug in Procedure get_job",fact
ry6owJ9lM_8,-- it would always either pick the highest rung or the lowest ! ),fact
ry6owJ9lM_9,"Empirically , the paper strangely does not actually evaluate a parallel version of Hyperband , but only evaluates the 5 parallel variants of SHA that Hyperband would run , each of them with all workers .",fact
ry6owJ9lM_10,"The experiments in Section 4.2 show that , using 25 workers , the best of these 5 variants obtains a 5-fold speedup over sequential Hyperband on CIFAR and an 8-fold speedup on SVHN .",fact
ry6owJ9lM_11,I am confused : the * best * of 5 SHA variants only achieves a 5-fold speedup using 25 workers ?,evaluation
ry6owJ9lM_12,"I.e. , parallel Hyperband , which would run the 5 SHA variants in parallel , would require 125 workers but only yield a 5-fold speedup ?",evaluation
ry6owJ9lM_13,"If I understand this correctly , I would clearly call this a negative result .",evaluation
ry6owJ9lM_14,"Likewise , for the large-scale experiment , a single run of Vizier actually yields as good performance as the best of the 5 SHA variants ,",fact
ry6owJ9lM_15,"and it is unknown beforehand which SHA variant works best -- in this example , actually Bracket 0 ( which is often the best ) stagnates .",fact
ry6owJ9lM_16,"Parallel Hyperband would run the 5 SHA variants in parallel ,",fact
ry6owJ9lM_17,so its performance at a budget of 10R with a total of 500 workers can be evaluated by taking the minimum of the 5 SHA variants at a budget of 2R .,fact
ry6owJ9lM_18,"This would obtain a perplexity of above 90 ,",fact
ry6owJ9lM_19,which is quite a bit worse than Vizier 's result of about 82 .,evaluation
ry6owJ9lM_20,"In general , the performance of parallel Hyperband can be computed by taking the minimum of the SHA variants and multiplying the time taken by 5 ;",fact
ry6owJ9lM_21,"this shows that at any time in the plot ( Figure 3 , left ) Vizier dominates parallel Hyperband .",fact
ry6owJ9lM_22,"Again , this is apparently a negative result .",evaluation
ry6owJ9lM_23,"( For Figure 3 , right , no results for Vizier are given yet . )",fact
ry6owJ9lM_24,"If I understand correctly , the experiment in Section 4.4 does not involve any run of Hyperband , but merely plots predictions of Qi et al. 's Paelo framework of how many models could be evaluated with a growing number of GPUs .",fact
ry6owJ9lM_25,"Therefore , all empirical results for parallel Hyperband reported in the paper appear to be negative .",fact
ry6owJ9lM_26,"This confuses me ,",evaluation
ry6owJ9lM_27,especially since the authors seem to take them as positive results .,fact
ry6owJ9lM_28,"Because the original Hyperband paper argued that Bayesian optimization does not parallelize as well as random search / Hyperband ,",fact
ry6owJ9lM_29,"and because Hyperband has been reported to work much better than Bayesian optimization on a single node ,",fact
ry6owJ9lM_30,I would have expected clear improvements of parallel Hyperband over parallel Bayesian optimization ( = Vizier in the authors ' setup ) .,evaluation
ry6owJ9lM_31,"However , this is not what I see in the results .",evaluation
ry6owJ9lM_32,Am I mistaken somewhere ?,non-arg
ry6owJ9lM_33,"If not , based on these negative results the paper does not seem to quite clear the bar for ICLR .",evaluation
ry6owJ9lM_34,"Details , in order of appearance in the paper : - Vizier : why did the authors only use Vizier 's default Bayesian optimization algorithm ?",evaluation
ry6owJ9lM_35,"The Vizier paper by Golovin et al ( 2017 ) states that for large budgets other optimizers often perform better , and the budget in the large scale experiments is as high as 5000 function evaluations .",fact
ry6owJ9lM_36,"Also , is n't there an automatic choice built into Vizier to pick the optimizer expected to be best ?",fact
ry6owJ9lM_37,I think using a suboptimal version of Vizier would be a problem for the experimental setup .,evaluation
ry6owJ9lM_38,- Algorithm 1 : this needs some improvement ; in particular fixing the bug I mentioned above .,request
ry6owJ9lM_39,- Section 3.1 : Li et al ( 2017 ) do not analyze any algorithm theoretically .,fact
ry6owJ9lM_40,They also do not discuss finite vs. infinite horizon .,fact
ry6owJ9lM_41,I believe the authors meant Li et al 's arXiv paper ( 2016 ) in both of these cases .,evaluation
ry6owJ9lM_42,"- Section 3.1 , point 2 : this is unclear to me , even though I know Hyperband very well .",evaluation
ry6owJ9lM_43,Can you please make this clearer ?,request
ry6owJ9lM_44,"- "" A complete theoretical treatment of asynchronous SHA is out of the scope of this paper """,quote
ry6owJ9lM_45,- > is some theoretical treatment in scope ?,request
ry6owJ9lM_46,"- Section 4.1 : It seems very useful to already recommend configurations in each rung of Hyperband ,",evaluation
ry6owJ9lM_47,and I am surprised that the methods section does not mention this .,evaluation
ry6owJ9lM_48,"From the text in this experiments section , it feels a little like that was always part of Hyperband ;",evaluation
ry6owJ9lM_49,"I did n't think it was ,",evaluation
ry6owJ9lM_50,"so I checked the original papers and blog posts ,",non-arg
ry6owJ9lM_51,"and both the ICLR 2017 and the arXiv 2016 paper state "" In fact , the first result returned by HYPERBAND after using a budget of 5R is often competitive with results returned by other searchers after using 50R . """,fact
ry6owJ9lM_52,"and Kevin Jamieson 's blog post on Hyperband ( <URL> ) explicitly states : "" While random and the Bayesian Optimization algorithms output their first recommendation after max_iter iterations , Hyperband does not output anything until about max_iter ( logeta ( max_iter ) +1 ) iterations [ ... ] """,fact
ry6owJ9lM_53,"Therefore , recommending after each rung seems to be a contribution of this paper ,",fact
ry6owJ9lM_54,and I think it would be nice to read about this in the methods section .,request
ry6owJ9lM_55,"- Experiment 1 ( SVM ) used dataset size as a budget , which is what Fabolas ( "" Fast Bayesian optimization on large datasets "" ) is designed for according to Klein et al ( 2017 ) .",fact
ry6owJ9lM_56,"On the other hand , Experiments ( 2 ) and ( 3 ) used the number of epochs as a budget , and Fabolas is not designed for that",fact
ry6owJ9lM_57,"( one would want to use a different kernel , for epochs , e.g. , like Freeze-Thaw Bayesian optimization ( FTBO ) by Swersky et al ( 2014 ) , instead of a kernel made for dataset sizes ) .",evaluation
ry6owJ9lM_58,"Therefore , it is not surprising that Fabolas does not work as well in those cases .",evaluation
ry6owJ9lM_59,The case of number of epochs as a budget would be the domain of FTBO .,fact
ry6owJ9lM_60,"I know that there is no reference implementation of FTBO ,",fact
ry6owJ9lM_61,"so I am not asking for a comparison , but the comparison against Fabolas is misleading for Experiments ( 2 ) and ( 3 ) .",evaluation
ry6owJ9lM_62,This does n't really change anything for the paper :,evaluation
ry6owJ9lM_63,the authors could still make the case that Fabolas has n't been designed for this case and that ( to the best of my knowledge ) there simply is n't an implementation of a BO algorithm that is .,fact
ry6owJ9lM_64,"Fabolas is arguably the closest thing ,",evaluation
ry6owJ9lM_65,"so the results could still be reported , just not as an apples-to-apples comparison ; probably best as "" Fabolas-like , with dataset size kernel "" in the figure .",request
ry6owJ9lM_66,The justification to not compare against Fabolas in the parallel regime is clearly valid .,evaluation
ry6owJ9lM_67,"- A clarification question : Section 4.4 does not report on any runs of actual neural networks , does it ?",non-arg
ry6owJ9lM_68,"And not on any runs of Hyperband , correct ?",non-arg
ry6owJ9lM_69,"Do I understand the reasoning correctly as pointing out that standard parallelization across multiple GPUs is not great , and that thus , in combination with parallel Hyperband , runs should be done mostly on one GPU only ?",non-arg
ry6owJ9lM_70,"How does this relate to the results in the cited paper "" Accurate , Large-batch SGD : Training ImageNet in 1 Hour "" ( <URL> ) ?",non-arg
ry6owJ9lM_71,"Quoting from its abstract : "" Using commodity hardware , our implementation achieves ∼ 90 % scaling efficiency when moving from 8 to 256 GPUs . """,quote
ry6owJ9lM_72,That seems like a very good utilization of parallel computing power ?,evaluation
ry6owJ9lM_73,- There is no conclusion / future work .,fact
SyOiDTtef_0,"The paper proposes an online distillation method , called co-distillation , where the two different models are trained to match the predictions of other model in addition to minimizing its own loss .",fact
SyOiDTtef_1,The proposed method is applied to two large-scale datasets,fact
SyOiDTtef_2,"and showed to perform better than other baselines such as label smoothing , and the standard ensemble .",fact
SyOiDTtef_3,The paper is clearly written and was easy to understand .,evaluation
SyOiDTtef_4,My major concern is the significance and originality of the proposed method .,evaluation
SyOiDTtef_5,"As written by the authors , the main contribution of the paper is to apply the codistillation method , which is pretty similar to Zhang et . al ( 2017 ) , at scale .",evaluation
SyOiDTtef_6,"But , because from Zhang 's method , I do n't see any significant difficulty in applying to large-scale problems ,",evaluation
SyOiDTtef_7,I 'm not sure that this can be a significant contribution .,evaluation
SyOiDTtef_8,"Rather , I think , it would have been better for the authors to apply the proposed methods to a smaller scale problems as well in order to explore more various aspects of the proposed methods including the effects of number of different models .",request
SyOiDTtef_9,"In this sense , it is also a limitation that the authors showing experiments where only two models are codistillated .",request
SyOiDTtef_10,"Usually , ensemble becomes stronger as the number of model increases .",evaluation
B1eq0Hqlz_0,The authors investigate a modified input layer that results in color invariant networks .,fact
B1eq0Hqlz_1,The proposed methods are evaluated on two car datasets .,fact
B1eq0Hqlz_2,"It is shown that certain color invariant "" input "" layers can improve accuracy for test-images from a different color distribution than the training images .",fact
B1eq0Hqlz_3,The proposed assumptions are not well motivated and seem arbitrary .,evaluation
B1eq0Hqlz_4,Why is using a permutation of each pixels ' color a good idea ?,non-arg
B1eq0Hqlz_5,The paper is very hard to read .,evaluation
B1eq0Hqlz_6,The message is unclear,evaluation
B1eq0Hqlz_7,"and the experiments to prove it are of very limited scope , i.e. one small dataset with the only experiment purportedly showing generalization to red cars .",evaluation
B1eq0Hqlz_8,Some examples of specific issues : - the abstract is almost incomprehensible and it is not clear what the contributions are,evaluation
B1eq0Hqlz_9,"- Some references to Figures are missing the figure number , eg . 3.2 first paragraph ,",fact
B1eq0Hqlz_10,"- It is not clear how many input channels the color invariant functions use , eg . p1 does it use only one channel and hence has fewer parameters ?",evaluation
B1eq0Hqlz_11,- are the training and testing sets all disjoint ( sec 4.3 ) ?,non-arg
B1eq0Hqlz_12,"- at random points figures are put in the appendix , even though they are described in the paper and seem to show key results ( eg "" tested on nored-test "" )",fact
B1eq0Hqlz_13,- Sec 4.6 : The explanation for why the accuracy drops for all models is not clear .,evaluation
B1eq0Hqlz_14,Is it because the total number of training images drops ? If that 's the case the whole experimental setup seems flawed .,evaluation
B1eq0Hqlz_15,"- Sec 4.6 : the authors refer to the "" order net "" beating the baseline ,",fact
B1eq0Hqlz_16,"however , from Fig 8 ( right most ) it appears as if all models beat the baseline .",fact
B1eq0Hqlz_17,In the conclusion they say that weighted order net beats the baseline on all three test sets w/o red cars in the training set .,fact
B1eq0Hqlz_18,Is that Fig 8 @ 0 % ?,non-arg
B1eq0Hqlz_19,"The baseline seems to be best performing on "" all cars "" and "" non-red cars """,fact
B1eq0Hqlz_20,In order to be at an appropriate level for any publication the experiments need to be much more general in scope .,request
Bk_WgJqgM_0,Authors proposed a neural network based machine translation method between two programming languages .,fact
Bk_WgJqgM_1,The model is based on both source/target syntax trees and performs an attentional encoder-decoder style network over the tree structure .,fact
Bk_WgJqgM_2,The new things in the paper are the task definition and using the tree-style network in both encoder and decoder .,evaluation
Bk_WgJqgM_3,"Although each structure of encoder/decoder/attention network is based on the application of some well-known components ,",evaluation
Bk_WgJqgM_4,"unfortunately , the paper pays much space to describe them .",evaluation
Bk_WgJqgM_5,"On the other hand , the whole model structure looks to be easily generalized to other tree-to-tree tasks and might have some potential to contribute this kind of problems .",evaluation
Bk_WgJqgM_6,"In experimental settings , there are many shortages of the description .",evaluation
Bk_WgJqgM_7,"First , it is unclear that what the linearization method of the syntax tree is ,",evaluation
Bk_WgJqgM_8,which could affect the final model accuracy .,fact
Bk_WgJqgM_9,"Second , it is also unclear what the method to generate train/dev/test data is .",evaluation
Bk_WgJqgM_10,Are those generated completely randomly ?,request
Bk_WgJqgM_11,"If so , there could be many meaningless ( e.g. , inexecutable ) programs in each dataset .",fact
Bk_WgJqgM_12,"What is the reasonableness of training such kind of data , or are they already avoided from the data ?",request
Bk_WgJqgM_13,"Third , the evaluation metrics "" token/program accuracy "" looks insufficient about measuring the correctness of the program",evaluation
Bk_WgJqgM_14,because it has sensitivity about meaningless differences between identifier names and some local coding styles .,fact
Bk_WgJqgM_15,Authors also said that CoffeeScript has a succinct syntax and Javascript has a verbose one without any agreement about what the syntax complexity is .,fact
Bk_WgJqgM_16,"Since any CoffeeScript programs can be compiled into the corresponding Javascript programs ,",fact
Bk_WgJqgM_17,"we should assume that CoffeeScript is the only subset of Javascript ( without physical difference of syntax ) ,",evaluation
Bk_WgJqgM_18,and this translation task may never capture the whole tendency of Javascript .,fact
Bk_WgJqgM_19,"In addition , authors had generated the source CoffeeScript codes ,",fact
Bk_WgJqgM_20,"which seems that this task is only one of "" synthetic "" task and no longer capture any real world 's programs .",evaluation
Bk_WgJqgM_21,"If authors were interested in the tendency of real program translation task , they should arrange the experiment by collecting parallel corpora between some unrelated programming languages using resources in the real world .",request
Bk_WgJqgM_22,Global attention mechanism looks somewhat not suitable for this task .,evaluation
Bk_WgJqgM_23,"Probably we can suppress the range of each attention by introducing some prior knowledge about syntax trees ( e.g. , only paying attention to the descendants in a specific subtree ) .",evaluation
Bk_WgJqgM_24,"Suggestion : After capturing the motivation of the task , I suspect that the traditional tree-to-tree ( also X-to-tree ) "" statistical "" machine translation methods still can also work correctly in this task .",evaluation
Bk_WgJqgM_25,"The traditional methods are basically based on the rule matching , which constructs a target tree by selecting source/target subtree pairs and arranging them according to the actual connections between each subtree in the source tree .",fact
Bk_WgJqgM_26,"This behavior might be suitable to transform syntax trees while keeping their whole structure , and also be able to treat the OOV ( e.g. , identifier names ) problem by a trivial modification .",fact
Bk_WgJqgM_27,"Although it is not necessary , it would like to apply those methods to this task as another baseline if authors are interested in .",request
B1oFM1FeG_0,"This paper presents , and analyzes , a method for learning word relationships based on co-occurrence .",fact
B1oFM1FeG_1,"In the method , relationships between pairs of words ( A , B ) are represented by the terms that tend to occur around co-mentions of A and B in text .",fact
B1oFM1FeG_2,"The paper shows the start of some interesting ideas ,",evaluation
B1oFM1FeG_3,but needs revisions and much more extensive experiments .,request
B1oFM1FeG_4,"On the plus side , the method proposed here does perform relatively well ( Table 1 ) and probably merits further investigation .",evaluation
B1oFM1FeG_5,"The experiments in Table 1 can only be considered preliminary , however .",evaluation
B1oFM1FeG_6,They only evaluate over a small number of relationships ( three ),fact
B1oFM1FeG_7,-- looking at 20 or so different relationships would greatly improve confidence in the conclusions .,request
B1oFM1FeG_8,Beyond Table 1 the paper makes a number of claims that are not supported or weakly supported ( the paper uses only a handful of examples as evidence ) .,evaluation
B1oFM1FeG_9,"An attempt to explain what Word2Vec is doing should be made with careful experiments over many relations and hundreds of examples ,",request
B1oFM1FeG_10,whereas this paper presents only a handful of examples for most of its claims .,evaluation
B1oFM1FeG_11,"Further , whether the behavior of the proposed algorithm actually reflects what word2vec is doing is left as a significant open question .",evaluation
B1oFM1FeG_12,"I appreciate the clarity of Assumption 1 and Proposition 1 ,",evaluation
B1oFM1FeG_13,but ultimately this formalism is not used,fact
B1oFM1FeG_14,"and because Assumption 1 about which nouns are "" semantically related "" to which other nouns attempts to trivialize a complex notion ( semantics ) and is clearly way too strong",evaluation
B1oFM1FeG_15,-- the paper would be better off without it .,request
B1oFM1FeG_16,Also Assumption 1 does not actually claim what the text says it claims,fact
B1oFM1FeG_17,"( the text says words outside the window are * not * semantically related , but the assumption does not actually say this )",fact
B1oFM1FeG_18,and furthermore is soon discarded and only the frequency of noun occurrences around co-mentions is used .,fact
B1oFM1FeG_19,I think the description of the algorithm could be retained without including Assumption 1 .,request
B1oFM1FeG_20,minor : References to numbered algorithms or assumptions should be capitalized in the text .,request
B1oFM1FeG_21,"what the introduction means about the "" dynamics "" of the vector equation is a little unclear",evaluation
B1oFM1FeG_22,"A submission should n't have acknowledgments , and in particular with names that undermine anonymity",request
B1oFM1FeG_23,"MLE has a particular technical meaning that is not utilized here ,",fact
B1oFM1FeG_24,"I would just refer to the most frequent words as "" most related nouns "" or similar",request
B1oFM1FeG_25,"In Table 1 , are the "" same dataset "" results with w2v for the nouns-only corpus , or with all the other words ?",request
B1oFM1FeG_26,The argument made assuming a perfect Zipf distribution ( with exponent equal to one ) should be made with data .,request
B1oFM1FeG_27,will likely by observed - > will likely be observed,request
B1oFM1FeG_28,"lions : dolphins probably ends up that way because of "" sea lions """,request
B1oFM1FeG_29,Table 4 caption : frequencies - > currencies,request
B1oFM1FeG_30,Table 2 -- claim is that improvements from <EQN> to <EQN> are ' nominal ' but they look non-negligible to me,evaluation
B1oFM1FeG_31,I did not understand how POS lying in the same subspace means that <VAR> has to be in the span of Vecs A-C .,evaluation
SylxFWcgG_0,This paper extends an existing thread of neural computation research focused on learning resuable subprocedures ( or options in RL-speak ) .,fact
SylxFWcgG_1,"Instead of simply input and output examples , as in most of the work in neural computation , they follow in the vein of the Neural Programmer-Interpreter ( Reed and de Freitas , 2016 ) and Li et . al. , 2017 ,",fact
SylxFWcgG_2,"where the supervision contains the full sequence of elementary actions in the domain for all samples , and some samples also contain the hierarchy of subprocedure calls .",fact
SylxFWcgG_3,The main focus of their work is learning from fewer fully annotated samples than prior work .,fact
SylxFWcgG_4,They introduce two new ideas in order to enable this :,fact
SylxFWcgG_5,1 . They limit the memory state of each level in the program heirarchy to simply a counter indicating the number of elementary actions/subprocedure calls taken so far ( rather than a full RNN embedded hidden/cell state as in prior work ) .,fact
SylxFWcgG_6,They also limit the subprocedures such that they do not accept any arguments .,fact
SylxFWcgG_7,"2 . By considering this very limited set of possible hidden states , they can compute the gradients using a dynamic program that seems to be more accurate than the approximate dynamic program used in Li et . al. , 2017 .",fact
SylxFWcgG_8,"The main limitation of the work is this extremely limited memory state , and the lack of arguments .",evaluation
SylxFWcgG_9,"Without arguments , everything that parameterizes the subprocedures must be in the visible world state .",fact
SylxFWcgG_10,"In both of their domains , this is true ,",fact
SylxFWcgG_11,but this places a significant limitation on the algorithms which can be modeled with this technique .,evaluation
SylxFWcgG_12,"Furthermore , the limited memory state means that the only way a subprocedure can remember anything about the current observation is to call a different subprocedure .",fact
SylxFWcgG_13,"Again , their two evalation tasks fit into this paradigm ,",fact
SylxFWcgG_14,but this places very significant limitations on the set of applicable domains .,evaluation
SylxFWcgG_15,I would have like to see more discussion on how constraining these limitations would be in practice .,request
SylxFWcgG_16,"For example , it seems it would be impossible for this architecture to perform the Nanocraft task if the parameters of the task ( width , height , etc. ) were only provided in the first observation , rather than every observation .",fact
SylxFWcgG_17,None-the-less I think this work is an important step in our understanding of the learning dynamics for neural programs .,evaluation
SylxFWcgG_18,"In particular , while the RNN hidden state memory used by the prior work enables the learning of more complicted programs * in theory * , this has not been shown in practice .",evaluation
SylxFWcgG_19,"So , it 's possible that all the prior work is doing is learning to approixmate a much simpler architecture of this form .",evaluation
SylxFWcgG_20,"Specifically , I think this work can act as a great base-line by forcing future work to focus on domains which can not be easily solved by a simpler architecture of this form .",evaluation
SylxFWcgG_21,"This limitation will also force the community to think about which tasks require a more complicated form of memory , and which can be solved with a very simple memory of this form .",evaluation
SylxFWcgG_22,I also have the following additional concerns about the paper : 1 . I found the current explanation of the algorithm to be very difficult to understand .,evaluation
SylxFWcgG_23,"It 's extremely difficult to understand the core method without reading the appendix ,",evaluation
SylxFWcgG_24,and even with the appendix I found the explanation of the level-by-level decomposition to be too terse .,evaluation
SylxFWcgG_25,2 . It 's not clear how their gradient approximation compares to the technique used by Li et . al .,evaluation
SylxFWcgG_26,"They obviously get better results on the addition and Nanocraft domains ,",fact
SylxFWcgG_27,but I would have liked a more clear explanation and/or some experiments providing insights into what enables these improvements ( or at least an admission by the authors that they do n't really understand what enabled the performance improvements ) .,request
BknXbsdxG_0,"In this paper , an number of very strong ( even extraordinary ) claims are made :",evaluation
BknXbsdxG_1,"* The abstract promises "" a framework to understand the unprecedented performance and robustness of deep neural networks using field theory . """,fact
BknXbsdxG_2,"* Page 8 states that this is "" This is a first attempt to describe a neural network with a scalar quantum field theory . """,quote
BknXbsdxG_3,"* Page 2 promises the use of the "" Goldstone theorem "" ( no less ) to understand phase transition in deep learning",fact
BknXbsdxG_4,"* It also claim that many "" seemingly different experimental results can be explained by the presence of these zero eigenvalue weights . """,quote
BknXbsdxG_5,"* Three important results are stated as "" theorem "" , with a statement like "" Deep feedforward networks learn by breaking symmetries "" proven in 5 lines , with no formal mathematics .",fact
BknXbsdxG_6,"These are extraordinary claims ,",evaluation
BknXbsdxG_7,"but when reaching page 5 , one sees that the basis of these claims seems to be the Lagrangian of a simple phi-4 theory ,",evaluation
BknXbsdxG_8,"and Fig. 1 shows the standard behaviour of the so-called mexican hat in physics , the basis of the second-order transition .",fact
BknXbsdxG_9,"Given physicists have been working on neural network for more than three or four decades ,",fact
BknXbsdxG_10,I am surprise that this would enough to solve all these problems !,evaluation
BknXbsdxG_11,"I tried to understand these many results ,",non-arg
BknXbsdxG_12,but I am afraid I can not really understand or see them .,evaluation
BknXbsdxG_13,"In many case , the explanation seems to be a vague analogy .",evaluation
BknXbsdxG_14,"These are not without interest ,",evaluation
BknXbsdxG_15,"and maybe there is indeed something deep in this paper , but it is so far hidden by the hype .",evaluation
BknXbsdxG_16,"Still , I fail to see how the fact that phase transitions and negative direction in the landscape is a new phenomena , and how it explains all the stated phenomenology .",evaluation
BknXbsdxG_17,"Beside , there are quite a lot of things known about the landscape of these problems",evaluation
BknXbsdxG_18,"Maybe I am indeed missing something ,",non-arg
BknXbsdxG_19,but i clearly suspect the authors are simply overselling physics results .,evaluation
BknXbsdxG_20,"I have been wrong many times ,",non-arg
BknXbsdxG_21,"but I beleive that the authors should probably precise their claim , and clarify the relation between their results and both the physics AND statistics litterature , or better , with the theoretical physics litterature applied to learning , which is --- astonishing -- absent in the paper .",evaluation
BknXbsdxG_22,About the content : The main problem for me is that the whole construction using field theory seems to be used to advocate for the appearence of a phase transition in neural nets and in learning .,evaluation
BknXbsdxG_23,This rises three comments : ( 1 ) So we really need to use quantum field theory for this ?,non-arg
BknXbsdxG_24,I do not see what should be quantum here,evaluation
BknXbsdxG_25,"( despite the very vague remarks page 12 "" WHY QUANTUM FIELD THEORY ? "" )",non-arg
BknXbsdxG_26,( 2 ) This is not new .,fact
BknXbsdxG_27,"Phase transitions in learning in neural nets are being discussed since aboutn 40 years , see for instance all the pionnering work of Sompolinky et al .",fact
BknXbsdxG_28,one can see for instance the nice review in <URL>,reference
BknXbsdxG_29,"In non aprticular order , phase transition and symmetry breaking are discussed in * "" Statistical mechanics of learning from examples "" , Phys . Rev. A 45 , 6056 – Published 1 April 1992",reference
BknXbsdxG_30,"* "" The statistical mechanics of learning a rule "" , Rev. Mod . Phys . 65 , 499 – Published 1 April 1993",reference
BknXbsdxG_31,* Phase transitions in the generalization behaviour of multilayer neural networks,reference
BknXbsdxG_32,<URL>,reference
BknXbsdxG_33,"* Note that some of these results are now rigourous ,",fact
BknXbsdxG_34,"as shown in "" Phase Transitions , Optimal Errors and Optimality of Message-Passing in Generalized Linear Models "" , <URL>",reference
BknXbsdxG_35,"* The landscape of these problems has been studied quite extensivly ,",fact
BknXbsdxG_36,"see for instance "" Identifying and attacking the saddle point problem in high-dimensional non-convex optimization "" , <URL>",reference
BknXbsdxG_37,( 3 ) There is nothing particular about deep neural net and neural nets about this .,evaluation
BknXbsdxG_38,"Negative direction in the Hessian in learning problems appears in matrix and tensor factorizaion , where phase transition are well understood ( even rigorously , see for instance , <URL> ) or in problems such as unsupervised learning , as e.g. :",fact
BknXbsdxG_39,<URL>,reference
BknXbsdxG_40,<URL>,reference
BknXbsdxG_41,"Here are additional comments : PAGE 1 : * "" It has been discovered that the training process ceases when it goes through an information bottleneck ( ShwartzZiv & Tishby , 2017 ) "" .",quote
BknXbsdxG_42,"While this paper indeed make a nice suggestion , I would not call it a discovery yet as this has never been shown on a large network .",evaluation
BknXbsdxG_43,"Beside , another paper in the conference is claiming exacly the opposite ,",fact
BknXbsdxG_44,"see : "" On the Information Bottleneck Theory of Deep Learning "" .",reference
BknXbsdxG_45,This is still subject of discussion .,evaluation
BknXbsdxG_46,"* "" In statistical terms , a quantum theory describes errors from the mean of random variables . """,quote
BknXbsdxG_47,"Last time I studied quantum theory , it was a theory that aim to explain the physical behaviours at the molecular , atomic and sub-atomic levels , usinge either on the wave function ( Schrodinger ) or the Matrix operatir formalism ( Hesienbger ) ( or if you want , the path integral formalism of Feynman ) .",fact
BknXbsdxG_48,It is certainly NOT a theory that describes errors from the mean of random variables .,evaluation
BknXbsdxG_49,"This is , i beleive , the field of "" statistics "" or "" probability "" for correlated variables .",evaluation
BknXbsdxG_50,"It is certianly used in physics , and heavily both in statistical physics and in quantum thoery ,",fact
BknXbsdxG_51,but this is not what the theory is about in the first place .,fact
BknXbsdxG_52,"Beside , there is little quantum in this paper ,",evaluation
BknXbsdxG_53,I think most of what the authors say apply to a statistical field theory,evaluation
BknXbsdxG_54,( <URL> ),reference
BknXbsdxG_55,"* "" In the limit of a continuous sample space , the quantum theory becomes a quantum field theory . """,quote
BknXbsdxG_56,"Again , what is quantum about all this ?",non-arg
BknXbsdxG_57,"This true for a field theory , as well for continous theories of , say , mechanics , fracture , etc. . .",fact
BknXbsdxG_58,"PAGE 2 : * "" Using a scalar field theory we show that a phase transition must exist towards the end of training based on empirical results . """,quote
BknXbsdxG_59,So it is a scalar classical field theory after all .,fact
BknXbsdxG_60,This sounds a little bit less impressive that a quantum field theory .,evaluation
BknXbsdxG_61,"Note that the fact that phase transition arises in learning , and in a statistical theory applied to any learning process , is an old topic , with a classical litterature .",fact
BknXbsdxG_62,"The authors might be interested by the review "" The statistical mechanics of learning a rule "" , Rev. Mod . Phys . 65 , 499 – Published 1 April 1993",evaluation
BknXbsdxG_63,"PAGE 8 : * "" In this work we solved one of the most puzzling mysteries of deep learning by showing that deep neural networks undergo spontaneous symmetry breaking . """,quote
BknXbsdxG_64,I am afraid I fail to see what is so mysterious about this nor what the authors showed about it .,evaluation
BknXbsdxG_65,"In any case , gradient descent break symmetry spontaneously in many systems , including phi-4 , the Ising model or ( in learning problems ) the community detection problem",fact
BknXbsdxG_66,( see eg <URL> ) .,reference
BknXbsdxG_67,I am afraid I miss what is new there ...,evaluation
BknXbsdxG_68,"* "" This is a first attempt to describe a neural network with a scalar quantum field theory . """,quote
BknXbsdxG_69,"Given there seems to be little quantum in the paper ,",evaluation
BknXbsdxG_70,I fail to see the relevance of the statement .,evaluation
BknXbsdxG_71,"Secondly , I beleive that field theory has been used , many times and in greater lenght , both for statistical and dynamical problems in neural nets , see eg .",evaluation
BknXbsdxG_72,* <URL>,reference
BknXbsdxG_73,* <URL>,reference
BknXbsdxG_74,* <URL>,reference
BknXbsdxG_75,* <URL>,reference
BknXbsdxG_76,* <URL>,reference
rJ9BTHFez_0,Summary ******* The paper provides a collection of existing results in statistics .,fact
rJ9BTHFez_1,"Comments ******** Page 1 : references to Q-learning and Policy-gradients look awkwardly recent ,",evaluation
rJ9BTHFez_2,given that these have been around for several decades .,fact
rJ9BTHFez_3,I dont get what is the novelty in this paper .,evaluation
rJ9BTHFez_4,There is no doubt that all the tools that are detailed here are extremely useful and powerful results in mathematical statistics .,evaluation
rJ9BTHFez_5,But they are all known .,fact
rJ9BTHFez_6,"The Gibbs variational principle is folklore ,",evaluation
rJ9BTHFez_7,"Proposition 1,2 are available in all good text books on the topic ,",fact
rJ9BTHFez_8,and Proposition 4 is nothing but a transportation Lemma .,fact
rJ9BTHFez_9,"Now , Proposition 3 is about soft-Bellman operators .",fact
rJ9BTHFez_10,This perhaps is less standard,evaluation
rJ9BTHFez_11,because contraction property of soft-Bellman operator in infinite norm is more recent than for Bellman operators .,fact
rJ9BTHFez_12,"But as mentioned by the authors , this is not new either .",fact
rJ9BTHFez_13,"Also I do n't really see the point of providing the proofs of these results in the main material , and not for instance in appendix ,",evaluation
rJ9BTHFez_14,as there is no novelty either in the proof techniques .,evaluation
rJ9BTHFez_15,"I do n't get the sentence "" we have restricted so far the proof in the bandit setting "" :",evaluation
rJ9BTHFez_16,bandits are not even mentioned earlier .,fact
rJ9BTHFez_17,Decision ******** I am sorry but unless I missed something ( that then should be clarified ) this seems to be an empty paper : Strong reject .,evaluation
BkSq8vBxG_0,"The authors report a number of experiments using off-the-shelf sentence embedding methods for performing extractive summarisation , using a number of simple methods for choosing the extracted sentences .",fact
BkSq8vBxG_1,"Unfortunately the contribution is too minor , and the work too incremental , to be worthy of a place at a top-tier international conference such as ICLR .",evaluation
BkSq8vBxG_2,The overall presentation is also below the required standard .,evaluation
BkSq8vBxG_3,"The work would be better suited for a focused summarisation workshop ,",evaluation
BkSq8vBxG_4,where there would be more interest from the participants .,evaluation
BkSq8vBxG_5,Some of the statements motivating the work are questionable .,evaluation
BkSq8vBxG_6,"I do n't know if sentence vectors * in particular * have been especially successful in recent NLP ( unless we count neural MT with attention as using "" sentence vectors "" ) .",evaluation
BkSq8vBxG_7,"It 's also not the case that the sentence reordering and text simplification problems have been solved , as is suggested on p. 2 .",fact
BkSq8vBxG_8,The most effective method is a simple greedy technique .,evaluation
BkSq8vBxG_9,"I 'm not sure I 'd describe this as being "" based on fundamental principles of vector semantics "" ( p. 4 ) .",evaluation
BkSq8vBxG_10,The citations often have the authors mentioned twice .,fact
BkSq8vBxG_11,"The reference to "" making or breaking applications "" in the conclusion strikes me as premature to say the least .",evaluation
Byw1O6Fgz_0,"The paper is interesting ,",evaluation
Byw1O6Fgz_1,"but needs more work ,",request
Byw1O6Fgz_2,and should provide clear and fair comparisons .,request
Byw1O6Fgz_3,"Per se , the model is incrementally new ,",evaluation
Byw1O6Fgz_4,"but it is not clear what the strengths are ,",evaluation
Byw1O6Fgz_5,and the presentations needs to be done more carefully .,request
Byw1O6Fgz_6,"In detail : - please fix several typos throughout the manuscript , and have a native speaker ( and preferably an ASR expert ) proofread the paper",request
Byw1O6Fgz_7,"Introduction - please define HMM/GMM model ( and other abbreviations that will be introduced later ) ,",request
Byw1O6Fgz_8,"it can not be assumed that the reader is familiar with all of them ( "" ASG "" is used before it is defined , ... )",evaluation
Byw1O6Fgz_9,"- The standard units that most ASR systems use can be called "" senones "" ,",fact
Byw1O6Fgz_10,"and they are context dependent sub-phonetic units ( see <URL> ) , not phonetic states .",fact
Byw1O6Fgz_11,Also the units that generate the alignment and the units that are trained on an alignment can be different,fact
Byw1O6Fgz_12,( I can use a system with 10000 states to write alignments for a system with 3000 states ),fact
Byw1O6Fgz_13,- this needs to be corrected .,request
Byw1O6Fgz_14,"- When introducing CNNs , please also cite Waibel and TDNNs",request
Byw1O6Fgz_15,"- they are * the same * as 1-d CNNs , and predate them .",fact
Byw1O6Fgz_16,They have been extended to 2-d later on ( Spatio-temporal TDNNs ),fact
Byw1O6Fgz_17,"- The most influential deep learning paper here might be Seide , Li , Yu Interspeech 2011 on CD-DNN-HMMs , rather than overview articles",evaluation
Byw1O6Fgz_18,"- Many papers get rid of the HMM pipeline ,",evaluation
Byw1O6Fgz_19,"I would add <URL> , which predates Deep Speech",request
Byw1O6Fgz_20,"- What is a "" sequence-level variant of CTC "" ?",non-arg
Byw1O6Fgz_21,CTC is a sequence training criterion,fact
Byw1O6Fgz_22,"- The reason that Deep Speech 2 is better on noisy test sets is not only the fact they trained on more data , but they also trained on "" noisy "" ( matched ) data",fact
Byw1O6Fgz_23,- how is this an end-to-end approach if you are using an n-gram language model for decoding ?,fact
Byw1O6Fgz_24,Architecture - MFSC are log Filterbanks ...,fact
Byw1O6Fgz_25,- 1D CNNs would be TDNNs,fact
Byw1O6Fgz_26,"- Figure 2 : can you plot the various transition types ( normalized , un-normalized , ... ) in the plots ?",request
Byw1O6Fgz_27,"not sure if it would help , but it might",evaluation
Byw1O6Fgz_28,- Maybe provide a reference for HMM/GMM and EM ( forward backward training ),request
Byw1O6Fgz_29,"- MMI was also widely used in HMM/GMM systems , not just NN systems",evaluation
Byw1O6Fgz_30,"- the "" blank "" states do * not * model "" garbage "" frames ,",fact
Byw1O6Fgz_31,"if one wants to interpret them , they might be said to model "" non-stationary "" frames between CTC "" peaks "" ,",fact
Byw1O6Fgz_32,"but these are different from silence , garbage , noise , ...",fact
Byw1O6Fgz_33,- what is the relationship of the presented ASG criterion to MMI ?,request
Byw1O6Fgz_34,the form of equation ( 3 ) looks like an MMI criterion to me ?,fact
Byw1O6Fgz_35,"Experiments - Many of the previous comments still hold ,",evaluation
Byw1O6Fgz_36,please proofread,request
Byw1O6Fgz_37,"- you say there is no "" complexity "" incrase when using "" logadd """,fact
Byw1O6Fgz_38,- how do you measure this ?,request
Byw1O6Fgz_39,number of operations ?,request
Byw1O6Fgz_40,"is there an implementation of "" logadd "" that is ( absolutely ) as fast as "" add "" ?",request
Byw1O6Fgz_41,- There is discussion as to what i-vectors model ( speaker or environment information ),fact
Byw1O6Fgz_42,"- I would leave out this discussion entirely here ,",request
Byw1O6Fgz_43,"it is enough to mention that other systems use adaptation , and maybe re-run an unadapted baselien for comparsion",evaluation
Byw1O6Fgz_44,"- There are techniques for incremental adaptation and a constrained MLLR ( feature adaptation ) approaches that are very eficient , if one wnats to get into this",fact
Byw1O6Fgz_45,- it may also be interesting to discuss the role of the language model to see which factors influence system performance,request
Byw1O6Fgz_46,"- some of the other papers might use data augmentation , which would increase noise robustness",evaluation
Byw1O6Fgz_47,"( did not check , but this might explain some of the results in table 4 )",evaluation
Byw1O6Fgz_48,- I am confused by the references in the caption of Table 3,evaluation
Byw1O6Fgz_49,- surely the Waibel reference is meant to be for TDNNs,fact
Byw1O6Fgz_50,"( and should appear earlier in the paper ) ,",request
Byw1O6Fgz_51,while p-norm came later,fact
Byw1O6Fgz_52,"( Povey used it first for ASR , I think )",fact
Byw1O6Fgz_53,and is related to Maxout,evaluation
Byw1O6Fgz_54,- can you also compare the training times ?,request
Byw1O6Fgz_55,Conculsion - can you show how your approach is not so computationally expensive as RNN based approaches ?,request
Byw1O6Fgz_56,either in terms of FLOPS or measured times,request
BkC-HgcxG_0,"In this paper , the authors present an analysis of SGD within an SDE framework .",fact
BkC-HgcxG_1,The ideas and the presented results are interesting and are clearly of interest to the deep learning community .,evaluation
BkC-HgcxG_2,The paper is well-written overall .,evaluation
BkC-HgcxG_3,"However , the paper has important problems .",evaluation
BkC-HgcxG_4,1 ) The analysis is widely based on the recent paper by Mandt et al .,evaluation
BkC-HgcxG_5,"While being an interesting work on its own , the assumptions made in that paper are very strict and not very realistic .",evaluation
BkC-HgcxG_6,"For instance , the assumption that the stochastic gradient noise being Gaussian is very restrictive and trying to justify it just by the usual CLT is not convincing especially when the parameter space is extremely large ,",evaluation
BkC-HgcxG_7,the setting that is considered in the paper .,fact
BkC-HgcxG_8,2 ) There is a mistake in the proof Theorem 1 .,fact
BkC-HgcxG_9,"Even with the assumption that the gradient of sigma is bounded , eq 20 can not be justified and the equality can only be "" approximately equal to "" .",fact
BkC-HgcxG_10,The result will only hold if sigma does not depend on theta .,fact
BkC-HgcxG_11,"However , letting sigma depend on theta is the only difference from Mandt et al .",fact
BkC-HgcxG_12,"On the other hand , with constant sigma the result is very trivial and can be found in any text book on SDEs ( showing the Gibbs distribution ) .",evaluation
BkC-HgcxG_13,"Therefore , presenting it as a new result is misleading .",evaluation
BkC-HgcxG_14,"3 ) Even if the sigma is taken constant and theorem 1 is corrected , I do n't think theorem 2 is conclusive .",evaluation
BkC-HgcxG_15,"Theorem 2 basically assumes that the distribution is locally a proper Gaussian ( it is stated as locally convex , however it is taken as quadratic )",fact
BkC-HgcxG_16,"and the result just boils down to computing some probability under a Gaussian distribution ,",fact
BkC-HgcxG_17,which is still quite trivial .,evaluation
BkC-HgcxG_18,"Apart from this assumption not being very realistic ,",evaluation
BkC-HgcxG_19,"the result does not justify the claims on "" the probability of ending in a certain minimum """,fact
BkC-HgcxG_20,-- which is on the other hand a vague statement .,evaluation
BkC-HgcxG_21,"First of all "" ending in "" a certain area depends on many different factors , such as the structure of the distribution , the initial point , the distance between the modes etc .",fact
BkC-HgcxG_22,Also it is not very surprising that the inverse image of a wider Gaussian density is larger than of a pointy one .,evaluation
BkC-HgcxG_23,This again does not justify the claims .,fact
BkC-HgcxG_24,"For instance consider a GMM with two components , where the means of the individual components are close to each other , but one component having a very large variance and a smaller weight , and the other one having a lower variance and higher weight .",fact
BkC-HgcxG_25,"With authors ' claim , the algorithm should spend more time on the wider one ,",fact
BkC-HgcxG_26,however it is evident that this will not be the case .,fact
BkC-HgcxG_27,4 ) There is a conceptual mistake that the authors assume that SGD will attain the exact stationary distribution even when the SDE is simulated by the fixed step-size Euler integrator .,fact
BkC-HgcxG_28,"As soon as one uses eta > 0 the algorithm will never attain the stationary distribution of the continuous-time process , but will attain a stationary distribution that is close to the ideal one ( of course with several smoothness , growth assumptions ) .",fact
BkC-HgcxG_29,The error between the ideal distribution and the empirical distribution will be usually O ( eta ) depending on the assumption,fact
BkC-HgcxG_30,and therefore changing eta will result in a different distribution than the ideal one .,fact
BkC-HgcxG_31,With this in mind the stationary distributions for ( eta/S ) and ( 2eta/2S ) will be clearly different .,fact
BkC-HgcxG_32,The experiments are very interesting and I do not underestimate their value .,evaluation
BkC-HgcxG_33,"However , the current analysis unfortunately does not properly explain the rather strong claims of the authors , which is supposed to be the main contribution of this paper .",evaluation
rJBLYC--f_0,The paper proposes a novel approach on estimating the parameters of Mean field games ( MFG ) .,fact
rJBLYC--f_1,The key of the method is a reduction of the unknown parameter MFG to an unknown parameter Markov Decision Process ( MDP ) .,fact
rJBLYC--f_2,This is an important class of models,evaluation
rJBLYC--f_3,and I recommend the acceptance of the paper .,evaluation
rJBLYC--f_4,I think that the general discussion about the collective behavior application should be more carefully presented,request
rJBLYC--f_5,and some better examples of applications should be easy to provide .,request
rJBLYC--f_6,In addition the authors may want to enrich their literature review,request
rJBLYC--f_7,"and give references to alternative work on unknown MDP estimation methods cf. [1] , [2] below .",request
rJBLYC--f_8,[1] <CIT>,reference
rJBLYC--f_9,[2] <CIT>,reference
S1c4VEXWz_0,This paper provides an overview of the Deep Voice 3 text-to-speech system .,fact
S1c4VEXWz_1,It describes the system in a fair amount of detail and discusses some trade-offs w.r.t. audio quality and computational constraints .,evaluation
S1c4VEXWz_2,Some experimental validation of certain architectural choices is also provided .,fact
S1c4VEXWz_3,My main concern with this work is that it reads more like a tech report :,evaluation
S1c4VEXWz_4,"it describes the workings and design choices behind one particular system in great detail ,",evaluation
S1c4VEXWz_5,"but often these choices are simply stated as fact and not really motivated , or compared to alternatives .",evaluation
S1c4VEXWz_6,"This makes it difficult to tell which of these aspects are crucial to get good performance , and which are just arbitrary choices that happen to work okay .",evaluation
S1c4VEXWz_7,"As this system was clearly developed with actual deployment in mind ( and not purely as an academic pursuit ) ,",evaluation
S1c4VEXWz_8,all of these choices must have been well-deliberated .,request
S1c4VEXWz_9,It is unfortunate that the paper does n't demonstrate this .,evaluation
S1c4VEXWz_10,I think this makes the work less interesting overall to an ICLR audience .,evaluation
S1c4VEXWz_11,"That said , it is perhaps useful to get some insight into what types of models are actually used in practice .",request
S1c4VEXWz_12,"An exception to this is the comparison of "" converters "" , model components that convert the model 's internal representation of speech into waveforms .",fact
S1c4VEXWz_13,This comparison is particularly interesting,evaluation
S1c4VEXWz_14,"because some of the results are remarkable , i.e. Griffin-Lim spectrogram inversion and the WORLD vocoder achieving very similar MOS scores in some cases ( Table 2 ) .",evaluation
S1c4VEXWz_15,I wish there would be more of that kind of thing in the paper .,evaluation
S1c4VEXWz_16,The comparison of attention mechanisms is also useful .,evaluation
S1c4VEXWz_17,"I 'm on the fence as I think it is nice to get some insight into a practical pipeline which benefits from many current trends in deep learning research ( autoregressive models , monotonic attention , ... ) ,",evaluation
S1c4VEXWz_18,but I also feel that the paper is a bit meager when it comes to motivating all the architectural aspects .,evaluation
S1c4VEXWz_19,I think the paper is well written,evaluation
S1c4VEXWz_20,so I 've tentatively recommended acceptance .,evaluation
S1c4VEXWz_21,"Other comments : - The separation of the "" decoder "" and "" converter "" stage is not entirely clear to me .",evaluation
S1c4VEXWz_22,"It seems that the decoder is trained to predict spectrograms autoregressively , but its final layer is then discarded and its hidden representation is then used as input to the converter stage instead ?",fact
S1c4VEXWz_23,"The motivation for doing this is unclear to me ,",evaluation
S1c4VEXWz_24,"surely it would be better to train everything end-to-end , including the converter ?",request
S1c4VEXWz_25,"This seems like an unnecessary detour ,",evaluation
S1c4VEXWz_26,what 's the reasoning behind this ?,request
S1c4VEXWz_27,"- At the bottom of page 2 it is said that "" the whole model is trained end-to-end , excluding the vocoder "" ,",fact
S1c4VEXWz_28,which I think is an unfortunate turn of phrase .,evaluation
S1c4VEXWz_29,"It 's either end-to-end , or it is n't .",fact
S1c4VEXWz_30,"- In Section 3.3 , the point of mixing of h_k and h_e is unclear to me .",evaluation
S1c4VEXWz_31,Why is this done ?,request
S1c4VEXWz_32,- The gated linear unit in Figure 2a shows that speaker embedding information is only injected in the linear part .,fact
S1c4VEXWz_33,Has this been experimentally validated to work better than simpler mechanisms such as adding conditioning-dependent biases/gains ?,request
S1c4VEXWz_34,"- When the decoder is trained to do autoregressive prediction of spectrograms , is it autoregressive only in time , or also in frequency ?",request
S1c4VEXWz_35,"I 'm guessing it 's the former ,",evaluation
S1c4VEXWz_36,but this means there is an implicit independence assumption,fact
S1c4VEXWz_37,"( the intensities in different frequency bins are conditionally independent , given all past timesteps ) .",fact
S1c4VEXWz_38,Has this been taken into consideration ?,request
S1c4VEXWz_39,"Maybe it does n't matter because the decoder is never used directly anyway , and this is only a "" feature learning "" stage of sorts ?",non-arg
S1c4VEXWz_40,- Why use the L1 loss on spectrograms ?,request
S1c4VEXWz_41,"- The recent work on Parallel WaveNet may allow for speeding up WaveNet when used as a vocoder ,",fact
S1c4VEXWz_42,this could be worth looking into seeing as inference speed is used as an argument to choose different vocoder strategies ( with poorer audio quality as a result ) .,evaluation
S1c4VEXWz_43,"- The title heavily emphasizes that this model can do multi-speaker TTS with many ( 2000 ) speakers ,",evaluation
S1c4VEXWz_44,but that seems to be only a minor aspect that is only discussed briefly in the paper .,evaluation
S1c4VEXWz_45,And it is also something that preceding systems were already capable of,fact
S1c4VEXWz_46,( although maybe it has n't been tested with a dataset of this size before ) .,fact
S1c4VEXWz_47,It might make sense to rethink the title to emphasize some of the more relevant and novel aspects of this work .,request
H1PuapUef_0,* Paper summary * The paper considers GANs from a theoretical point of view .,fact
H1PuapUef_1,The authors approach GANs from the 3-Wasserstein point of view and provide several insights for a very specific setting .,fact
H1PuapUef_2,"In my point of view , the main novel contribution of the paper is to notice the following fact : ( * ) It is well known that the 2-Wasserstein distance <VAR> between multivariate Gaussian PY and its empirical version QY scales as <VAR> , i.e. converges very slow as the dimensionality of the space <VAR> increases .",evaluation
H1PuapUef_3,"In other words , QY is not such a good way to estimate PY in this setting .",fact
H1PuapUef_4,A somewhat better way is use a Gaussian distribution PZ with covariance matrix S computed as a sample covariance of QY .,fact
H1PuapUef_5,In this case <VAR> scales as <VAR> .,fact
H1PuapUef_6,The paper introduces this observation in a very strange way within the context of GANs .,evaluation
H1PuapUef_7,"Moreover , I think the final conclusion of the paper ( Eq . 19 ) has a mistake ,",fact
H1PuapUef_8,which makes it hard to see why ( * ) has any relation to GANs at all .,evaluation
H1PuapUef_9,There are several other results presented in the paper regarding relation between PCA and the 2-Wasserstein minimization for Gaussian distributions ( Lemma 1 & Theorem 1 ) .,fact
H1PuapUef_10,"This is indeed an interesting point ,",evaluation
H1PuapUef_11,however the proof is almost trivial,evaluation
H1PuapUef_12,and I am not sure if this provides any significant contribution for the future research .,evaluation
H1PuapUef_13,"Overall , I think the paper contains several novel ideas ,",evaluation
H1PuapUef_14,but its structure requires a * significant * rework,evaluation
H1PuapUef_15,and in the current form it is not ready for being published .,evaluation
H1PuapUef_16,* Detailed comments * In the first part of the paper ( Section 2 ) the authors propose to use the optimal transport distance <VAR> between the data distribution PY ( or its empirical version QY ) and the model as the objective for GAN optimization .,fact
H1PuapUef_17,This idea is not novel :,evaluation
H1PuapUef_18,"WGAN [ 1 ] proposed ( and successfully implemented ) to minimize the particular case of W1 distance by going through the dual form ,",fact
H1PuapUef_19,"[ 2 ] proposed to approach any Wc using auto-encoder reformulation of the primal ( and also shoed that [ 5 ] is doing exactly W2 minimization ) ,",fact
H1PuapUef_20,and [ 3 ] proposed the same using Sinkhorn algorithm .,fact
H1PuapUef_21,So this point does not seem to be novel .,evaluation
H1PuapUef_22,"The rest of the paper only considers 2-Wasserstein distance with Gaussian PY and Gaussian <VAR> ( which I will abbreviate with R ) ,",fact
H1PuapUef_23,which looks like an extremely limited scenario ( and certainly has almost no connection to the applications of GANs ) .,evaluation
H1PuapUef_24,"Section 3 first establishes a relation between PCA and minimizing 2-Wasserstein distance for Gaussian distributions ( Lemma 1 , Theorem 1 ) .",fact
H1PuapUef_25,Then the authors show that if R minimizes <VAR> and QR minimizes <VAR> then the excess loss <VAR> approaches zero at the rate <VAR> ( both for linear and unconstrained generators ) .,fact
H1PuapUef_26,This result basically provides an upper bound showing that GANs need exponentially many samples to minimize W2 distance .,fact
H1PuapUef_27,"I do n't find these results novel ,",evaluation
H1PuapUef_28,as they already appeared in [ 4 ] with a matching lower bound for the case of Gaussians,fact
H1PuapUef_29,( Theorem B. 1 in Appendix can be modified easily to show this ) .,fact
H1PuapUef_30,"As the authors note in the conclusion of Section 3 , these results have little to do with GANs ,",fact
H1PuapUef_31,as GANs are known to learn quite quickly,evaluation
H1PuapUef_32,( which contradicts the theory of Section 3 ) .,fact
H1PuapUef_33,"Finally , in Section 4 the authors approach the same W2 problem from its dual form and notice that for the LQG model the optimal discriminator is quadratic .",fact
H1PuapUef_34,Based on this they reformulate the W2 minimization for LQG as the constrained optimization with respect to p.d. matrix A ( Eq 16 ) .,fact
H1PuapUef_35,"The same conclusion does not work unfortunately for <VAR> ,",fact
H1PuapUef_36,which is the real training objective of GANs .,fact
H1PuapUef_37,"Theorem 3 shows that nevertheless , if we still constrain discriminator in the dual form of <VAR> to be quadratic , the resulting soliton <VAR> performs the empirical PCA of Pn .",fact
H1PuapUef_38,"This leads to the final conclusion of the paper ,",fact
H1PuapUef_39,which I think contains a mistake .,fact
H1PuapUef_40,"In Eq 19 the first equation , according to the definitions of the authors , reads <EQN> where QR is trained to minimize min_R <VAR> and PZ is as defined in ( * ) in the beginning of these notes .",fact
H1PuapUef_41,"However , PZ is not the solution of min_R <VAR>",fact
H1PuapUef_42,as the authors notice in the 2nd paragraph of page 8 .,fact
H1PuapUef_43,Thus ( ** ) is not true,fact
H1PuapUef_44,"( at least , it is not proved in the current version of the text ) .",fact
H1PuapUef_45,PZ is a solution of <VAR> where the discriminator is constrained to be quadratic * .,fact
H1PuapUef_46,"This mismatch is especially strange ,",evaluation
H1PuapUef_47,given the authors emphasize in the introduction that they provide bounds on divergences which are the same as used during the training ( see 2nd paragraph on page 2 ),fact
H1PuapUef_48,"--- here the bound is on W2 , but the empirical GAN actually does a regularized training ( with constrained discriminator ) .",evaluation
H1PuapUef_49,"Finally , I do n't think the experiments provide any convincing insights ,",evaluation
H1PuapUef_50,because the authors use W1-minimization to illustrate properties of the W2 .,fact
H1PuapUef_51,"Essentially the authors say "" we do n't have a way to perform W2 minimization , so we rather do the W1 minimization and assume that these two are kind of similar "" .",evaluation
H1PuapUef_52,* Other comments * ( 1 ) Discussion in Section 2.1 seems to never play a role in the paper .,fact
H1PuapUef_53,"( 2 ) Page 4 : in p-Wasserstein distance , <VAR> does not need to be a Euclidean metric .",evaluation
H1PuapUef_54,It can be any metric .,fact
H1PuapUef_55,"( 3 ) Lemma 2 seems to repeat the result from ( Canas and Rosasco , 2012 )",fact
H1PuapUef_56,as later cited by authors on page 7 ?,fact
H1PuapUef_57,( 4 ) It is not obvious how does Theorem 2 translate to the excess loss ?,evaluation
H1PuapUef_58,"( 5 ) Section 4 . I am wondering how exactly the authors are going to compute the conjugate of the discriminator , given the discriminator most likely is a deep neural network ?",evaluation
H1PuapUef_59,[1] <CIT>,reference
H1PuapUef_60,[2] <CIT>,reference
H1PuapUef_61,[3] <CIT>,reference
H1PuapUef_62,[4] <CIT>,reference
H1PuapUef_63,[5] <CIT>,reference
Hyx7bEPez_0,"In this paper , the authors studied the problem of semi-supervised few-shot classification , by extending the prototypical networks into the setting of semi-supervised learning with examples from distractor classes .",fact
Hyx7bEPez_1,"The studied problem is interesting ,",evaluation
Hyx7bEPez_2,and the paper is well-written .,evaluation
Hyx7bEPez_3,Extensive experiments are performed to demonstrate the effectiveness of the proposed methods .,evaluation
Hyx7bEPez_4,"While the proposed method is a natural extension of the existing works ( i.e. , soft k-means and meta-learning ) .",evaluation
Hyx7bEPez_5,"On top of that , It seems the authors have over-claimed their model capability at the first place",evaluation
Hyx7bEPez_6,as the proposed model can not properly classify the distractor examples but just only consider them as a single class of outliers .,fact
Hyx7bEPez_7,"Overall , I would like to vote for a weakly acceptance regarding this paper .",evaluation
ryBhOOXlM_0,The authors ask when the hidden layer units of a multi-layer feed-forward neural network will display selectivity to object categories .,fact
ryBhOOXlM_1,"They train 3-layer ANNs to categorize binary patterns ,",fact
ryBhOOXlM_2,and find that typically at least some of the hidden layer units are category selective .,fact
ryBhOOXlM_3,"The number of category selective ( "" localist "" ) units varies depending on the size of the hidden layer , the structure of the outputs the network is trained to return ( i.e. , one-hot vs distributed ) , the neurons ' activation functions , and the level of dropout-induced noise in the training procedure .",fact
ryBhOOXlM_4,"Overall , I find the work to hint at an interesting phenomenon .",evaluation
ryBhOOXlM_5,"However , the paper as presented uses an overly-simplistic task for the ANNs ,",fact
ryBhOOXlM_6,and the work is sloppily presented .,evaluation
ryBhOOXlM_7,These factors detract from my enthusiasm .,evaluation
ryBhOOXlM_8,My specific criticisms are as follows : 1 ) The binary pattern classification seems overly simplistic a task for this study .,evaluation
ryBhOOXlM_9,"If you want to compare to the medial temporal lobe 's Jennifer Aniston cells ( i.e. , the Quiroga result ) , then an object recognition task seems much more meaningful , as does a deeper network structure .",request
ryBhOOXlM_10,"Likewise , to inform the representations we see in deep object recognition networks , it is better to just study those networks , instead of simple shallow binary classification networks .",request
ryBhOOXlM_11,"Or , at least show that the findings apply to those richer settings , where the networks do "" real "" tasks .",request
ryBhOOXlM_12,"2 ) The paper is somewhat sloppy , and could use a thorough proofreading .",evaluation
ryBhOOXlM_13,"For example , what are "" figures 3 , ?? and 6 "" ?",request
ryBhOOXlM_14,And which is Figure 3.3.1 ?,request
ryBhOOXlM_15,3 ) What formula is used to quantify the selectivity ?,request
ryBhOOXlM_16,"And do the results depend on the cut-off used to label units as "" selective "" or not ( i.e. , using a higher or lower cutoff than 0.05 ) ?",request
ryBhOOXlM_17,"Given that the 0.05 number is somewhat arbitrary , this seems worth checking .",request
ryBhOOXlM_18,4 ) I do n't think that very many people would argue that the presence of distributed representations strictly excludes the possibility of some of the units having some category selectivity .,evaluation
ryBhOOXlM_19,"Consequently , I find the abstract and introduction to be a bit off-putting , coming off almost as a rant against PDP .",evaluation
ryBhOOXlM_20,"This is a minor stylistic thing , but I 'd encourage the authors to tone it down a bit .",request
ryBhOOXlM_21,"5 ) The finding that more of the selective units arise in the hidden layer in the presence of higher levels of noise is interesting ,",evaluation
ryBhOOXlM_22,"and the authors provide some nice intuition for this phenomenon ( i.e. , getting redundant local representations makes the system robust to the dropout ) .",fact
ryBhOOXlM_23,"This seems interesting in light of the Quiroga findings of Jennifer Aniston cells : the fact that the ( small number of ) units they happened to record from showed such selectivity suggests that many neurons in the brain would have this selectivity , so there must be a large number of category selective units .",evaluation
ryBhOOXlM_24,"Does that finding , coupled with the result from Fig. 6 , imply that those "" grandmother cell "" observations might reflect an adaptation to increase robustness to noise ?",non-arg
ByGPUUYgz_0,This paper attacks an important problems with an interesting and promising methodology .,evaluation
ByGPUUYgz_1,"The authors deal with inference in models of collective behavior , specifically at how to infer the parameters of a mean field game representation of collective behavior .",fact
ByGPUUYgz_2,"The technique the authors innovate is to specify a mean field game as a model , and then use inverse reinforcement learning to learn the reward functions of agents in the mean field game .",fact
ByGPUUYgz_3,"This work has many virtues , and could be an impactful piece .",evaluation
ByGPUUYgz_4,"There is still minimal work at the intersection of machine learning and collective behavior ,",evaluation
ByGPUUYgz_5,and this paper could help to stimulate the growth of that intersection .,evaluation
ByGPUUYgz_6,"The application to collective behavior could be an interesting novel application to many in machine learning ,",evaluation
ByGPUUYgz_7,and conversely the inference techniques that are innovated should be novel to many researchers in collective behavior .,evaluation
ByGPUUYgz_8,"At the same time , the scientific content of the work has critical conceptual flaws .",evaluation
ByGPUUYgz_9,"Most fundamentally , the authors appear to implicitly center their work around highly controversial claims about the ontological status of group optimization , without the careful justification necessary to make this kind of argument .",evaluation
ByGPUUYgz_10,"In addition to that , the authors appear to implicitly assume that utility function inference can be used for causal inference .",evaluation
ByGPUUYgz_11,"That is , there are two distinct mistakes the authors make in their scientific claims :",fact
ByGPUUYgz_12,1 ) The authors write as if mean field games represent population optimization,fact
ByGPUUYgz_13,"( Mean field games are not about what a _ group _ optimizes ; they are about what _ individuals _ optimize , and this individual optimization leads to certain patterns in collective behaviors )",fact
ByGPUUYgz_14,2 ) The authors write as if utility/reward function inference alone can provide causal understanding of collective or individual behavior,fact
ByGPUUYgz_15,1 - I should say that I am highly sympathetic to the claim that many types of collective behavior can be viewed as optimizing some kind of objective function .,evaluation
ByGPUUYgz_16,"However , this claim is far from mainstream , and is in fact highly contested .",evaluation
ByGPUUYgz_17,"For instance , many prominent pieces of work in the study of collective behavior have highlighted its irrational aspects , from the madness of crowds to herding in financial markets .",evaluation
ByGPUUYgz_18,"Since it is so fringe to attribute causal agency to groups , let alone optimal agency ,",evaluation
ByGPUUYgz_19,"in the remainder of my review I will give the authors the benefit of the doubt and assume when they say things like "" population behavior may be optimal "" , they mean "" the behavior of individuals within a population may be optimal "" .",evaluation
ByGPUUYgz_20,"If the authors do mean to say this , they should be more careful about their language use in this regard ( individuals are the actors , not populations ) .",request
ByGPUUYgz_21,"If the authors do indeed mean to attribute causal agency to groups ( as suggested in their MDP representation ) , they will run into all the criticisms I would have about an individual-level analysis and more .",evaluation
ByGPUUYgz_22,"Suffice it to say , mean field games themselves do n't make claims about aggregate-level optimization .",fact
ByGPUUYgz_23,A Nash equilibrium achieves a balance between individual-level reward functions .,fact
ByGPUUYgz_24,These reward functions are only interpretable at the individual level .,fact
ByGPUUYgz_25,There is no objective function the group itself in aggregate is optimizing in mean field games .,fact
ByGPUUYgz_26,"For instance , even though the mean field game model of the Mexican wave produces wave solutions ,",fact
ByGPUUYgz_27,the model is premised on people having individual utility functions that lead to emergent wave behavior .,fact
ByGPUUYgz_28,The model does not have the representational capacity to explain that people actually intend to create the emergent behavior of a wave ( even though in this case they do ) .,fact
ByGPUUYgz_29,"Furthermore , the fact that mean field games aggregate to a single-agent MDP does not imply that that the group can rightfully be thought of as an agent optimizing the reward function ,",fact
ByGPUUYgz_30,because there is an exact correspondence between the rewards of the individual agents in the MFG and of the aggregate agent in the MDP by construction .,fact
ByGPUUYgz_31,2 - The authors also claim that their inference methods can help explain why people choose to talk about certain topics .,fact
ByGPUUYgz_32,"As far as the extent to which utility / reward function inference can provide causal explanations of individual ( or collective ) behavior , the argument that is invariably brought against a claim of optimization is that almost any behavior can be explained as optimal post-hoc with enough degrees of freedom in the utiliy function of the behavioral model .",fact
ByGPUUYgz_33,"Since optimization frameworks are so flexible ,",evaluation
ByGPUUYgz_34,they have little explanatory power and are hard to falsify .,evaluation
ByGPUUYgz_35,"In fact , there is literally no way that the modeling framework of the authors even affords the possibility that individual/collective behavior is not optimal .",fact
ByGPUUYgz_36,Optimality is taken as an assumption that allows the authors to infer what reward function is being optimized .,evaluation
ByGPUUYgz_37,The authors state that the reward function they infer helps to interpret collective behavior,fact
ByGPUUYgz_38,because it reveals what people are optimizing .,fact
ByGPUUYgz_39,"However , the reward function actually discovered is not interpretable at all .",fact
ByGPUUYgz_40,It is simply a summary of the statistical properties of changes in popularity of the topics of conversation in the Twitter data the authors ' study .,fact
ByGPUUYgz_41,"To quote the authors ' insights : "" The learned reward function reveals that a real social media population favors states characterized by a highly non-uniform distribution with negative mass gradient in decreasing order of topic popularity , as well as transitions that increase this distribution imbalance . """,quote
ByGPUUYgz_42,The authors might as well have simply visualized the topic popularities and changes in popularities to arrive at such an insight .,request
ByGPUUYgz_43,"To take the authors claims literally , we would say that people have an intrinsic preference for everyone to arbitrarily be talking about the same thing , regardless of the content or relevance of that topic .",fact
ByGPUUYgz_44,"To draw an analogy , this is like observing that on some days everybody on the street is carrying open umbrellas and on other days not , and inferring that the people on the street have a preference for everyone having their umbrellas open together ( and the model would then predict that if one person opens an umbrella on a sunny day , everybody else will too ) .",evaluation
ByGPUUYgz_45,"To the authors credit , they do make a brief attempt to present empirical evidence for their optimization view , stating succinctly : "" The high prediction accuracy of the learned policy provides evidence that real population behavior can be understood and modeled as the result of an emergent population-level optimization with respect to a reward function . """,fact
ByGPUUYgz_46,"Needless to say , this one-sentence argument for a highly controversial scientific claims falls flat on closer inspection .",evaluation
ByGPUUYgz_47,"Setting aside the issues of correlation versus causation , predictive accuracy does not in and of itself provide scientific plausibility .",fact
ByGPUUYgz_48,"When an n-gram model produces text that is in the style of a particular writer , we do not conclude that the writer must have been composing based on the n-gram 's generative mechanism .",fact
ByGPUUYgz_49,Predictive accuracy only provides evidence when combined in the first place with scientific plausibility through other avenues of evidence .,fact
ByGPUUYgz_50,"The authors could attempt to address these issues by making what is called an "" as-if "" argument ,",evaluation
ByGPUUYgz_51,but it 's not even clear such an argument could work here in general .,evaluation
ByGPUUYgz_52,"With all this in mind , it would be more instructive to show that the inference method the authors introduce could infer the correct utility functions used in standard mean field games , such as modeling traffic congestion and the Mexican wave .",request
ByGPUUYgz_53,"-- All that said , the general approach taken in the authors ' work is highly promising ,",evaluation
ByGPUUYgz_54,"and there are many fruitful directions I would be exicted to see this work taken --- e.g. , combining endogenous and exogenous rewards or looking at more complex applications .",evaluation
ByGPUUYgz_55,"As a technical contribution , the paper is wonderful ,",evaluation
ByGPUUYgz_56,and I would enthusiastically support acceptance .,evaluation
ByGPUUYgz_57,"The authors simply either need to be much more careful with the scientific claims about collective behavior they make , or limit the scope of the contribution of the paper to be modeling / inference in the area of collective behavior .",request
ByGPUUYgz_58,"Mean field games are an important class of models in collective behavior ,",evaluation
ByGPUUYgz_59,and being able to infer their parameters is a nice step forward purely due to the importance of that class of games .,evaluation
ByGPUUYgz_60,Identifying where the authors ' inference method could be applied to draw valid scientific conclusions about collective behavior could then be an avenue for future work .,evaluation
ByGPUUYgz_61,Examples of plausible scientific applications might include parameter inference in settings where mean field games are already typically applied in order to improve the fit of those models or to learn about trade-offs people make in their utility functions in those settings .,evaluation
ByGPUUYgz_62,"-- Other minor comments : - ( Introduction ) It is not clear at all how the Arab Spring , Black Lives Matter , and fake news are similar --- i.e. , whether a single model could provide insight into these highly heterogeneous events",evaluation
ByGPUUYgz_63,--- nor is it clear what end the authors hope to achieve by modeling them,evaluation
ByGPUUYgz_64,--- the ethics of modeling protests in a field crowded with powerful institutional actors is worth carefully considering .,evaluation
ByGPUUYgz_65,"- If I understand correctly , the fact that the authors assume a factored reward function seems limiting .",evaluation
ByGPUUYgz_66,Is n't the major benefit of game theory it 's ability to accommodate utility functions that depend on the actions of others ?,evaluation
ByGPUUYgz_67,"- The authors state that one of their essential insights is that "" solving the optimization problem of a single-agent MDP is equivalent to solving the inference problem of an MFG . """,fact
ByGPUUYgz_68,This statement feels a bit too cute at the expense of clarity .,evaluation
ByGPUUYgz_69,"The authors perform inference via inverse-RL ,",fact
ByGPUUYgz_70,so it 's more clear to say the authors are attempting to use statistical inference to figure out what is being optimized .,evaluation
ByGPUUYgz_71,"- The relationship between MFGs and a single-agent MDP is nice and a fine observation , but not as surprising as the authors frame it as .",evaluation
ByGPUUYgz_72,"Any multiagent MDP can be naively represented as a single-agent MDP where the agent has control over the entire population ,",fact
ByGPUUYgz_73,and we already know that stochastic games are closely related to MDPs .,fact
ByGPUUYgz_74,It 's therefore hard to imagine that there woud n't be some sort of correspondence .,evaluation
ry07SzQgG_0,This paper investigates human priors for playing video games .,fact
ry07SzQgG_1,"Considering a simple video game , where an agent receives a reward when she completes a game board , this paper starts by stating that : - Firstly , the humans perform better than an RL agent to complete the game board .",fact
ry07SzQgG_2,"- Secondly , with a simple modification of textures the performances of human players collapse , while those of a RL agent stay the same .",fact
ry07SzQgG_3,"If I have no doubts about these results , I have a concern about the method .",evaluation
ry07SzQgG_4,"In the case of human players the time needed to complete the game is plotted ,",fact
ry07SzQgG_5,and in the case of a RL agent the number of steps needed to complete the game is plotted ( fig 1 ) .,fact
ry07SzQgG_6,"Formally , we can not conclude that one minute is lesser than 4 million of steps .",fact
ry07SzQgG_7,This issue could be easily fixed .,evaluation
ry07SzQgG_8,"Unfortunately , I have other concerns about the method and the conclusions .",evaluation
ry07SzQgG_9,"For instance , masking where objects are or suppressing visual similarity between similar objects should also deteriorate the performance of a RL agent .",fact
ry07SzQgG_10,So it can not be concluded that the change of performances is due to human priors .,fact
ry07SzQgG_11,"In these cases , I think that the change of performances is due to the increased difficulty of the game .",fact
ry07SzQgG_12,The authors have to include RL agent in all their experiments to be able to dissociate what is due to human priors and what is due to the noise introduced in the game .,request
BJHcawFxM_0,This paper proposes training binary and ternary weight distribution networks through the local reparametrization trick and continuous optimization .,fact
BJHcawFxM_1,"The argument is that due to the central limit theorem ( CLT ) the distribution on the neuron pre-activations is approximately Gaussian , with a mean given by the inner product between the input and the mean of the weight distribution and a variance given by the inner product between the squared input and the variance of the weight distribution .",fact
BJHcawFxM_2,"As a result , the parameters of the underlying discrete distribution can be optimized via backpropagation by sampling the neuron pre-activations with the reparametrization trick .",fact
BJHcawFxM_3,The authors further propose appropriate initialisation schemes and regularization techniques to either prevent the violation of the CLT or to prevent underfitting .,fact
BJHcawFxM_4,The method is evaluated on multiple experiments .,fact
BJHcawFxM_5,This paper proposed a relatively simple idea for training networks with discrete weights that seems to work in practice .,evaluation
BJHcawFxM_6,"My main issue is that while the authors argue about novelty ,",fact
BJHcawFxM_7,the first application of CLT for sampling neuron pre-activations at neural networks with discrete r.v.s is performed at [ 1 ] .,fact
BJHcawFxM_8,"While [ 1 ] was only interested in faster convergence and not on optimization of the parameters of the underlying distribution ,",fact
BJHcawFxM_9,the extension was very straightforward .,evaluation
BJHcawFxM_10,I would thus suggest that the authors update the paper accordingly .,request
BJHcawFxM_11,"Other than that , I have some other comments : - The L2 regularization on the distribution parameters for the ternary weights is a bit ad-hoc ;",evaluation
BJHcawFxM_12,why not penalise according to the entropy of the distribution which is exactly what you are trying to achieve ?,request
BJHcawFxM_13,- For the binary setting you mentioned that you had to reduce the entropy thus added a “ beta density regulariser ” .,fact
BJHcawFxM_14,Did you add R ( p ) or log R ( p ) to the objective function ?,non-arg
BJHcawFxM_15,"Also , with <EQN> the beta density is unimodal with a peak at <EQN> ;",fact
BJHcawFxM_16,"essentially this will force the probabilities to be close to 0.5 , i.e. exactly what you are trying to avoid .",fact
BJHcawFxM_17,To force the probability near the endpoints you have to use <EQN> which results into a “ bowl ” shaped Beta distribution .,fact
BJHcawFxM_18,I thus wonder whether any gains you observed from this regulariser are just an artifact of optimization .,fact
BJHcawFxM_19,"- I think that a baseline ( at least for the binary case ) where you learn the weights with a continuous relaxation , such as the concrete distribution , and not via CLT would be helpful .",request
BJHcawFxM_20,Maybe for the network to properly converge the entropy for some of the weights needs to become small ( hence break the CLT ) .,fact
BJHcawFxM_21,[1] <CIT>,reference
S1uLIj8lG_0,* sec. 2.2 is about label-preserving translation,fact
S1uLIj8lG_1,and many notations are introduced .,evaluation
S1uLIj8lG_2,"However , it is not clear what label here refers to ,",evaluation
S1uLIj8lG_3,and it does not shown in the notation so far at all .,fact
S1uLIj8lG_4,"Only until the end of sec. 2.2 , the function <VAR> is introduced and its revelation - Google Search as label function is discussed only at Fig. 4 and sec. 2.3 .",fact
S1uLIj8lG_5,"* pp. 5 first paragraph : when assuming D_X and D_Y being perfect , why <EQN> ?",request
S1uLIj8lG_6,"To trace back , in fact it is helpful to have at least a simple intro/def . to the functions <VAR> and <VAR> of Eq . ( 1 ) .",evaluation
S1uLIj8lG_7,* Somehow there is a feeling that the notations in sec. 2.1 and sec. 2.2 are not well aligned .,evaluation
S1uLIj8lG_8,"It is helpful to start providing the math notations as early as sec. 2.1 ,",evaluation
S1uLIj8lG_9,"so labels , pseudo labels , the algorithm illustrated in Fig. 2 etc. can be consistently integrated with the rest notations .",evaluation
S1uLIj8lG_10,"* F ( ) is firstly shown in Fig. 2 the beginning of pp. 3 , and is mentioned in the main text as late as of pp. 5 .",fact
S1uLIj8lG_11,* Table 2 : The CNN baseline gives an error rate of 7.80,fact
S1uLIj8lG_12,while the proposed variants are 7.73 and 7.60 respectively .,fact
S1uLIj8lG_13,The difference of 0.07 / 0.20 are not so significant .,evaluation
S1uLIj8lG_14,Any explanation for that ?,request
S1uLIj8lG_15,Minor issues : * The uppercase X in the sentence before Eq . ( 2 ) should be calligraphic X,request
rkHhxN2lG_0,This paper focuses on the density estimation when the amount of data available for training is low .,fact
rkHhxN2lG_1,"The main idea is that a meta-learning model must be learnt , which learns to generate novel density distributions by learn to adapt a basic model on few new samples .",fact
rkHhxN2lG_2,The paper presents two independent method .,fact
rkHhxN2lG_3,The first method is effectively a PixelCNN combined with an attention module .,fact
rkHhxN2lG_4,"Specifically , the support set is convolved to generate two sets of feature maps , the so called "" key "" and the "" value "" feature maps .",fact
rkHhxN2lG_5,"The key feature map is used from the model to compute the attention in particular regions in the support images to generate the pixels for the new "" target "" image .",fact
rkHhxN2lG_6,"The value feature maps are used to copmpute the local encoding , which is used to generate the respective pixels for the new target image , taking into account also the attention values .",fact
rkHhxN2lG_7,"The second method is simpler ,",evaluation
rkHhxN2lG_8,and very similar to fine-tuning the basis network on the few new samples provided during training .,evaluation
rkHhxN2lG_9,"Despite some interesting elements ,",evaluation
rkHhxN2lG_10,the paper has problems .,evaluation
rkHhxN2lG_11,"First , the novelty is rather limited .",evaluation
rkHhxN2lG_12,"The first method seems to be slightly more novel ,",evaluation
rkHhxN2lG_13,although it is unclear whether the contribution by combining different models is significant .,evaluation
rkHhxN2lG_14,The second method is too similar to fine-tuning :,evaluation
rkHhxN2lG_15,"although the authors claim that <VAR> can be any function that minimizes the total loss <VAR> ,",fact
rkHhxN2lG_16,in the end it is clear that the log-likelihood is used .,fact
rkHhxN2lG_17,"How is this approach ( much ) different from standard fine-tuning ,",fact
rkHhxN2lG_18,"since the quantity <VAR> is anyways unknown and can not be "" trained "" to be maximized .",fact
rkHhxN2lG_19,"Besides the limited novelty ,",evaluation
rkHhxN2lG_20,the submission leaves several parts unclear .,evaluation
rkHhxN2lG_21,"First , why are the convolutional features of the support set in the first methods divided into "" key "" and "" value "" feature maps as in <EQN> , <EQN> ?",request
rkHhxN2lG_22,"Is this division arbitrary , or is there a more basic reason ?",request
rkHhxN2lG_23,"Also , is there any different between key and value ?",request
rkHhxN2lG_24,Why not use the same feature map for computing the attention and computing eq ( 7 ) ?,request
rkHhxN2lG_25,"Also , in the first model it is suggested that an additional feature can be having a 1-of-K channel for the supporting image label :",fact
rkHhxN2lG_26,"the reason is that you might have multiple views of objects , and knowing which view contributes to the attention can help learning the density .",fact
rkHhxN2lG_27,"However , this assumes that the views are ordered , namely that the recording stage has a very particular format .",fact
rkHhxN2lG_28,"Is n't this a bit unrealistic , given the proposed setup anyways ?",evaluation
rkHhxN2lG_29,"Regarding the second method , it is not clear why leaving this room for flexibility ( by allowing L_inner to be any function ) to the model is a good idea .",evaluation
rkHhxN2lG_30,Is n't this effectively opening the doors to massive overfitting ?,evaluation
rkHhxN2lG_31,"Besides , is n't the statement that the function <VAR> void ?",evaluation
rkHhxN2lG_32,"At the end of the day one can also claim the same for gradient descent : you do n't need to have the true gradients of the true loss , as long as the objective function obtains gradually lower and lower values ?",fact
rkHhxN2lG_33,"Last , it is unclear what is the connection between the first and the second model .",evaluation
rkHhxN2lG_34,Are these two independent models that solve the same problem ?,request
rkHhxN2lG_35,Or are they connected ?,request
rkHhxN2lG_36,"Regarding the evaluation of the models , the nature of the task makes the evaluation hard :",evaluation
rkHhxN2lG_37,for real data like images one can not know the true distribution of particular support examples .,fact
rkHhxN2lG_38,"Surrogate tasks are explored , first image flipping , then likelihood estimation of Omniglot characters , then image generation .",fact
rkHhxN2lG_39,"Image flipping does not sound a very relevant task to density estimation , given that the task is deterministic .",evaluation
rkHhxN2lG_40,"Perhaps , what would make more sense would be to generate a new image given that the support set has images of a particular orientation , meaning that the model must learn how to learn densities from arbitrary rotations .",evaluation
rkHhxN2lG_41,"Regarding Omniglot character generation , the surrogate task of computing likelihood of known samples gives a bit better ,",evaluation
rkHhxN2lG_42,"however , this is to be expected when combining a model without attention , with an attention module .",evaluation
rkHhxN2lG_43,"All in all , the paper has some interesting ideas .",evaluation
rkHhxN2lG_44,I encourage the authors to work more on their submission and think of a better evaluation and resubmit .,request
Skoq_d6gz_0,This paper presents a semi-supervised extension for applying GANs to regression tasks .,fact
Skoq_d6gz_1,The authors propose two architectures : one adds a supervised regression loss to the standard unsupervised GAN discriminator loss .,fact
Skoq_d6gz_2,The other replaces the real/fake output of the discriminator with only a real-valued output and then applies a kernel on top of this output to predict if samples are real or fake .,fact
Skoq_d6gz_3,"The methods are evaluated on a public driving dataset ,",fact
Skoq_d6gz_4,and are shown to outperform an Improved-GAN which predicts the real-valued labels discretized into 10 classes .,fact
Skoq_d6gz_5,"This is a nice idea ,",evaluation
Skoq_d6gz_6,but I am not completely convinced by the experimental results .,evaluation
Skoq_d6gz_7,The proposed method is compared to Improved-GAN where the real-valued labels are discretized into 10 classes .,fact
Skoq_d6gz_8,Why 10 ?,non-arg
Skoq_d6gz_9,How was this chosen ?,non-arg
Skoq_d6gz_10,"The authors rightfully state that "" [ ... ] this discretization will add some unavoidable quantization error to our training "" ( Sec 5.2 ) and then again in the conclusion "" determining the number of [ discretization ] classes for each application is non-trivial "" ,",fact
Skoq_d6gz_11,yet nowhere do they explore the effects of this .,fact
Skoq_d6gz_12,"Surely , this is a very important part of the evaluation ?",evaluation
Skoq_d6gz_13,And surely as we improve the discretization-resolution the gap between the two will close ?,evaluation
Skoq_d6gz_14,This needs to be evaluated .,request
Skoq_d6gz_15,"Also , the main motivation for a GAN-based regression model is based on the paucity of labeled training data .",evaluation
Skoq_d6gz_16,"However , this is another place where the argument would greatly benefit from some empirical backing .",request
Skoq_d6gz_17,"I.e. , I would really at least like to see how a discriminative regression model ( e.g. a pretrained convnet fine-tuned for regression ) compares to the proposed technique when trained ( fine-tuned ) only on the ( smaller ) labeled data set , perhaps augmented with standard image augmentation techniques to increase the size .",request
Skoq_d6gz_18,"Overall , I found the paper a little hard to read ( especially understanding how Architecture 2 works and moreover what its motivation is )",evaluation
Skoq_d6gz_19,and empirical evaluation a bit lacking .,evaluation
Skoq_d6gz_20,"I also found the claims of "" solving "" the regression task using GANs unfounded based on the experimental results presented .",evaluation
Skoq_d6gz_21,"In conclusion , while the technique looks promising , the novelty seems fairly low",evaluation
Skoq_d6gz_22,and the evaluation can benefit from one or more additional baselines,request
Skoq_d6gz_23,"( at the very least showing how varying the discretization resolution of the Improved-GAN affects the results , but preferably one or more discriminative baselines ) ,",request
Skoq_d6gz_24,and also perhaps on one or more additional data sets to showcase the technique 's generality .,request
Skoq_d6gz_25,Nits : Several part are quite repetitive and can benefit from a rewrite .,request
Skoq_d6gz_26,Particularly the last paragraphs in the Introduction .,request
Skoq_d6gz_27,Section 3 : notation seems inconsistent ( <VAR> vs <VAR> directly below in Eqn 1 ),evaluation
Skoq_d6gz_28,"The second architecture needs to be explained a little better , and motivated a little better .",request
Skoq_d6gz_29,"Eqn 5 : I think it should be <VAR> , and not <VAR>",request
ry9X12Fgz_0,The authors present two autoregressive models for sampling action probabilities from a factorized discrete action space .,fact
ry9X12Fgz_1,"On a multi-agent gridworld task and a multi-agent multi-armed bandit task , the proposed method seems to benefit from their lower-variance entropy estimator for exploration bonus .",fact
ry9X12Fgz_2,"A few key citations were missing - notably the LSTM model they propose is a clear instance of an autoregressive density estimator , as in PixelCNN , WaveNet and other recently popular deep architectures .",evaluation
ry9X12Fgz_3,"In that context , this work can be viewed as applying deep autoregressive density estimators to policy gradient methods .",evaluation
ry9X12Fgz_4,At least one of those papers ought to be cited .,request
ry9X12Fgz_5,"It also seems like a simple , obvious baseline is missing from their experiments - simply independently outputting D independent softmaxes from the policy network .",evaluation
ry9X12Fgz_6,"Without that baseline it 's not clear that any actual benefit is gained by modeling the joint distribution between actions , especially since the optimal policy for an MDP is provably deterministic anyway .",evaluation
ry9X12Fgz_7,"The method could even be made to capture dependencies between different actions by adding a latent probabilistic layer in the middle of the policy network , inducing marginal dependencies between different actions .",evaluation
ry9X12Fgz_8,A direct comparison against one of the related methods in the discussion section would help better contextualize the paper as well .,request
ry9X12Fgz_9,"A final point on clarity of presentation - in keeping with the convention in the field , the readability of the tables could be improved by putting the top-performing models in bold , and Table 2 should almost certainly be replaced by a boxplot .",request
r1cIB5Fxf_0,Paper proposes to use a convolutional network with 3 layers ( convolutional + maxpoolong + fully connected layers ) to embed time series in a new space such that an Euclidian distance is effective to perform a classification .,fact
r1cIB5Fxf_1,The algorithm is simple and experiments show that it is effective on a limited benchmark .,evaluation
r1cIB5Fxf_2,It would be interesting to enlarge the dataset to be able to compare statistically the results with state-of-the-art algorithms .,request
r1cIB5Fxf_3,"In addition , Authors compare themselves with time series metric learning and generalization of DTW algorithms .",fact
r1cIB5Fxf_4,It would also be interesting to compare with other types of time series classification algorithms ( Bagnall 2016 ) .,request
S1jezarxG_0,The paper offers a formal proof that gradient descent on the logistic loss converges very slowly to the hard SVM solution in the case where the data are linearly separable .,fact
S1jezarxG_1,"This result should be viewed in the context of recent attempts at trying to understand the generalization ability of neural networks , which have turned to trying to understand the implicit regularization bias that comes from the choice of optimizer .",evaluation
S1jezarxG_2,"Since we do not even understand the regularization bias of optimizers for the simpler case of linear models ,",fact
S1jezarxG_3,I consider the paper 's topic very interesting and timely .,evaluation
S1jezarxG_4,"The overall discussion of the paper is well written ,",evaluation
S1jezarxG_5,"but on a more detailed level the paper gives an unpolished impression , and has many technical issues .",evaluation
S1jezarxG_6,"Although I suspect that most ( or even all ) of these issues can be resolved , they interfere with checking the correctness of the results .",evaluation
S1jezarxG_7,"Unfortunately , in its current state I therefore do not consider the paper ready for publication .",evaluation
S1jezarxG_8,Technical Issues : The statement of Lemma 5 has a trivial part and for the other part the proof is incorrect : Let <EQN> .,fact
S1jezarxG_9,"- Then the statement <EQN> is trivial ,",fact
S1jezarxG_10,because it follows directly from <EQN> for all u.,fact
S1jezarxG_11,"I would expect the intended statement to be <EQN> ,",evaluation
S1jezarxG_12,which actually follows from the proof of the lemma .,fact
S1jezarxG_13,- The proof of the claim that <EQN> is incorrect :,fact
S1jezarxG_14,"<EQN> does not in itself imply that <EQN> , as claimed .",fact
S1jezarxG_15,"For instance , we might have <EQN> when <EQN> for <EQN> and <EQN> for all other t.",fact
S1jezarxG_16,Definition of <VAR> in Theorem 4 : - Why would tilde { w } be unique ?,fact
S1jezarxG_17,"In particular , if the support vectors do not span the space , because all data lie in the same lower-dimensional hyperplane , then this is not the case .",fact
S1jezarxG_18,"- The KKT conditions do not rule out the case that <EQN> , but <EQN> ( i.e. a support vector that touches the margin , but does not exert force against it ) .",fact
S1jezarxG_19,"Such n are then included in <VAR> , but lead to problems in ( 2.7 ) ,",fact
S1jezarxG_20,"because they would require <EQN> , which is not possible .",fact
S1jezarxG_21,"In the proof of Lemma 6 , case 2 . at the bottom of p. 14 : - After the first inequality , <EQN> should be <EQN>",request
S1jezarxG_22,- After the second inequality the part between brackets is missing an additional term <VAR> .,request
S1jezarxG_23,"- In addition , the label ( 1 ) should be on the previous inequality and it should be mentioned that <EQN> is applied for <EQN> ( otherwise it might be false ) .",request
S1jezarxG_24,"In the proof of Lemma 6 , case 2 in the middle of p. 15 : - In the line of inequality ( 1 ) there is a <VAR> missing .",request
S1jezarxG_25,In the next line there is a factor <VAR> too much .,request
S1jezarxG_26,"- In addition , the inequality <EQN> holds for all x , so no need to mention that <EQN> .",request
S1jezarxG_27,In Lemma 1 : - claim ( 3 ) should be <EQN>,request
S1jezarxG_28,- In the proof : <EQN> only holds for large enough t.,fact
S1jezarxG_29,"Remarks : p. 4 The claim that "" we can expect the population ( or test ) misclassification error of <VAR> to improve "" because "" the margin of <VAR> keeps improving "" is worded a little too strongly ,",evaluation
S1jezarxG_30,because it presumes that the maximum margin solution will always have the best generalization error .,fact
S1jezarxG_31,In the proof sketch ( p. 3 ) : - Why does the fact that the limit is dominated by gradients that are a linear combination of support vectors imply that w_infinity will also be a non-negative linear combination of support vectors ?,request
S1jezarxG_32,"- "" converges to some limit "" . Mention that you call this limit w_infinity",request
S1jezarxG_33,"Minor Issues : In ( 2.4 ) : add "" for all n "" .",request
S1jezarxG_34,"p. 10 , footnote : Should n't "" <EQN> "" be something like "" <EQN> "" ?",request
S1jezarxG_35,A. 9 : ell should be ell ',request
S1jezarxG_36,The paper needs a round of copy editing .,request
S1jezarxG_37,"For instance : - top of p. 4 : "" where <VAR> A is the unique """,quote
S1jezarxG_38,"- p. 10 : "" the solution <VAR> to TO eq . A. 2 """,quote
S1jezarxG_39,"- p. 10 : "" might BOT be unique """,quote
S1jezarxG_40,"- p. 10 : "" penrose-moorse pseudo inverse "" -> "" Moore-Penrose pseudoinverse """,request
S1jezarxG_41,"In the bibliography , Kingma and Ba is cited twice , with different years .",fact
Bk9oIe5gG_0,The paper investigates different representation learning methods to create a latent space for intrinsic goal generation in guided exploration algorithms .,fact
Bk9oIe5gG_1,The research is in principle very important and interesting .,evaluation
Bk9oIe5gG_2,The introduction discusses a great deal about intrinsic motivations and about goal generating algorithms .,evaluation
Bk9oIe5gG_3,"This is really great ,",evaluation
Bk9oIe5gG_4,just that the paper only focuses on a very small aspect of learning a state representation in an agent that has no intrinsic motivation other than trying to achieve random goals .,evaluation
Bk9oIe5gG_5,I think the paper ( not only the Intro ) could be a bit condensed to more concentrate on the actual contribution .,evaluation
Bk9oIe5gG_6,The contribution is that the quality of the representation and the sampling of goals is important for the exploration performance and that classical methods like ISOMap are better than Autoencoder-type methods .,fact
Bk9oIe5gG_7,"Also , it is written in the Conclusions ( and in other places ) : "" [ . . ] we propose a new intrinsically Motivated goal exploration strategy .... "" .",fact
Bk9oIe5gG_8,This is not really true .,fact
Bk9oIe5gG_9,"There is nothing new with the intrinsically motivated selection of goals here , just that they are in another space .",fact
Bk9oIe5gG_10,"Also , there is no intrinsic motivation .",fact
Bk9oIe5gG_11,I also think the title is misleading .,evaluation
Bk9oIe5gG_12,The paper is in principle interesting .,evaluation
Bk9oIe5gG_13,"However , I doubt that the experimental evaluations are substantial enough for profound conclusion .",evaluation
Bk9oIe5gG_14,"Several points of critic : - the input space was very simple in all experiments , not suitable for distinguishing between the algorithms ,",evaluation
Bk9oIe5gG_15,"for instance , ISOMap typically suffers from noise and higher dimensional manifolds , etc .",fact
Bk9oIe5gG_16,"- only the ball/arrow was in the input image , not the robotic arm .",fact
Bk9oIe5gG_17,"I understand this because in phase 1 the robot would not move ,",fact
Bk9oIe5gG_18,but this connects to the next point : - The representation learning is only a preprocessing step requiring a magic first phase .,fact
Bk9oIe5gG_19,- > Representation is not updated during exploration,fact
Bk9oIe5gG_20,- The performance of any algorithm ( except FI ) in the Arm-Arrow task is really bad but without comment .,evaluation
Bk9oIe5gG_21,- I am skeptical about the VAE and RFVAE results .,evaluation
Bk9oIe5gG_22,"The difference between Gaussian sampling and the KDE is a bit alarming ,",evaluation
Bk9oIe5gG_23,as the KL in the VAE training is supposed to match the <VAR> with <VAR> .,fact
Bk9oIe5gG_24,Given the power of the encoder/decoder it should be possible to properly represent the simple embedded 2D/3D manifold and not just a very small part of it as suggested by Fig 10 .,fact
Bk9oIe5gG_25,I have a hard time believing these results .,evaluation
Bk9oIe5gG_26,I urge you to check for any potential errors made .,request
Bk9oIe5gG_27,If there are not mistakes then this is indeed alarming .,evaluation
Bk9oIe5gG_28,Questions : - Is it true that the robot always starts from same initial condition ?!,evaluation
Bk9oIe5gG_29,Context = Emptyset .,non-arg
Bk9oIe5gG_30,"- For ISOMap etc , you also used a 10dim embedding ?",non-arg
Bk9oIe5gG_31,Suggestion : - The main problem seems to be that some algorithms are not representing the whole input space .,evaluation
Bk9oIe5gG_32,- an additional measure that quantifies the difference between true input distribution and reproduced input distribution could tier the algorithms apart and would measure more what seems to be relevant here .,request
Bk9oIe5gG_33,One could for instance measure the KL-divergence between the true input and the sampled ( reconstructed ) input ( using samples and KDE or the like ) .,fact
Bk9oIe5gG_34,- This could be evaluated on many different inputs ( also those with a bit more complicated structure ) without actually performing the goal finding .,fact
Bk9oIe5gG_35,- BTW : I think Fig 10 is rather illustrative and should be somehow in the main part of the paper,evaluation
Bk9oIe5gG_36,"On the positive side , the paper provides lots of details in the Appendix .",evaluation
Bk9oIe5gG_37,"Also , it uses many different Representation Learning algorithms and uses measures from manifold learning to access their quality .",fact
Bk9oIe5gG_38,"In the related literature , in particular concerning the intrinsic motivation , I think the following papers are relevant : <CIT>",reference
Bk9oIe5gG_39,and <CIT> .,reference
Bk9oIe5gG_40,Typos and small details : p3 par2 : for PCA you cited Bishop .,fact
Bk9oIe5gG_41,"Not critical , but either cite one the original papers or maybe remove the cite altogether",request
Bk9oIe5gG_42,p4 par-2 : has multiple interests ... : interests - > purposes ?,request
Bk9oIe5gG_43,p4 par-1 : Outcome Space to the agent is is ...,fact
Bk9oIe5gG_44,Sec 2.2 par1 : are rapidly mentioned ... - > briefly,request
Bk9oIe5gG_45,"Sec 2.3 ... Outcome Space O , we can rewrite the architecture as : and then comes the algorithm .",request
Bk9oIe5gG_46,This is a bit weird,evaluation
Bk9oIe5gG_47,Sec 3 : par1 : experimental campaign - > experiments ?,request
Bk9oIe5gG_48,p7 : Context Space : the object was reset to a random position or always to the same position ?,request
Bk9oIe5gG_49,Footnote 14 : superior to - > larger than,request
Bk9oIe5gG_50,p8 par2 : Exploration Ratio Ratio_expl ... probably also want to add ( ER ) as it is later used,request
Bk9oIe5gG_51,Sec 4 : slightly underneath - > slightly below,request
Bk9oIe5gG_52,p9 par1 : unfinished sentence : It is worth noting that the ....,fact
Bk9oIe5gG_53,one sentence later : RP architecture ? RPE ?,request
Bk9oIe5gG_54,Fig 3 : the error of the methods ( except FI ) are really bad .,evaluation
Bk9oIe5gG_55,An MSE of 1 means hardly any performance !,evaluation
Bk9oIe5gG_56,p11 par2 : for e.g. with the SAGG .... . grammar ?,request
Bk9oIe5gG_57,Plots in general : use bigger font sizes .,request
S12o7fqlM_0,This paper tackles the task of learning embeddings of multi-relational graphs using a neural network .,fact
S12o7fqlM_1,"As much of previous work , the proposed architecture works on triples ( h , r , t ) wth h , t entities and r the relation type .",fact
S12o7fqlM_2,"Despite interesting experimental results , I find that the paper carries too many imprecisions as is .",evaluation
S12o7fqlM_3,"* One of the main originality of the approach is to be able for a given input triple to train by sequentially removing in turn the head h , then the tail t and finally the relation r.",fact
S12o7fqlM_4,( called multi-shot in the paper ) .,fact
S12o7fqlM_5,"However , most ( if not all ) approaches learning embeddings of multi-relational graphs also create multiple examples given a triple .",fact
S12o7fqlM_6,"And that , at least since "" Learning Structured Embeddings of Knowledge Bases "" by Bordes et al. 2011 that was predicting h and t ( not r ) .",fact
S12o7fqlM_7,The only difference is that here it is done sequentially,fact
S12o7fqlM_8,while most methods sample one case each time .,fact
S12o7fqlM_9,Not really meaningful or at least not proved meaningful here .,evaluation
S12o7fqlM_10,* The sequential/RNN-like structure is unclear and it is hard to see how it relates to the data .,evaluation
S12o7fqlM_11,"* Writing that the proposed method "" unsupervised , which is distinctly different from previous works "" is not true or should be rephrased .",request
S12o7fqlM_12,The only difference comes from that the prediction function ( softmax and not ranking for instance ) and the loss used .,fact
S12o7fqlM_13,But none of the methods compared in the experiments use more information than GEN ( the original graph ) .,fact
S12o7fqlM_14,GEN is not the only model using a softmax by the way .,fact
S12o7fqlM_15,* The fact of predicting indistinctly a fact or its reverse seems rather worrying to me .,evaluation
S12o7fqlM_16,"Predicting that "" John is_father_of Paul "" or that "" John is_child_of Paul "" is not the same . . !",fact
S12o7fqlM_17,How is assessed the fact that a prediction is conceptually correct ?,request
S12o7fqlM_18,Using types ?,non-arg
S12o7fqlM_19,* The bottom part of Table 2 is surprising .,evaluation
S12o7fqlM_20,"How come for the task of predicting Head , the model trained only at predicting heads ( <EQN> ) performs worse than the model trained only at predicting tails ( <EQN> ) ?",fact
Bk8FeZjgf_0,"Instead of either optimization-based variational EM or an amortized inference scheme implemented via a neural network as in standard VAE models , this paper proposes a hybrid approach that essentially combines the two .",fact
Bk8FeZjgf_1,"In particular , the VAE inference step , i.e. , estimation of <VAR> , is conducted via application of a recent learning-to-learn paradigm",fact
Bk8FeZjgf_2,"( Andrychowicz et al. , 2016 ) ,",reference
Bk8FeZjgf_3,whereby direct gradient ascent on the ELBO criteria with respect to moments of <VAR> is replaced with a neural network that iteratively outputs new parameter estimates using these gradients .,fact
Bk8FeZjgf_4,The resulting iterative inference framework is applied to a couple of small datasets and shown to produce both faster convergence and a better likelihood estimate .,fact
Bk8FeZjgf_5,"Although probably difficult for someone to understand that is not already familiar with VAE models ,",evaluation
Bk8FeZjgf_6,"I felt that this paper was nonetheless clear and well-presented , with a fair amount of useful background information and context .",evaluation
Bk8FeZjgf_7,"From a novelty standpoint though , the paper is not especially strong",evaluation
Bk8FeZjgf_8,given that it represents a fairly straightforward application of,evaluation
Bk8FeZjgf_9,"( Andrychowicz et al. , 2016 ) .",reference
Bk8FeZjgf_10,"Indeed the paper perhaps anticipates this perspective and preemptively offers that "" variational inference is a qualitatively different optimization problem "" than that considered in ( Andrychowicz et al. , 2016 ) , and also that non-recurrent optimization models are being used for the inference task , unlike prior work .",evaluation
Bk8FeZjgf_11,"But to me , these are rather minor differentiating factors ,",evaluation
Bk8FeZjgf_12,"since learning-to-learn is a quite general concept already ,",evaluation
Bk8FeZjgf_13,and the exact model structure is not the key novel ingredient .,evaluation
Bk8FeZjgf_14,"That being said , the present use for variational inference nonetheless seems like a nice application ,",evaluation
Bk8FeZjgf_15,and the paper presents some useful insights such as Section 4.1 about approximating posterior gradients .,evaluation
Bk8FeZjgf_16,"Beyond background and model development , the paper presents a few experiments comparing the proposed iterative inference scheme against both variational EM , and pure amortized inference as in the original , standard VAE .",fact
Bk8FeZjgf_17,"While these results are enlightening ,",evaluation
Bk8FeZjgf_18,most of the conclusions are not entirely unexpected .,evaluation
Bk8FeZjgf_19,"For example , given that the model is directly trained with the iterative inference criteria in place ,",fact
Bk8FeZjgf_20,"the reconstructions from Fig. 4 seem like exactly what we would anticipate , with the last iteration producing the best result .",evaluation
Bk8FeZjgf_21,It would certainly seem strange if this were not the case .,evaluation
Bk8FeZjgf_22,"And there is no demonstration of reconstruction quality relative to existing models ,",fact
Bk8FeZjgf_23,which could be helpful for evaluating relative performance .,evaluation
Bk8FeZjgf_24,"Likewise for Fig. 6 ,",fact
Bk8FeZjgf_25,where faster convergence over traditional first-order methods is demonstrated ;,fact
Bk8FeZjgf_26,"but again , these results are entirely expected",evaluation
Bk8FeZjgf_27,as this phenomena has already been well-documented in,fact
Bk8FeZjgf_28,"( Andrychowicz et al. , 2016 ) .",reference
Bk8FeZjgf_29,"In terms of Fig. 5 ( b ) and Table 1 , the proposed approach does produce significantly better values of the ELBO critera ;",fact
Bk8FeZjgf_30,"however , is this really an apples-to-apples comparison ?",evaluation
Bk8FeZjgf_31,"For example , does the standard VAE have the same number of parameters/degrees-of-freedom as the iterative inference model , or might eq . ( 4 ) involve fewer parameters than eq . ( 5 ) since there are fewer inputs ?",evaluation
Bk8FeZjgf_32,"Overall , I wonder whether iterative inference is better than standard inference with eq . ( 4 ) , or whether the recurrent structure from eq . ( 5 ) just happens to implicitly create a better neural network architecture for the few examples under consideration .",evaluation
Bk8FeZjgf_33,"In other words , if one plays around with the standard inference architecture a bit , perhaps similar results could be obtained .",evaluation
Bk8FeZjgf_34,"Other minor comment : * In Fig. 5 ( a ) , it seems like the performance of the standard inference model is still improving",evaluation
Bk8FeZjgf_35,but the iterative inference model has mostly saturated .,fact
Bk8FeZjgf_36,"* A downside of the iterative inference model not discussed in the paper is that it requires computing gradients of the objective even at test time ,",fact
Bk8FeZjgf_37,whereas the standard VAE model would not .,fact
H1Mbr8b4f_0,General comment ============== Low-rank decomposing convolutional filters has been used to speedup convolutional networks at the cost of a drop in prediction performance .,fact
H1Mbr8b4f_1,"The authors a ) extended existing decomposition techniques by an iterative method for decomposition and fine-tuning convolutional filter weights ,",fact
H1Mbr8b4f_2,and b ) and algorithm to determine the rank of each convolutional filter .,fact
H1Mbr8b4f_3,The authors show that their method enables a higher speedup and lower accuracy drop than existing methods when applied to VGG16 .,fact
H1Mbr8b4f_4,The proposed method is a useful extension of existing methods but needs to evaluated more rigorously .,evaluation
H1Mbr8b4f_5,The manuscript is hard to read due to unclear descriptions and grammatical errors .,evaluation
H1Mbr8b4f_6,Major comments ============= 1 . The authors authors showed that their method enables a higher speedup and lower drop in accuracy than existing methods when applied to VGG16 .,fact
H1Mbr8b4f_7,"The authors should analyze if this also holds true for ResNet and Inception , which are more widely used than VGG16 .",request
H1Mbr8b4f_8,2 . The authors measured the actual speedup on a single CPU ( Intel Core i5 ) .,fact
H1Mbr8b4f_9,The authors should measure the actual speedup also on a single GPU .,request
H1Mbr8b4f_10,3 . It is unclear how the actual speedup was measured .,evaluation
H1Mbr8b4f_11,Does it correspond to the seconds per update step or the overall training time ?,request
H1Mbr8b4f_12,"In the latter case , how long were models trained ?",request
H1Mbr8b4f_13,4 . How and which hyper-parameters were optimized ?,request
H1Mbr8b4f_14,"The authors should use the same hyper-parameters for all methods ( Jaderberg , Zhang , Rank selection ) .",request
H1Mbr8b4f_15,The authors should also analyze the sensitivity of speedup and accuracy drop depending on the learning rate for ‘ Rank selection ’ .,request
H1Mbr8b4f_16,5 . Figure 4 : the authors should show the same plot for more convolutional layers at varying depth from both VGG and ResNet .,request
H1Mbr8b4f_17,6 . The manuscript is hard to understand and not written clearly enough .,evaluation
H1Mbr8b4f_18,"In the abstract , what does ‘ two-pass decomposition ’ , ‘ proper ranks ’ , ‘ the instability problem ’ , or ‘ systematic ’ mean ?",non-arg
H1Mbr8b4f_19,"What are ‘ edge devices ’ , ‘ vanilla parameters ’ ?",non-arg
H1Mbr8b4f_20,"The authors should also avoid uninformative adjectives , clutter , and vague terms throughout the manuscript such as ‘ vital importance ’ or ‘ little room for fine-tuning ’ .",request
H1Mbr8b4f_21,Minor comments ============= 1 . The authors should use ‘ significantly ’ only if a statistical hypothesis was performed .,request
H1Mbr8b4f_22,"2 . The manuscript contains several typos and grammatical flaws ,",evaluation
H1Mbr8b4f_23,"e.g. ‘ have been widely applied to have the breakthrough ’ , ‘ The CP decomposition factorizes the tensors into a sum of series rank-one tensors . ’ , ‘ Our two-pass decomposition provides the better result as compared with the original CP decomposition ’ .",quote
H1Mbr8b4f_24,"3 . For clarity , the authors should express equation 5 in terms of Y_1 , Y_2 , Y_3 , and Y_4 .",request
H1Mbr8b4f_25,"4 . Equation 2 , bottom : C_in , W_f , H_f , and C_out are undefined at this point .",fact
SyFscqngM_0,This paper essentially uses CycleGANs for Domain Adaptation .,fact
SyFscqngM_1,My biggest concern is that it does n't adequately compare to similar papers that perform adaptation at the pixel level,evaluation
SyFscqngM_2,( eg . Shrivastava et al - ' Learning from Simulated and Unsupervised Images through Adversarial Training ',reference
SyFscqngM_3,"and Bousmalis et al - ' Unsupervised Pixel-level Domain Adaptation with GANs ' ,",reference
SyFscqngM_4,two similar papers published in CVPR 2017 - the first one was even a best paper - and available on arXiv since December 2016-before CycleGANs ) .,fact
SyFscqngM_5,I believe the authors should have at least done an ablation study to see if they cycle-consistency loss truly makes a difference on top of these works-that would be the biggest selling point of this paper .,request
SyFscqngM_6,"The experimental section had many experiments , which is great .",evaluation
SyFscqngM_7,However I think for semantic segmentation it would be very interesting to see whether using the adapted synthetic GTA5 samples would improve the SOTA on Cityscapes .,request
SyFscqngM_8,"It would n't be unsupervised domain adaptation ,",fact
SyFscqngM_9,but it would be very impactful .,evaluation
SyFscqngM_10,"Finally I 'm not sure the oracle ( train on target ) mIoU on Table 2 is SOTA ,",evaluation
SyFscqngM_11,and I believe the proposed model 's performance is really far from SOTA .,evaluation
SyFscqngM_12,Pros : * CycleGANs for domain adaptation !,evaluation
SyFscqngM_13,Great idea !,evaluation
SyFscqngM_14,"* I really like the work on semantic segmentation ,",evaluation
SyFscqngM_15,I think this is a very important direction,evaluation
SyFscqngM_16,Cons : * I do n't think Domain separation networks is a pixel-level transformation -,fact
SyFscqngM_17,"that 's a feature-level transformation ,",fact
SyFscqngM_18,you probably mean to use Bousmalis et al. 2017 .,evaluation
SyFscqngM_19,Also Shrivastava et al is missing from the image-level papers .,fact
SyFscqngM_20,"* the authors claim that Bousmalis et al , Liu & Tuzel and Shrivastava et al ahve only been shown to work for small image sizes .",fact
SyFscqngM_21,There 's a recent work by Bousmalis et al. ( Using Simulation and Domain Adaptation to Improve Efficiency of Deep Robotic Grasping ) that shows these methods working well ( w/o cycle-consistency ) for settings similar to semantic segmentation at a relatively high resolution .,fact
SyFscqngM_22,"Also it was mentioned that these methods do not necessarily preserve content , when pixel-da explicitly accounts for that with a task loss ( identical to the semantic loss used in this submission )",fact
SyFscqngM_23,"* The authors talk about the content similarity loss on the foreground in Bousmalis et al. 2017 ,",fact
SyFscqngM_24,"but they could compare to this method w/o using the content similarity or using a different content similarity tailored to the semantic segmentation tasks , which would be trivial .",evaluation
SyFscqngM_25,* Math seems wrong in ( 4 ) and ( 6 ) .,fact
SyFscqngM_26,( 4 ) should be probably have a minus instead of a plus .,evaluation
SyFscqngM_27,"( 6 ) has an argmin of a min ,",fact
SyFscqngM_28,not sure what is being optimized here .,evaluation
SyFscqngM_29,"In fact , I 'm not sure if eg you use the gradients of f_T for training the generators ?",evaluation
SyFscqngM_30,* The authors mention that the pixel-da approach cross validates with some labeled data .,fact
SyFscqngM_31,"Although I agree that is not an ideal validation ,",evaluation
SyFscqngM_32,"I 'm not sure if it 's equivalent or not the authors ' validation setting ,",evaluation
SyFscqngM_33,as they do n't describe what that is .,fact
SyFscqngM_34,"* The authors present the semantic loss as novel ,",fact
SyFscqngM_35,however this is the task loss proposed by the pixel-da paper .,fact
SyFscqngM_36,"* I did n't understand what pixel-only and feat-only meant in tables 2 , 3 , 4 .",fact
SyFscqngM_37,I could n't find an explanation in captions or in text,evaluation
ry9RWezWM_0,The authors purpose a method for creating mini batches for a student network by using a second learned representation space to dynamically selecting examples by their ' easiness and true diverseness ' .,fact
ry9RWezWM_1,The framework is detailed,evaluation
ry9RWezWM_2,"and results on MNIST , cifar10 and fashion-MNIST are presented .",fact
ry9RWezWM_3,The work presented is novel but there are some notable omissions :,evaluation
ry9RWezWM_4,- there are no specific numbers presented to back up the improvement claims ;,fact
ry9RWezWM_5,graphs are presented but not specific numeric results,fact
ry9RWezWM_6,- there is limited discussion of the computational cost of the framework presented,evaluation
ry9RWezWM_7,- there is no comparison to a baseline in which the additional learning cycles used for learning the embedding are used for training the student model .,fact
ry9RWezWM_8,- only small data sets are evaluated .,fact
ry9RWezWM_9,This is unfortunate,evaluation
ry9RWezWM_10,"because if there are to be large gains from this approach , it seems that they are more likely to be found in the domain of large scale problems , than toy data sets like mnist .",evaluation
By-CxBKgz_0,"This paper presents Defense-GAN : a GAN that used at test time to map the input generate an image <VAR> close ( in <VAR> ) to the input image <VAR> , by applying several steps of gradient descent of this MSE .",fact
By-CxBKgz_1,The GAN is a WGAN trained on the train set ( only to keep the generator ) .,fact
By-CxBKgz_2,"The goal of the whole approach is to be robust to adversarial examples , without having to change the ( downstream task ) classifier , only swapping in the <VAR> for the x.",fact
By-CxBKgz_3,+ The paper is easy to follow .,evaluation
By-CxBKgz_4,+ It seems ( but I am not an expert in adversarial examples ) to cite the relevant litterature ( that I know of ) and compare to reasonably established attacks and defenses .,fact
By-CxBKgz_5,"+ Simple/directly applicable approach that seems to work experimentally ,",fact
By-CxBKgz_6,but - A missing baseline is to take the nearest neighbour of the ( perturbed ) x from the training set .,fact
By-CxBKgz_7,"- Only MNIST-sized images , and MNIST-like ( 60k train set , 10 labels ) datasets : MNIST and F-MNIST .",fact
By-CxBKgz_8,- Between 0.043 sec and 0.825 sec to reconstruct an MNIST-sized image .,fact
By-CxBKgz_9,"? MagNet results were very often worse than no defense in Table 4 ,",fact
By-CxBKgz_10,could you comment on that ?,non-arg
By-CxBKgz_11,"- In white-box attacks , it seems to me like L steps of gradient descent on <VAR> should be directly extended to L steps of ( at least ) FGSM-based attacks , at least as a control .",request
H1rLr8ZNM_0,"This paper proposed to combine three kinds of data sources : real , simulated and unlabeled , to help solve "" small "" data issue occurring in packet stream .",fact
H1rLr8ZNM_1,"A directed information flow graph was constructed ,",fact
H1rLr8ZNM_2,a multi-headed network was trained by using Keras and GAN library .,fact
H1rLr8ZNM_3,Its use on the packet sequence classification can archive comparable accuracy while relieve operation engineers from heavy background learning .,evaluation
H1rLr8ZNM_4,The presentation of this paper can be improved .,request
H1rLr8ZNM_5,"* With the missing citations as "" ( ? ) "" and not clearly defined concepts , including property of function H ( any function ? convex ? ) in ( 3 ) ,",request
H1rLr8ZNM_6,"full name of TCP/abbr of GAN when first appear , etc .",request
H1rLr8ZNM_7,reader might need to make guesses to follow .,evaluation
H1rLr8ZNM_8,"* P2 : You can draw your audience by expanding the "" related work "" like a story :",request
H1rLr8ZNM_9,more background of GAN etc. and one or two highlight formula to help clear the idea,request
H1rLr8ZNM_10,"* P3 : What 's the purpose of inserting "" dummy packets to denote the timeout between two time stamps "" ?",non-arg
H1rLr8ZNM_11,"* P3 : Help sell to "" non-engineer "" by maybe having image example or even plainer language to describe the meaning ( deep difference/purpose ) of "" 3 levels of feature engineering "" ; and when addressing features , mentioned as 1,2,3 , while in Table 1 , shown as Feature = 0,1,2 ;",request
H1rLr8ZNM_12,"* P6 : section 4.2 mentioned "" only metrics cared by operators "" , is this what you mean by "" relieve operation engineers ... "" ,",non-arg
H1rLr8ZNM_13,and which is or how to decide the cutoff accuracy the engineers should make a Go or No Go decision ?,non-arg
B1pcOYBlG_0,Quality This paper demonstrates that convolutional and relational neural networks fail to solve visual relation problems by training networks on artificially generated visual relation data .,fact
B1pcOYBlG_1,This points at important limitations of current neural network architectures where architectures depend mainly on rote memorization .,evaluation
B1pcOYBlG_2,Clarity The rationale in the paper is straightforward .,evaluation
B1pcOYBlG_3,I do think that breakdown of networks by testing on increasing image variability is expected given that there is no reason that networks should generalize well to parts of input space that were never encountered before .,evaluation
B1pcOYBlG_4,"Originality While others have pointed out limitations before ,",fact
B1pcOYBlG_5,this paper considers relational networks for the first time .,fact
B1pcOYBlG_6,"Significance This work demonstrates failures of relational networks on relational tasks ,",fact
B1pcOYBlG_7,which is an important message .,evaluation
B1pcOYBlG_8,"At the same time , no new architectures are presented to address these limitations .",fact
B1pcOYBlG_9,Pros Important message about network limitations .,evaluation
B1pcOYBlG_10,Cons Straightforward testing of network performance on specific visual relation tasks .,evaluation
B1pcOYBlG_11,No new theory development .,fact
B1pcOYBlG_12,Conclusions drawn by testing on out of sample data may not be completely valid .,fact
S1EAO5qxM_0,"In the centre loss , the centre is learned .",fact
S1EAO5qxM_1,Now it 's calculated as the average of the last layer 's features,fact
S1EAO5qxM_2,"To enable training with SGD , the authors calculate the centre within a mini batch",fact
Sy7QPPYxM_0,The paper proposes an adaptation of existing Graph ConvNets and evaluates this formulation on a several existing benchmarks of the graph neural network community .,fact
Sy7QPPYxM_1,"In particular , a tree structured LSTM is taken and modified .",fact
Sy7QPPYxM_2,"The authors describe this as adapting it to general graphs , stacking , followed by adding edge gates and residuality .",fact
Sy7QPPYxM_3,"My biggest concern is novelty , as the modifications are minor .",evaluation
Sy7QPPYxM_4,"In particular , the formulation can be seen in a different way .",fact
Sy7QPPYxM_5,"As I see it , instead of adapting Tree LSTMs to arbitary graphs , it can be seen as taking the original formulation by Scarselli and replacing the RNN by a gated version , i.e. adding the known LSTM gates ( input , output , forget gate ) .",fact
Sy7QPPYxM_6,This is a minor modification .,evaluation
Sy7QPPYxM_7,"Adding stacking and residuality are now standard operations in deep learning ,",fact
Sy7QPPYxM_8,"and edge-gates have also already been introduced in the literature , as described in the paper .",fact
Sy7QPPYxM_9,"A second concern is the presentation of the paper , which can be confusing at some points .",evaluation
Sy7QPPYxM_10,A major example is the mathematical description of the methods .,evaluation
Sy7QPPYxM_11,"When reading the description as given , one should actually infer that Graph ConvNets and Graph RNNs are the same thing , which can be seen by the fact that equations ( 1 ) and ( 6 ) are equivalent .",evaluation
Sy7QPPYxM_12,"Another example , after ( 2 ) , the important point to raise is the difference to classical ( sequential ) RNNs , namely the fact that the dependence graph of the model is not a DAG anymore , which introduces cyclic dependencies .",evaluation
Sy7QPPYxM_13,"Generally , a clear introduction of the problem is also missing .",evaluation
Sy7QPPYxM_14,"What are the inputs ,",request
Sy7QPPYxM_15,"what are the outputs ,",request
Sy7QPPYxM_16,what kind of problems should be solved ?,request
Sy7QPPYxM_17,"The update equations for the hidden states are given for all models ,",fact
Sy7QPPYxM_18,but how is the output calculated given the hidden states from variable numbers of nodes of an irregular graph ?,request
Sy7QPPYxM_19,"The model has been evaluated on standard datasets with a performance ,",fact
Sy7QPPYxM_20,"which seems to be on par , or a slight edge , which could probably be due to the newly introduced residuality .",evaluation
Sy7QPPYxM_21,A couple of details : - the length of a graph is not defined . The size of the set of nodes might be meant .,evaluation
Sy7QPPYxM_22,- at the beginning of section 2.1 I do not understand the reference to word prediction and natural language processing .,evaluation
Sy7QPPYxM_23,RNNs are not restricted to NLP,evaluation
Sy7QPPYxM_24,and I think there is no need to introduce an application at this point .,request
Sy7QPPYxM_25,"- It is unclear what does the following sentence means : "" ConvNets are more pruned to deep networks than RNNs "" ?",evaluation
Sy7QPPYxM_26,"- What are "" heterogeneous graph domains "" ?",request
H1_YBgxZz_0,This paper presents a method for clustering based on latent representations learned from the classification of transformed data after pseudo-labellisation corresponding to applied transformation .,fact
H1_YBgxZz_1,Pipeline : - Data are augmented with domain-specific transformations .,fact
H1_YBgxZz_2,"For instance , in the case of MNIST , rotations with different degrees are applied .",fact
H1_YBgxZz_3,"All data are then labelled as "" original "" or "" transformed by ... ( specific transformation ) "" .",fact
H1_YBgxZz_4,- Classification task is performed with a neural network on augmented dataset according to the pseudo-labels .,fact
H1_YBgxZz_5,"- In parallel of the classification , the neural network also learns the latent representation in an unsupervised fashion .",fact
H1_YBgxZz_6,- k-means clustering is performed on the representation space observed in the hidden layer preceding the augmented softmax layer .,fact
H1_YBgxZz_7,Detailed Comments : ( * ) Pros - The method outperforms the state-of-art regarding unsupervised methods for handwritten digits clustering on MNIST .,fact
H1_YBgxZz_8,"- Use of ACOL and GAR is interesting , also the idea to make "" labeled "" data from unlabelled ones by using data augmentation .",evaluation
H1_YBgxZz_9,"( * ) Cons - minor : in the title , I find the expression "" unsupervised clustering "" uselessly redundant since clustering is by definition unsupervised .",evaluation
H1_YBgxZz_10,- Choice of datasets : we already obtained very good accuracy for the classification or clustering of handwritten digits .,evaluation
H1_YBgxZz_11,This is not a very challenging task .,evaluation
H1_YBgxZz_12,"And just because something works on MNIST , does not mean it works in general .",fact
H1_YBgxZz_13,"What are the performances on more challenging datasets like colored images ( CIFAR-10 , labelMe , ImageNet , etc. ) ?",request
H1_YBgxZz_14,- This is not clear what is novel here,evaluation
H1_YBgxZz_15,since ACOL and GAR already exist .,fact
H1_YBgxZz_16,The novelty seems to be in the adaptation to GAR from the semi-supervised to the unsupervised setting with labels indicating if data have been transformed or not .,evaluation
H13MWgq4M_0,"This paper identifies and proposes a fix for a shortcoming of the Deep Information Bottleneck approach , namely that the induced representation is not invariant to monotonic transform of the marginal distributions ( as opposed to the mutual information on which it is based ) .",fact
H13MWgq4M_1,"The authors address this shortcoming by applying the DIB to a transformation of the data , obtained by a copula transform .",fact
H13MWgq4M_2,"This explicit approach is shown on synthetic experiments to preserve more information about the target , yield better reconstruction and converge faster than the baseline .",fact
H13MWgq4M_3,"The authors further develop a sparse extension to this Deep Copula Information Bottleneck ( DCIB ) , which yields improved representations ( in terms of disentangling and sparsity ) on a UCI dataset .",fact
H13MWgq4M_4,( significance ) This is a promising idea .,evaluation
H13MWgq4M_5,"This paper builds on the information theoretic perspective of representation learning ,",fact
H13MWgq4M_6,and makes progress towards characterizing what makes for a good representation .,fact
H13MWgq4M_7,"Invariance to transforms of the marginal distributions is clearly a useful property ,",evaluation
H13MWgq4M_8,and the proposed method seems effective in this regard .,evaluation
H13MWgq4M_9,"Unfortunately , I do not believe the paper is ready for publication as it stands ,",evaluation
H13MWgq4M_10,as it suffers from lack of clarity and the experimentation is limited in scope .,evaluation
H13MWgq4M_11,( clarity ) While Section 3.3 clearly defines the explicit form of the algorithm,evaluation
H13MWgq4M_12,"( where data and labels are essentially pre-processed via a copula transform ) ,",fact
H13MWgq4M_13,details regarding the “ implicit form ” are very scarce .,evaluation
H13MWgq4M_14,"From Section 3.4 , it seems as though the authors are optimizing the form of the gaussian information bottleneck <VAR> , in the hopes of recovering an encoder <VAR> which gaussianizes the input ( thus emulating the explicit transform ) ?",fact
H13MWgq4M_15,"Could the authors clarify whether this interpretation is correct , or alternatively provide additional clarifying details ?",request
H13MWgq4M_16,There are also many missing details in the experimental section :,evaluation
H13MWgq4M_17,how were the number of “ active ” components selected ?,request
H13MWgq4M_18,Which versions of the algorithm ( explicit/implicit ) were used for which experiments ?,request
H13MWgq4M_19,"I believe explicit was used for Section 4.1 , and implicit for 4.2",fact
H13MWgq4M_20,but again this needs to be spelled out more clearly .,request
H13MWgq4M_21,"I would also like to see a discussion ( and perhaps experimental comparison ) to standard preprocessing techniques , such as PCA-whitening .",request
H13MWgq4M_22,( quality ) The experiments are interesting and seem well executed .,evaluation
H13MWgq4M_23,"Unfortunately , I do not think their scope ( single synthetic , plus a single UCI dataset ) is sufficient .",evaluation
H13MWgq4M_24,"While the gap in performance is significant on the synthetic task ,",evaluation
H13MWgq4M_25,this gap appears to shrink significantly when moving to the UCI dataset .,fact
H13MWgq4M_26,"How does this method perform for more realistic data , even e.g. MNIST ?",request
H13MWgq4M_27,"I think it is crucial to highlight that the deficiencies of DIB matter in practice , and are not simply a theoretical consideration .",request
H13MWgq4M_28,"Similarly , the representation analyzed in Figure 7 is promising ,",evaluation
H13MWgq4M_29,"but again the authors could have targeted other common datasets for disentangling , e.g. the simple sprites dataset used in the beta-VAE paper .",request
H13MWgq4M_30,I would have also liked to see a more direct and systemic validation of the claims made in the paper .,request
H13MWgq4M_31,"For example , the shortcomings of DIB identified in Section 3.1 , 3.2 could have been verified more directly by plotting <VAR> for various monotonic transformations of x.",request
H13MWgq4M_32,A direct comparison of the explicit and implicit forms of the algorithms would also also make for a stronger paper in my opinion .,request
H13MWgq4M_33,Pros : * Theoretically well motivated,evaluation
H13MWgq4M_34,* Promising results on synthetic task,evaluation
H13MWgq4M_35,* Potential for impact,evaluation
H13MWgq4M_36,Cons : * Paper suffers from lack of clarity ( method and experimental section ),evaluation
H13MWgq4M_37,* Lack of ablative / introspective experiments,fact
H13MWgq4M_38,* Weak empirical results ( small or toy datasets only ) .,evaluation
HJ3OcT3gG_0,"In this paper , the authors propose a novel method for generating adversarial examples when the model is a black-box and we only have access to its decisions ( and a positive example ) .",fact
HJ3OcT3gG_1,It iteratively takes steps along the decision boundary while trying to minimize the distance to the original positive example .,fact
HJ3OcT3gG_2,Pros : - Novel method that works under much stricter and more realistic assumptions .,evaluation
HJ3OcT3gG_3,- Fairly thorough evaluation .,evaluation
HJ3OcT3gG_4,- The paper is clearly written .,evaluation
HJ3OcT3gG_5,Cons : - Need a fair number of calls to generate a small perturbation .,fact
HJ3OcT3gG_6,Would like to see more analysis of this .,request
HJ3OcT3gG_7,"- Attack works for making something outside the boundary ( not X ) ,",fact
HJ3OcT3gG_8,but is less clear how to generate image to meet a specific classification ( X ) .,evaluation
HJ3OcT3gG_9,"3.2 attempts this slightly by using an image in the class ,",fact
HJ3OcT3gG_10,but is less clear for something like FaceID .,evaluation
HJ3OcT3gG_11,- Unclear how often the images generated look reasonable .,evaluation
HJ3OcT3gG_12,Do different random initializations given different quality examples ?,request
Bycjn6tef_0,Paper summary : Existing works on multi-task neural networks typically use hand-tuned weights for weighing losses across different tasks .,fact
Bycjn6tef_1,This work proposes a dynamic weight update scheme that updates weights for different task losses during training time by making use of the loss ratios of different tasks .,fact
Bycjn6tef_2,Experiments on two different network indicate that the proposed scheme is better than using hand-tuned weights for multi-task neural networks .,fact
Bycjn6tef_3,Paper Strengths : - The proposed technique seems simple yet effective for multi-task learning .,evaluation
Bycjn6tef_4,- Experiments on two different network architectures showcasing the generality of the proposed method .,fact
Bycjn6tef_5,Major Weaknesses : - The main weakness of this work is the unclear exposition of the proposed technique .,evaluation
Bycjn6tef_6,Entire technique is explained in a short section-3.1 with many important details missing .,evaluation
Bycjn6tef_7,There is no clear basis for the main equations 1 and 2 .,evaluation
Bycjn6tef_8,How does equation-2 follow from equation-1 ?,non-arg
Bycjn6tef_9,Where is the expectation coming from ?,non-arg
Bycjn6tef_10,What exactly does ‘ F ’ refer to ?,non-arg
Bycjn6tef_11,There is dependency of ‘ F ’ on only one of sides in equations 1 and 2 ?,non-arg
Bycjn6tef_12,"More importantly , how does the gradient normalization relate to loss weight update ?",non-arg
Bycjn6tef_13,It is very difficult to decipher these details from the short descriptions given in the paper .,evaluation
Bycjn6tef_14,"- Also , several details are missing in toy experiments .",fact
Bycjn6tef_15,What is the task here ?,non-arg
Bycjn6tef_16,What are input and output distributions and what is the relation between input and output ?,non-arg
Bycjn6tef_17,Are they just random noises ?,non-arg
Bycjn6tef_18,"If so , is the network learning to overfit to the data as there is no relationship between input and output ?",non-arg
Bycjn6tef_19,Minor Weaknesses : - There are no training time comparisons between the proposed technique and the standard fixed loss learning .,fact
Bycjn6tef_20,- Authors claim that they operate directly on the gradients inside the network .,fact
Bycjn6tef_21,"But , as far as I understood , the authors only update loss weights in this paper .",fact
Bycjn6tef_22,Did authors also experiment with gradient normalization in the intermediate CNN layers ?,non-arg
Bycjn6tef_23,- No comparison with state-of-the-art techniques on the experimented tasks and datasets .,fact
Bycjn6tef_24,Clarifications : - See the above mentioned issues with the exposition of the technique .,non-arg
Bycjn6tef_25,"- In the experiments , why are the input images downsampled to 320x320 ?",non-arg
Bycjn6tef_26,- What does it mean by ‘ unofficial dataset ’ ( page-4 ) .,non-arg
Bycjn6tef_27,Any references here ?,non-arg
Bycjn6tef_28,- Why is ' task normalized ' test-time loss as good measure for comparison between models in the toy example ( Section 4 ) ?,non-arg
Bycjn6tef_29,"The loss ratios depend on initial loss ,",fact
Bycjn6tef_30,which is not important for the final performance of the system .,evaluation
Bycjn6tef_31,Suggestions : - I strongly suggest the authors to clearly explain the proposed technique to get this into a publishable state .,request
Bycjn6tef_32,- The term ’ GradNorm ’ seem to be not defined anywhere in the paper .,fact
Bycjn6tef_33,"Review Summary : Despite promising results , the proposed technique is quite unclear from the paper .",evaluation
Bycjn6tef_34,"With its poor exposition of the technique , it is difficult to recommend this paper for publication .",evaluation
SksrEW9eG_0,Summary : The paper proposes a new dialog model combining both retrieval-based and generation-based modules .,fact
SksrEW9eG_1,"Answers are produced in three phases : a retrieval-based model extracts candidate answers ; a generator model , conditioned on retrieved answers , produces an additional candidate ; a reranker outputs the best among all candidates .",fact
SksrEW9eG_2,The approach is interesting :,evaluation
SksrEW9eG_3,"the proposed ensemble can improve on both the retrieval module and the generation module ,",fact
SksrEW9eG_4,since it does not restrict modeling power ( e.g. the generator is not forced to be consistent with the candidates ) .,fact
SksrEW9eG_5,I am not aware of similar approaches for this task .,evaluation
SksrEW9eG_6,One work that comes to mind regarding the blend of retrieval and generation is Memory Networks,evaluation
SksrEW9eG_7,( e.g. <URL> and references ) :,reference
SksrEW9eG_8,"given a query , a set of relevant memories is extracted from a KB using an inverted index and the memories are fed into the generator .",fact
SksrEW9eG_9,"However , the extracted items in the current work are candidate answers which are used both to feed the generator and to participate in reranking .",fact
SksrEW9eG_10,The experimental section focuses on the task of building conversational systems .,fact
SksrEW9eG_11,The performance measures used are 1 ) a human evaluation score with three volunteers and 2 ) BLUE scores .,fact
SksrEW9eG_12,"While these methods are not very satisfying ,",evaluation
SksrEW9eG_13,effective evaluation of such systems is a known difficulty .,evaluation
SksrEW9eG_14,"The results show that the ensemble outperforms the individual modules , indicating that :",fact
SksrEW9eG_15,the multi-seq2seq models have learned to use the new inputs as needed and that the ranker is correlated with the evaluation metrics .,fact
SksrEW9eG_16,"However , the results themselves do not look impressive to me :",evaluation
SksrEW9eG_17,"the subjective evaluation is close to the "" borderline "" score ;",evaluation
SksrEW9eG_18,"in the examples provided , one is good , the other is borderline/bad , and the baseline always provides something very short .",fact
SksrEW9eG_19,Does the LSTM work particularly poor on this dataset ?,request
SksrEW9eG_20,"Given that this is a novel dataset , I do n't know what the state-of-the-art should be .",evaluation
SksrEW9eG_21,Could you provide more insight ?,request
SksrEW9eG_22,Have you considered adding a benchmark dataset ( e.g. a QA dataset ) ?,request
SksrEW9eG_23,Specific questions : 1 . The paper motivates conditioning on the candidates in two ways .,fact
SksrEW9eG_24,"First , that the candidates bring additional information which the decoder can use ( e.g. read from the candidates locations , actions , etc. ) .",fact
SksrEW9eG_25,"Second , that the probability of universal replies must decrease due to the additional condition .",fact
SksrEW9eG_26,I think the second argument depends on how the conditioning is performed :,fact
SksrEW9eG_27,"if the candidates are simply appended to the input , the model can learn to ignore them .",fact
SksrEW9eG_28,"2 . The copy mechanism is a nice touch , encouraging the decoder to use the provided queries .",evaluation
SksrEW9eG_29,"Why not copy from the query too , e.g. with some answers reusing part of the query < "" Where are you going ? "" , "" I 'm going to the park "" > ?",request
SksrEW9eG_30,3 . How often does the model select the generated answer vs. the extracted answers ?,request
SksrEW9eG_31,In both examples provided the selected answer is the one merging the candidate answers .,fact
SksrEW9eG_32,Minor issues : - Section 3.2 : using and the state,fact
SksrEW9eG_33,- Section 3.2 : more than one replies,fact
SksrEW9eG_34,"- last sentence on page 3 : what are the "" following principles "" ?",request
Hk0lS3teG_0,The authors analyze show theoretical shortcomings in previous methods of explaining neural networks and propose an elegant way to remove these shortcomings in their methods PatternNet and PatternAttribution .,fact
Hk0lS3teG_1,The quest of visualizing neural network decision is now a very active field with many contributions .,evaluation
Hk0lS3teG_2,The contribution made by the authors stands out due to its elegant combination of theoretical insights and improved performance in application .,evaluation
Hk0lS3teG_3,The work is very detailed and reads very well .,evaluation
Hk0lS3teG_4,I am missing at least one figure with comparison with more state-of-the-art methods,request
Hk0lS3teG_5,"( e.g. I would love to see results from the method by Zintgraf et al. 2017 which unlike all included prior methods seems to produce much crisper visualizations and also is very related because it learns from the data , too ) .",request
Hk0lS3teG_6,Minor questions and comments : * Fig 3 : Why is the random method so good at removing correlation from fc6 ?,request
Hk0lS3teG_7,And the S_w even better ?,request
Hk0lS3teG_8,Something seems special about fc6 .,evaluation
Hk0lS3teG_9,* Fig 4 : Why is the identical estimator better than the weights estimator and that one better than S_a ?,request
Hk0lS3teG_10,* It would be nice to compare the image degradation experiment with using the ranking provided by the work from Zintgraf which should by definition function as a kind of gold standard,request
Hk0lS3teG_11,"* Figure 5 , 4th row ( mailbox ) : It looks like the umbrella significantly contributes to the network decision to classify the image as "" mailbox "" which does n't make too much sense .",evaluation
Hk0lS3teG_12,"Is is a problem of the visualization ( maybe there is next to no weight on the umbrella ) , of PatternAttribution or a strange but interesting a artifact of the analyzed network ?",evaluation
Hk0lS3teG_13,"* page 8 "" ... closed form solutions ( Eq ( 4 ) and Eq . ( 7 ) ) """,quote
Hk0lS3teG_14,The first reference seems to be wrong .,fact
Hk0lS3teG_15,I guess Eq 4 . should instead reference the unnumbered equation after Eq . 3 .,request
r1ke1YDlz_0,SIGNIFICANCE AND ORIGINALITY : The authors propose to accelerate the learning of complex tasks by exploiting traces of experts .,fact
r1ke1YDlz_1,"Unlike the most common form of imitation learning or behavioral cloning , the authors formulate their solution in the case where the expert ’s state trajectory is observable , but the expert ’s actions are not .",fact
r1ke1YDlz_2,This is an important and useful problem in robotics and other applications .,evaluation
r1ke1YDlz_3,"Within this specific setting the authors differentiate their approach from others by developing a solution that does NOT estimate an explicit dynamics model ( e.g. , <VAR> ) .",fact
r1ke1YDlz_4,The benefits of not estimating an explicit action model are not really demonstrated in a clear way .,evaluation
r1ke1YDlz_5,The author ’s articulate a specific solution that provides heuristic guidance rewards that cause the learner to favor actions that achieve subgoals calculated from expert behavior and refactors the representation of the Q function so that it has a component that is a function of the subgoal extracted from the expert .,fact
r1ke1YDlz_6,These subgoals are linear functions of the expert ’s change in state ( or change in state features ) .,fact
r1ke1YDlz_7,The resultant policy is a function of the expert traces on which it depends .,fact
r1ke1YDlz_8,The authors show they can retrain a new policy that does not require the expert traces .,fact
r1ke1YDlz_9,"As far as I am aware , this is a novel approach to the problem .",evaluation
r1ke1YDlz_10,The authors claim that this factorization is important and useful,fact
r1ke1YDlz_11,but the paper does n’t really illustrate this well .,evaluation
r1ke1YDlz_12,They demonstrate the usefulness of the algorithm against a DQN baseline on Doom game problems .,fact
r1ke1YDlz_13,The algorithm learns faster than unassisted DQN as shown by learning curve plots .,fact
r1ke1YDlz_14,"They also evaluate the algorithms on the quality of the final policies for their approach , DQN , and a supervised learning from demonstration approach ( LfD ) that requires expert actions .",fact
r1ke1YDlz_15,The proposed approach does as well or better than competing approaches .,fact
r1ke1YDlz_16,QUALITY Ablation studies show that the guidance rewards are important to achieving the improved performance of the proposed method which is important confirmation that the architecture is working in the intended way .,evaluation
r1ke1YDlz_17,"However , it would also be useful to do an ablation study of the “ factorization ” of action values .",request
r1ke1YDlz_18,Is this important to achieving better results as well or is the guidance reward enough ?,request
r1ke1YDlz_19,This seems like a key claim to establish .,evaluation
r1ke1YDlz_20,CLARITY The details of the memory based kernel density estimation and neural gradient training seemed complicated by the way that the process was implemented .,evaluation
r1ke1YDlz_21,Is it possible to communicate the intuitions behind what is going on ?,request
r1ke1YDlz_22,"I was able to work out the intuitions behind the heuristic rewards , but I still do n’t clearly get what the Q-value factorization is providing :",evaluation
r1ke1YDlz_23,"To keep my text readable , I assume we are working in feature space instead of state space and use different letters for learner and expert :",non-arg
r1ke1YDlz_24,Learner : <EQN> Expert ’s i ^ th state visit : <EQN> where Ei ’ is the successor state to Ei,non-arg
r1ke1YDlz_25,The paper builds upon approximate n-step discrete-action Q-learning where the Q value for an action is a linear function of the state features : <EQN> where parameters <EQN> .,evaluation
r1ke1YDlz_26,"After observing an experience ( S , A , R , S ’ ) we use Bellman Error as a loss function to optimize Qp for parameter p.",fact
r1ke1YDlz_27,I ignore the complexities of n-step learning and discount factors for clarity .,non-arg
r1ke1YDlz_28,<EQN>,fact
r1ke1YDlz_29,"The authors suggest we can augment the environment reward R with a heuristic reward Rh proportional to the similarity between the learner “ subgoal "" and the expert “ subgoal "" in similar states .",fact
r1ke1YDlz_30,The authors propose to use cosine distance between representations of what they call the “ subgoals ” of learner and expert .,fact
r1ke1YDlz_31,A subgoal is defined as a linear transformation of the distance traveled by an agent during a transition .,fact
r1ke1YDlz_32,"The heuristic reward is proportional to the cosine distance between the learner and expert “ subgoals "" <EQN> The learner ’s direction in state S is just ( S-S ’ ) in feature space .",fact
r1ke1YDlz_33,The authors model the behavior of the expert as a kernel density type approximator giving the expected direction of the expert starting from a states similar to the one the learner is in .,fact
r1ke1YDlz_34,"Let < Wk S , Wk Ej > be a weighted similarity between learner state features S and expert state features Ej and Ej ’ be the successor state features encountered by the expert .",fact
r1ke1YDlz_35,"Then the expected expert direction for learner state S is : SUMj < Wk S , Wk Ej > ( Ej - Ej ’ )",fact
r1ke1YDlz_36,Presumably the linear Wk transform helps us pick out the important dimensions of similarity between S and Ej .,fact
r1ke1YDlz_37,"Mapping the learner and expert directions into subgoal space using Wv , the heuristic reward is <VAR>",fact
r1ke1YDlz_38,"I ignore the ReLU here , but I assume that is operates element-wise and just clips negative values ?",fact
r1ke1YDlz_39,There is only one layer here,fact
r1ke1YDlz_40,so we do n’t have complex non-linear things going on ?,evaluation
r1ke1YDlz_41,"In addition to introducing a heuristic reward term , the authors propose to alter the Q-function to be specific to the subgoal .",fact
r1ke1YDlz_42,<EQN>,fact
r1ke1YDlz_43,"The subgoal is the same as the first part , namely a linear transform of the expected expert direction in states similar to state S.",fact
r1ke1YDlz_44,<EQN>,fact
r1ke1YDlz_45,"So in some sense , the Q function is really just a function of S , as g is calculated from S.",fact
r1ke1YDlz_46,<VAR>,fact
r1ke1YDlz_47,So this allows the Q-function more flexibility to capture each subgoal in a different linear space ?,request
r1ke1YDlz_48,I do n’t really get the intuition behind this formulation .,evaluation
r1ke1YDlz_49,It allows the subgoal to adjust the value of the underlying model ?,request
r1ke1YDlz_50,Essentially the expert defines a new Q-value problem at every state for the learner ?,fact
r1ke1YDlz_51,In some sense are we are defining a model for the action taken by the expert ?,request
r1ke1YDlz_52,"ADDITIONAL THOUGHTS While the authors compare to an unassisted baseline , they do n’t compare to methods that use an action model",fact
r1ke1YDlz_53,which is not a fatal flaw but would have been nice .,evaluation
r1ke1YDlz_54,"One can imagine there might be scenarios where the local guidance rewards of this form could be problematic , particularly in scenarios where the expert and learner are not identical",evaluation
r1ke1YDlz_55,"and it is possible to return to previous states , such as the grid worlds the authors discuss :",fact
r1ke1YDlz_56,"If the expert ’s first few transitions were easily approximable , the learner would get local rewards that cause it to mimic expert behavior .",evaluation
r1ke1YDlz_57,"However , if the next step in the expert ’s path was difficult to approximate , then the reward for imitating the expert would be lower .",fact
r1ke1YDlz_58,Would the learner then just prefer to go back towards those states that it can approximate and endlessly loop ?,request
r1ke1YDlz_59,"In this case , perhaps expressing heuristic rewards as potentials as described in Ng ’s shaping paper might solve the problem .",request
r1ke1YDlz_60,PROS AND CONS Important problem generally .,evaluation
r1ke1YDlz_61,"Avoiding the estimation of a dynamics model was stated as a given , but perhaps more could be put into motivating this goal .",request
r1ke1YDlz_62,Hopefully it is possible to streamline the methodology section to communicate the intuitions more easily .,request
ryjxrEwlM_0,The authors propose a mechanism for learning task-specific region embeddings for use in text classification .,fact
ryjxrEwlM_1,"Specifically , this comprises a standard word embedding an accompanying local context embedding .",fact
ryjxrEwlM_2,"The key idea here is the introduction of a <VAR> tensor K , where h is the embedding dim ( same as the word embedding size ) , c is a fixed window size around a target word , and v is the vocabulary size .",fact
ryjxrEwlM_3,"Each word in v is then associated with an <VAR> matrix that is meant to encode how it affects nearby words ,",fact
ryjxrEwlM_4,in particular this may be viewed as parameterizing a projection to be applied to surrounding word embeddings .,evaluation
ryjxrEwlM_5,"The authors propose two specific variants of this approach , which combine the K matrix and constituent word embeddings ( in a given region ) in different ways .",fact
ryjxrEwlM_6,Region embeddings are then composed ( summed ) and fed through a standard model .,fact
ryjxrEwlM_7,Strong points --- + The proposed approach is simple and largely intuitive :,evaluation
ryjxrEwlM_8,essentially the context matrix allows word-specific contextualization .,fact
ryjxrEwlM_9,"Further , the work is clearly presented .",evaluation
ryjxrEwlM_10,"+ At the very least the model does seem comparable in performance to various recent methods ( as per Table 2 ) ,",evaluation
ryjxrEwlM_11,however as noted below the gains are marginal,fact
ryjxrEwlM_12,and I have some questions on the setup .,evaluation
ryjxrEwlM_13,"+ The authors perform ablation experiments ,",fact
ryjxrEwlM_14,which are always nice to see .,evaluation
ryjxrEwlM_15,Weak points --- - I have a critical question for clarification in the experiments .,evaluation
ryjxrEwlM_16,"The authors write ' Optimal hyperparameters are tuned with 10 % of the training set on Yelp Review Full dataset , and identical hyperparameters are applied to all datasets '",fact
ryjxrEwlM_17,"-- is this true for * all * models , or only the proposed approach ?",request
ryjxrEwlM_18,"- The gains here appear to be consistent ,",fact
ryjxrEwlM_19,but they seem marginal .,fact
ryjxrEwlM_20,"The biggest gain achieved over all datasets is apparently .7 ,",fact
ryjxrEwlM_21,and most of the time the model very narrowly performs better ( .2-.4 range ) .,fact
ryjxrEwlM_22,"Moreoever , it is not clear if these results are averaged over multiple runs of SGD or not",evaluation
ryjxrEwlM_23,( variation due to initialization and stochastic estimation can account for up to 1 point in variance,fact
ryjxrEwlM_24,"-- see "" A sensitivity analysis of ( and practitioners guide to ) CNNs ... "" Zhang and Wallace , 2015 . )",reference
ryjxrEwlM_25,- The related work section seems light .,evaluation
ryjxrEwlM_26,"For instance , there is no discussion at all of LSTMs and their application to text classificatio ( e.g. , Tang et al. , EMNLP 2015 )",fact
ryjxrEwlM_27,"-- although it is noted that the authors do compare against D-LSTM , or char-level CNNs for the same ( see Zhang et al. , NIPs 2015 ) .",fact
ryjxrEwlM_28,Other relevant work not discussed includes Iyyer et al. ( ACL 2015 ) .,fact
ryjxrEwlM_29,"In their respective ways , these papers address some of the same issues the authors consider here .",fact
ryjxrEwlM_30,"- The two approaches to inducing the final region embedding ( word-context and then context-word in sections 3.2 and 3.3 , respectively ) feel a bit ad-hoc .",evaluation
ryjxrEwlM_31,I would have appreciated more intuition behind these approaches .,evaluation
ryjxrEwlM_32,"Small comments --- There is a typo in Figure 4 -- "" Howerver "" should be "" However """,request
SyJaBw1eG_0,Summary : The paper considers second-order optimization methods for training of neural networks .,fact
SyJaBw1eG_1,"In particular , the contribution of the paper is a Hessian-free method that works on blocks of parameters",fact
SyJaBw1eG_2,"( this is a user defined splitting of the parameters in blocks , e.g. , parameters of each layer is one block , or parameters in several layers could constitute a block ) .",fact
SyJaBw1eG_3,"This results into a block-diagonal approximation to the curvature matrix , in order to improve Hessian-free convergence properties :",fact
SyJaBw1eG_4,"in the latter , a single step might require many CG steps ,",fact
SyJaBw1eG_5,so the benefit from using second-order information is not apparent .,evaluation
SyJaBw1eG_6,"This is mainly an experimental work ,",fact
SyJaBw1eG_7,"where the authors show the merits of their approach on deep autoencoders , convolutional networks and LSTMs :",fact
SyJaBw1eG_8,results show favourable performance compared to the original Hessian-free approach and the Adam method .,evaluation
SyJaBw1eG_9,"Originality : The paper is based on the works of Collobert ( 2004 ) and Le Roux et al. ( 2008 ) , as well as the work of Martens :",fact
SyJaBw1eG_10,"the twist is that each layer of the neural network is considered a parameter block ,",fact
SyJaBw1eG_11,so that gradient interactions among weights in a single layer are more useful than those between weights in different layers .,fact
SyJaBw1eG_12,This increases the separability of the problem and reduces the complexity .,fact
SyJaBw1eG_13,Importance : Understanding the difference between first - and second-order methods for NN training is an important topic .,evaluation
SyJaBw1eG_14,"Using second-order methods could be considered at its infancy , compared to the wide variety of first-order methods .",evaluation
SyJaBw1eG_15,Having new results on second-order methods with interesting results would definitely attract some attention at the conference .,evaluation
SyJaBw1eG_16,Presentation/Clarity : The paper is well structured and well written .,evaluation
SyJaBw1eG_17,"The authors clearly place their work w.r.t. state of the art and previous works ,",evaluation
SyJaBw1eG_18,so that it is clear what is new and what is known .,evaluation
SyJaBw1eG_19,Comments : 1 . It is not clear why the deficiency of first-order methods on training NNs with big batches motivates us to turn into second-order methods .,evaluation
SyJaBw1eG_20,Is there a reasoning for this statement ?,request
SyJaBw1eG_21,Or is it just because second-order methods are kind-of the only other alternative we have ?,request
SyJaBw1eG_22,"2 . Assuming we can perform a second-order method , like Newton 's method , on a deep NN .",fact
SyJaBw1eG_23,"Since originally Newton 's method was designed to find solutions that have gradient equal to zero ,",fact
SyJaBw1eG_24,"and since NNs have saddle points ( probably many more than local minima ) ,",fact
SyJaBw1eG_25,"even if we could perfectly perform second-order Newton motions , there is no guarantee whether we converge to a local minimum or a saddle point .",fact
SyJaBw1eG_26,"However , since we perform Newton 's method approximately in practice ,",fact
SyJaBw1eG_27,this might help escaping saddle points .,evaluation
SyJaBw1eG_28,Any comment on this aspect,request
SyJaBw1eG_29,"( I 'm not aware whether this is already commented in Schraudolph 2002 , where the Gauss-Newton matrix was proposed instead of the Hessian ) ?",non-arg
SJEDvEvez_0,"This reviewer has found the proposed approach quite compelling ,",evaluation
SJEDvEvez_1,but the empirical validation requires significant improvements :,evaluation
SJEDvEvez_2,"1 ) you should include in your comparison Query-by - Bagging & Boosting ,",request
SJEDvEvez_3,which are two of the best out-of-the-box active learning strategies,evaluation
SJEDvEvez_4,"2 ) in your empirical validation you have ( arbitrarily ) split the 14 datasets in 7 training and testing ones ,",fact
SJEDvEvez_5,but many questions are still unanswered :,evaluation
SJEDvEvez_6,"- would any 7-7 split work just as well ( ie , cross-validate over the 14 domains )",request
SJEDvEvez_7,"- do you what happens if you train on 1 , 2 , 3 , 8 , 10 , or 13 domains ? are the results significantly different ?",request
SJEDvEvez_8,OTHER COMMENTS : - p3 : both images in Figure 1 are labeled Figure 1 . a,fact
SJEDvEvez_9,"- p3 : typo "" theis "" -- > "" this """,request
SJEDvEvez_10,<CIT>,reference
ByV24asxM_0,Monte-Carlo Tree Search is a reasonable and promising approach to hyperparameter optimization or algorithm configuration in search spaces that involve conditional structure .,evaluation
ByV24asxM_1,This paper must acknowledge more explicitly that it is not the first to take a graph-search approach .,request
ByV24asxM_2,The cited work related to SMAC and Hyperopt / TPE addresses this problem similarly .,evaluation
ByV24asxM_3,The technique of separating a description language from the optimization algorithm is also used in both of these projects / lines of research .,fact
ByV24asxM_4,"The [ mis-cited ] paper titled “ Making a science of model search … ” is about using TPE to configure 1 , 2 , and 3 layer convnets for several datasets , including CIFAR-10 .",fact
ByV24asxM_5,"SMAC and Hyperopt have been used to search large search spaces involving pre-processing and classification algorithms ( e.g. auto-sklearn , autoweka , hyperopt-sklearn ) .",fact
ByV24asxM_6,There have been near-annual workshops on AutoML and Bayesian optimization at NIPS and ICML ( see e.g. automl.org ) .,fact
ByV24asxM_7,There is a benchmark suite of hyperparameter optimization problems that would be a better way to evaluate MCTS as a hyperparameter optimization algorithm : <URL>,evaluation
SJ45Qm8Zz_0,The paper makes a striking connection between two apparently unrelated problems : the problem of designing neural networks to handle a certain type of correlation and the problem of designing a structure to represent wave-function with quantum entanglement .,fact
SJ45Qm8Zz_1,"In the wave-function context , the Schmidt decomposition of the wave function is an inner product of tensors .",fact
SJ45Qm8Zz_2,"Thus , the mathematical glue connecting the neural networks and quantum entanglement is shown to be tensor networks ,",fact
SJ45Qm8Zz_3,which can represent higher order tensors through inner product of lower-order tensors .,fact
SJ45Qm8Zz_4,The main technical contribution in the paper is to map convolutional networks with product pooling function ( called ConvACs ) to a tensor network .,evaluation
SJ45Qm8Zz_5,"Given this mapping , the authors exploit results in tensor networks ( in particular the quantum max-flow min-cut theorem ) to calculate the rank of the matricized tensor between a pair of vertex sets using the ( appropriately defined ) min-cut .",fact
SJ45Qm8Zz_6,"The connection has potential to yield fruitful new results ,",evaluation
SJ45Qm8Zz_7,"however , the potential is not manifested ( yet ) in the paper .",evaluation
SJ45Qm8Zz_8,The main application in deep convolutional networks proposed by the paper is to model how much correlation between certain partition of input variables can be captured by a given convolutional network design .,fact
SJ45Qm8Zz_9,"However , it is unclear how to use Theorem 1 to design neural networks that capture a certain correlation .",evaluation
SJ45Qm8Zz_10,A simple example is given in the experiment where the wider layers can be either early in the the neural network or at the later stages ; demonstrating that one does better than the other in a certain regime .,fact
SJ45Qm8Zz_11,It seems that there is an obvious intuition that explains this phenomenon : wider base networks with large filters are better suited to the global task and narrow base networks that have more parameters later down have more local early filters suited to the local task .,evaluation
SJ45Qm8Zz_12,"The experiments do not quite reveal the power of the proposed approach ,",evaluation
SJ45Qm8Zz_13,"and it is unclear how , if at all , the proposed approach can be applied to more complicated networks .",evaluation
SJ45Qm8Zz_14,"In summary , this paper is of high theoretical interest and has potential for future applications .",evaluation
Hk_LRZ5gG_0,This paper proposes several client-server neural network gradient update strategies aimed at reducing uplink usage while maintaining prediction performance .,fact
Hk_LRZ5gG_1,"The main approaches fall into two categories : structured , where low-rank/sparse updates are learned ,",fact
Hk_LRZ5gG_2,"and sketched , where full updates are either sub-sampled or compressed before being sent to the central server .",fact
Hk_LRZ5gG_3,Experiments are based on the federated averaging algorithm .,fact
Hk_LRZ5gG_4,"The work is valuable , but has room for improvement .",evaluation
Hk_LRZ5gG_5,"The paper is mainly an empirical comparison of several approaches , rather than from theoretically motivated algorithms .",evaluation
Hk_LRZ5gG_6,"This is not a criticism ,",non-arg
Hk_LRZ5gG_7,"however , it is difficult to see the reason for including the structured low-rank experiments in the paper",evaluation
Hk_LRZ5gG_8,"( itAs a reader , I found it difficult to understand the actual procedures used .",evaluation
Hk_LRZ5gG_9,"For example , what is the difference between the random mask update and the subsampling update",non-arg
Hk_LRZ5gG_10,"( why are there no random mask experiments after figure 1 , even though they performed very well ) ?",evaluation
Hk_LRZ5gG_11,"How is the structured update "" learned "" ?",non-arg
Hk_LRZ5gG_12,It would be very helpful to include algorithms .,request
Hk_LRZ5gG_13,"It seems like a good strategy is to subsample , perform Hadamard rotation , then quantise .",request
Hk_LRZ5gG_14,"For quantization , it appears that the HD rotation is essential for CIFAR , but less important for the reddit data .",evaluation
Hk_LRZ5gG_15,"It would be interesting to understand when HD works and why ,",request
Hk_LRZ5gG_16,"and perhaps make the paper more focused on this winning strategy , rather than including the low-rank algo .",request
Hk_LRZ5gG_17,"If convenient , could the authors comment on a similarly motivated paper under review at iclr 2018 :",request
Hk_LRZ5gG_18,VARIANCE-BASED GRADIENT COMPRESSION FOR EFFICIENT DISTRIBUTED DEEP LEARNING,reference
Hk_LRZ5gG_19,pros : - good use of intuition to guide algorithm choices,evaluation
Hk_LRZ5gG_20,- good compression with little loss of accuracy on best strategy,evaluation
Hk_LRZ5gG_21,- good problem for FA algorithm / well motivated,evaluation
Hk_LRZ5gG_22,cons : - some experiment choices do not appear well motivated / inclusion is not best choice,evaluation
Hk_LRZ5gG_23,- explanations of algos / lack of ' algorithms ' adds to confusion,evaluation
Hk_LRZ5gG_24,a useful reference : <CIT>,reference
HJpgrTKxf_0,Summary : The paper proposes a learnable skimming mechanism for RNN .,fact
HJpgrTKxf_1,The model decides whether to send the word to a larger heavy-weight RNN or a light-weight RNN .,fact
HJpgrTKxf_2,The heavy-weight and the light-weight RNN each controls a portion of the hidden state .,fact
HJpgrTKxf_3,"The paper finds that with the proposed skimming method , they achieve a significant reduction in terms of FLOPS .",fact
HJpgrTKxf_4,"Although it does n’t contribute to much speedup on modern GPU hardware , there is a good speedup on CPU ,",fact
HJpgrTKxf_5,and it is more power efficient .,fact
HJpgrTKxf_6,Contribution : - The paper proposes to use a small RNN to read unimportant text .,fact
HJpgrTKxf_7,"Unlike ( Yu et al. , 2017 ) , which skips the text , here the model decides between small and large RNN .",fact
HJpgrTKxf_8,Pros : - Models that dynamically decide the amount of computation make intuitive sense and are of general interests .,evaluation
HJpgrTKxf_9,- The paper presents solid experimentation on various text classification and question answering datasets .,fact
HJpgrTKxf_10,- The proposed method has shown reasonable reduction in FLOPS and CPU speedup with no significant accuracy degradation ( increase in accuracy in some tasks ) .,fact
HJpgrTKxf_11,"- The paper is well written , and the presentation is good .",evaluation
HJpgrTKxf_12,Cons : - Each model component is not novel .,fact
HJpgrTKxf_13,"The authors propose to use Gumbel softmax , but does compare other gradient estimators .",fact
HJpgrTKxf_14,"It would be good to use REINFORCE to do a fair comparison with ( Yu et al. , 2017 ) to see the benefit of using small RNN .",request
HJpgrTKxf_15,"- The authors report that training from scratch results in unstable skim rate , while Half pretrain seems to always work better than fully pretrained ones .",fact
HJpgrTKxf_16,"This makes the success of training a bit adhoc ,",evaluation
HJpgrTKxf_17,as one need to actively tune the number of pretraining steps .,fact
HJpgrTKxf_18,"- Although there is difference from ( Yu et al. , 2017 ) ,",fact
HJpgrTKxf_19,the contribution of this paper is still incremental .,evaluation
HJpgrTKxf_20,"Questions : - Although it is out of the scope for this paper to achieve GPU level speedup ,",evaluation
HJpgrTKxf_21,I am curious to know some numbers on GPU speedup .,request
HJpgrTKxf_22,"- One recommended task would probably be text summarization , in which the attended text can contribute to the output of the summary .",request
HJpgrTKxf_23,"Conclusion : - Based on the comments above , I recommend Accept",evaluation
B1A7YkceM_0,The authors propose a procedure to generate an ensemble of sparse structured models .,fact
B1A7YkceM_1,"To do this , the authors propose to ( 1 ) sample models using SG-MCMC with group sparse prior , ( 2 ) prune hidden units with small weights , ( 3 ) and retrain weights by optimizing each pruned model .",fact
B1A7YkceM_2,The ensemble is applied to MNIST classification and language modelling on PTB dataset .,fact
B1A7YkceM_3,I have two major concerns on the paper .,evaluation
B1A7YkceM_4,"First , the proposed procedure is quite empirically designed .",evaluation
B1A7YkceM_5,"So , it is difficult to understand why it works well in some problems .",evaluation
B1A7YkceM_6,Particularly . the justification on the retraining phase is weak .,evaluation
B1A7YkceM_7,It seems more like to use SG-MCMC to * initialize * models which will then be * optimized * to find MAP with the sparse-model constraints .,fact
B1A7YkceM_8,The second problem is about the baselines in the MNIST experiments .,evaluation
B1A7YkceM_9,"The FNN-300-100 model without dropout , batch-norm , etc. seems unreasonably weak baseline .",evaluation
B1A7YkceM_10,"So , the results on Table 1 on this small network is not much informative practically .",evaluation
B1A7YkceM_11,"Lastly , I also found a significant effort is also desired to improve the writing .",request
B1A7YkceM_12,The following reference also needs to be discussed in the context of using SG-MCMC in RNN .,request
B1A7YkceM_13,"- "" Scalable Bayesian Learning of Recurrent Neural Networks for Language Modeling "" , Zhe Gan * , Chunyuan Li * , Changyou Chen , Yunchen Pu , Qinliang Su , Lawrence Carin",reference
S1gH28vgM_0,1 ) Summary This paper proposed a new method for predicting multiple future frames in videos .,fact
S1gH28vgM_1,A new formulation is proposed where the frames ’ inherent noise is modeled separate from the uncertainty of the future .,fact
S1gH28vgM_2,"This separation allows for directly modeling the stochasticity in the sequence through a random variable <EQN> where the posterior <VAR> is approximated by a neural network ,",fact
S1gH28vgM_3,"and as a result , sampling of a random future is possible through sampling from the prior <VAR> during testing .",fact
S1gH28vgM_4,The random variable z can be modeled in a time-variant and time-invariant way .,fact
S1gH28vgM_5,"Additionally , this paper proposes a training procedure to prevent their method from ignoring the stochastic phenomena modeled by z.",fact
S1gH28vgM_6,"In the experimental section , the authors highlight the advantages of their method in 1 ) a synthetic dataset of shapes meant to clearly show the stochasticity in the prediction , 2 ) two robotic arm datasets for video prediction given and not given actions , and 3 ) A challenging human action dataset in which they perform future prediction only given previous frames .",fact
S1gH28vgM_7,2 ) Pros : + Novel/Sound future frame prediction formulation and training for modeling the stochasticity of future prediction .,fact
S1gH28vgM_8,+ Experiments on the synthetic shapes and robotic arm datasets highlight the proposed method ’s power of multiple future frame prediction possible .,evaluation
S1gH28vgM_9,"+ Good analysis on the number of samples improving the chance of outputting the correct future , the modeling power of the posterior for reconstructing the future , and a wide variety of qualitative examples .",evaluation
S1gH28vgM_10,+ Work is significant for the problem of modeling the stochastic nature of future frame prediction in videos .,evaluation
S1gH28vgM_11,3 ) Cons : Approximate posterior in non-synthetic datasets : The variable z seems to not be modeling the future very well .,evaluation
S1gH28vgM_12,"In the robot arm qualitative experiments , the robot motion is well modeled , however , the background is not .",evaluation
S1gH28vgM_13,"Given that for the approximate posterior computation the entire sequence is given ( e.g. reconstruction is performed ) ,",fact
S1gH28vgM_14,I would expect the background motion to also be modeled well .,evaluation
S1gH28vgM_15,"This issue is more evident in the Human 3.6 M experiments ,",evaluation
S1gH28vgM_16,as it seems to output blurriness regardless of the true future being observed .,evaluation
S1gH28vgM_17,This problem may mean the method is failing to model a large variety of objects and clearly works for the robotic arm,evaluation
S1gH28vgM_18,because a very similar large shape ( e.g. robot arm ) is seen in the training data .,fact
S1gH28vgM_19,Do you have any comments on this ?,non-arg
S1gH28vgM_20,Finn et al 2016 PNSR performance on Human 3.6 M :,reference
S1gH28vgM_21,"Is the same exact data , pre-processing , training , and architecture being utilized ?",request
S1gH28vgM_22,"In her paper , the PSNR for the first timestep on Human 3.6 M is about 41 ( maybe 42 ? ) while in this paper it is 38 .",fact
S1gH28vgM_23,Additional evaluation on Human 3.6 M : PSNR is not a good evaluation metric for frame prediction,evaluation
S1gH28vgM_24,"as it is biased towards blurriness ,",fact
S1gH28vgM_25,and also SSIM does not give us an objective evaluation in the sense of semantic quality of predicted frames .,fact
S1gH28vgM_26,"It would be good if the authors present additional quantitative evaluation to show that the predicted frames contain useful semantic information [ 1 , 2 , 3 , 4 ] .",request
S1gH28vgM_27,"For example , evaluating the predicted frames for the Human 3.6 M dataset to see if the human is still detectable in the image or if the expected action is being predicted could be useful to verify that the predicted frames contain the expected meaningful information compared to the baselines .",request
S1gH28vgM_28,Additional comments : Are all 15 actions being used for the Human 3.6 M experiments ?,non-arg
S1gH28vgM_29,"If so , the fact of the time-invariant model performs better than the time-variant one may not be the consistent action being performed ( last sentence of 5.2 ) .",fact
S1gH28vgM_30,"The motion performed by the actors in each action highly overlaps ( talking on the phone action may go from sitting to walking a little to sitting again , and so on ) .",evaluation
S1gH28vgM_31,"Unless actions such as walking and discussion were only used , it is unlikely the time-invariant z is performing better because of consistent action .",evaluation
S1gH28vgM_32,Do you have any comments on this ?,non-arg
S1gH28vgM_33,"4 ) Conclusion This paper proposes an interesting novel approach for predicting multiple futures in videos ,",evaluation
S1gH28vgM_34,"however , the results are not fully convincing in all datasets .",evaluation
S1gH28vgM_35,"If the authors can provide additional quantitative evaluation besides PSNR and SSIM ( e.g. evaluation on semantic quality ) , and also address the comments above , the current score will improve .",request
S1gH28vgM_36,References : [1] <CIT>,reference
S1gH28vgM_37,[2] <CIT>,reference
S1gH28vgM_38,[3] <CIT>,reference
S1gH28vgM_39,[4] <CIT>,reference
SJw9gV2ZM_0,This paper draws an interesting connection between deep neural networks and theories of quantum entanglement .,evaluation
SJw9gV2ZM_1,"They leveraged the tool for analyzing quantum entanglement to deep neural networks ,",fact
SJw9gV2ZM_2,and proposed a graph theoretical analysis for neural networks .,fact
SJw9gV2ZM_3,They demonstrated how their theory can help designing neural network architectures on the MNIST dataset .,fact
SJw9gV2ZM_4,I think the theoretical findings are novel,evaluation
SJw9gV2ZM_5,and may contribute to the important problem on understanding neural networks theoretically .,evaluation
SJw9gV2ZM_6,I am not familiar with the theory for quantum entanglement though .,fact
r1OoL_Yxz_0,The authors suggest using a mixture of shared and individual rewards within a MARL environment to induce cooperation among independent agents .,fact
r1OoL_Yxz_1,"They show that on their specific application this can lead to a better overall global performance than purely sharing the global signal , or using just the independent rewards .",fact
r1OoL_Yxz_2,The paper is a little too focused on the packet routing example domain and fails to deliver much in terms of a general theory of reward design for cooperative behaviours beyond showing that mixed rewards can lead to improved results in their domain .,evaluation
r1OoL_Yxz_3,"They discuss what and how rewards ,",fact
r1OoL_Yxz_4,"and this could be made more formal , as well as ( at the very least ) some guiding principles to follow when mixing rewards .",request
r1OoL_Yxz_5,"It feels like there is a missing section between sections 2 and 3 , where this methodological content could be described .",evaluation
r1OoL_Yxz_6,"The rest of the paper has similar issues , with key intuition and concepts either missing entirely or under-represented .",fact
r1OoL_Yxz_7,"The technical content often assumes that the reader is familiar with certain terms ,",fact
r1OoL_Yxz_8,and it is difficult to see what meaningful conclusions can be drawn from the evaluation .,evaluation
r1OoL_Yxz_9,"On a minor note , the use of the term cooperative in this paper could be better defined .",request
r1OoL_Yxz_10,"In game theory , cooperative games are those in which agents share rewards .",fact
r1OoL_Yxz_11,Non-cooperative ( game theory ) games are those where agents have general reward signals ( not necessarily cooperature or adversarial ) .,fact
r1OoL_Yxz_12,Conventionally ( yes there is existing reward design/shaping literature for MARL ) people have used the same terms in MARL .,fact
r1OoL_Yxz_13,"Perhaps the authors could define their approach as weakly cooperative , or emergent cooperation .",evaluation
r1OoL_Yxz_14,The related work could be better described .,request
r1OoL_Yxz_15,"There are existing papers on MARL and the issues with cooperation among independent learners ,",fact
r1OoL_Yxz_16,and this could be referenced .,request
r1OoL_Yxz_17,This includes reward shaping and reward potential .,request
r1OoL_Yxz_18,"I would also have expected to see brief mention of empowerment in this section too ( the agent favouring states where it has the power to control outcomes in an information theoretic sense ) , as an underyling principle for intrinsic reward .",request
r1OoL_Yxz_19,"However , more importantly , the authors really needed to do more to synthesize this into an overall picture of what principles are at play and what ideas/methods exist that have tried to exploit some of these principles .",request
r1OoL_Yxz_20,"Detailed comments : • [ p2 ] the authors say "" We set the meta reward signals as 1 - max ( U l ) . "" , before they define what U_l is .",fact
r1OoL_Yxz_21,"• [ p2 ] we have "" As many applications in the real world can be modeled using similar methods , we expect that other fields can also benefit from this work . """,quote
r1OoL_Yxz_22,"This statement is too vague ,",evaluation
r1OoL_Yxz_23,and the authors could do more to identify which application areas might benefit .,request
r1OoL_Yxz_24,"• [ p3 , first para ] "" However , the reward design studies for MARL is so limited . """,quote
r1OoL_Yxz_25,Drop the word ' so ' .,request
r1OoL_Yxz_26,"Also , I would argue that there have been quite a few ( non-deep ) discussions about reward design in MARL , cooperative , non-cooperative and competitive domains .",evaluation
r1OoL_Yxz_27,"• [ p3 , sec 2.2 ] "" This makes the diligent agents confuse about ... """,quote
r1OoL_Yxz_28,"should be "" confused "" , and I would advise against anthropomorphism at least when the meaning is obscured .",request
r1OoL_Yxz_29,"• [ p3 , sec 3 ] "" After having considered several other options , we finally choose the Packet Routing Domain as our experimental environments . """,quote
r1OoL_Yxz_30,Not sure what useful information is being conveyed here .,evaluation
r1OoL_Yxz_31,"• [ sec 3 ] THe domain could be better described with intuition and formal descriptions , e.g. link utilization ratio , etc , before .",request
r1OoL_Yxz_32,"• [ p6 ] "" Importantly , the proposed blR seems to have similar capacity with dlR , """,quote
r1OoL_Yxz_33,The discussion here is all in terms of the reward acronyms with very little call on intuition or other such assistance to the reader .,evaluation
r1OoL_Yxz_34,"• [ p7 ] "" We firstly try gR without any thinking """,quote
r1OoL_Yxz_35,The language could be better here .,request
Hy4tIW5xf_0,"The paper "" IMPROVING SEARCH THROUGH A3C REINFORCEMENT LEARNING BASED CONVERSATIONAL AGENT "" proposes to define an agent to guide users in information retrieval tasks .",fact
Hy4tIW5xf_1,"By proposing refinements of the query , categorizations of the results or some other bookmarking actions , the agent is supposed to help the user in achieving his search .",fact
Hy4tIW5xf_2,The proposed agent is learned via reinforcement learning .,fact
Hy4tIW5xf_3,"My concern with this paper is about the experiments that are only based on simulated agents , as it is the case for learning .",evaluation
Hy4tIW5xf_4,While it can be questionable for learning,evaluation
Hy4tIW5xf_5,"( but we understand why it is difficult to overcome ) ,",evaluation
Hy4tIW5xf_6,it is very problematic for the experiments to not have anything that demonstrates the usability of the approach in a real-world scenario .,evaluation
Hy4tIW5xf_7,I have serious doubts about the performances of such an artificially learned approach for achieving real-world search tasks .,evaluation
Hy4tIW5xf_8,"Also , for me the experimental section is not sufficiently detailed , which lead to not reproducible results .",evaluation
Hy4tIW5xf_9,"Moreover , authors should have considered baselines",request
Hy4tIW5xf_10,( only the two proposed agents are compared which is clearly not sufficient ) .,evaluation
Hy4tIW5xf_11,"Also , both models have some issues from my point of view .",evaluation
Hy4tIW5xf_12,"First , the Q-learning methods looks very complex :",evaluation
Hy4tIW5xf_13,how could we expect to get an accurate model with 10 ^ 7 states ?,evaluation
Hy4tIW5xf_14,"No generalization about the situations is done here ,",fact
Hy4tIW5xf_15,"examples of trajectories have to be collected for each individual considered state ,",fact
Hy4tIW5xf_16,which looks very huge ( especially if we think about the number of possible trajectories in such an MDP ) .,evaluation
Hy4tIW5xf_17,The second model is able to generalize from similar situations thanks to the neural architecture that is proposed .,fact
Hy4tIW5xf_18,"However , I have some concerns about it :",evaluation
Hy4tIW5xf_19,why keeping the history of actions in the inputs since it is captured by the LSTM cell ?,evaluation
Hy4tIW5xf_20,It is a redondant information that might disturb the process .,evaluation
Hy4tIW5xf_21,"Secondly , the proposed loss looks very heuristic for me ,",evaluation
Hy4tIW5xf_22,it is difficult to understand what is really optimized here .,evaluation
Hy4tIW5xf_23,"Particularly , the loss entropy function looks strange to me .",evaluation
Hy4tIW5xf_24,Is it classical ?,request
Hy4tIW5xf_25,Are there some references of such a method to maintain some exploration ability .,request
Hy4tIW5xf_26,"I understand the need of exploration ,",evaluation
Hy4tIW5xf_27,but including it in the loss function reduces the interpretability of the objective,evaluation
Hy4tIW5xf_28,( would n't it be preferable to use a more classical loss but with an epsilon greedy policy ? ) .,evaluation
Hy4tIW5xf_29,"Other remarks : - In the begining of "" varying memory capacity "" section , what is "" 100 , 150 and 250 "" ?",non-arg
Hy4tIW5xf_30,Time steps ?,non-arg
Hy4tIW5xf_31,What is the unit ?,non-arg
Hy4tIW5xf_32,Seconds ?,non-arg
Hy4tIW5xf_33,"- I did not understand the "" Capturing seach context at local and global level "" at all",evaluation
Hy4tIW5xf_34,"- In the loss entropy formula , the two negation signs could be removed",request
Hy4tIW5xf_35,- Would n't it be possible to use REINFORCE or other policy gradient method rather than roll-outs used in the paper ( which lead to biased gradient updates ) ?,request
rk-GXLRgz_0,This paper suggests a simple yet effective approach for learning with weak supervision .,evaluation
rk-GXLRgz_1,"This learning scenario involves two datasets , one with clean data ( i.e. , labeled by the true function ) and one with noisy data , collected using a weak source of supervision .",fact
rk-GXLRgz_2,"The suggested approach assumes a teacher and student networks ,",fact
rk-GXLRgz_3,"and builds the final representation incrementally , by taking into account the "" fidelity "" of the weak label when training the student at the final step .",fact
rk-GXLRgz_4,"The fidelity score is given by the teacher , after being trained over the clean data ,",fact
rk-GXLRgz_5,and it 's used to build a cost-sensitive loss function for the students .,fact
rk-GXLRgz_6,The suggested method seems to work well on several document classification tasks .,evaluation
rk-GXLRgz_7,"Overall , I liked the paper .",evaluation
rk-GXLRgz_8,"I would like the authors to consider the following questions - - Over the last 10 years or so , many different frameworks for learning with weak supervision were suggested ( e.g. , indirect supervision , distant supervision , response-based , constraint-based , to name a few ) .",fact
rk-GXLRgz_9,"First , I 'd suggest acknowledging these works and discussing the differences to your work .",request
rk-GXLRgz_10,Second - Is your approach applicable to these frameworks ?,request
rk-GXLRgz_11,"It would be an interesting to compare to one of those methods ( e.g. , distant supervision for relation extraction using a knowledge base ) , and see if by incorporating fidelity score , results improve .",request
rk-GXLRgz_12,- Can this approach be applied to semi-supervised learning ?,request
rk-GXLRgz_13,Is there a reason to assume the fidelity scores computed by the teacher would not improve the student in a self-training framework ?,request
rk-GXLRgz_14,"- The paper emphasizes that the teacher uses the student 's initial representation , when trained over the clean data .",fact
rk-GXLRgz_15,Is it clear that this step in needed ?,request
rk-GXLRgz_16,Can you add an additional variant of your framework when the fidelity score are computed by the teacher when trained from scratch ?,request
rk-GXLRgz_17,using different architecture than the student ?,request
rkCi3T3lG_0,"Summary : The paper proposes a new dataset for reading comprehension , called DuoRC .",fact
rkCi3T3lG_1,The questions and answers in the DuoRC dataset are created from different versions of a movie plot narrating the same underlying story .,fact
rkCi3T3lG_2,The DuoRC dataset offers the following challenges compared to the existing reading comprehension ( RC ) datasets –,fact
rkCi3T3lG_3,"1 ) low lexical overlap between questions and their corresponding passages ,",fact
rkCi3T3lG_4,"2 ) requires use of common-sense knowledge to answer the question ,",fact
rkCi3T3lG_5,"3 ) requires reasoning across multiples sentences to answer the question ,",fact
rkCi3T3lG_6,4 ) consists of those questions as well that can not be answered from the given passage .,fact
rkCi3T3lG_7,The paper experiments with two types of models,fact
rkCi3T3lG_8,– 1 ) a model which only predicts the span in a document and,fact
rkCi3T3lG_9,2 ) a model which generates the answer after predicting the span .,fact
rkCi3T3lG_10,Both these models are built off of an existing model on SQuAD – the Bidirectional Attention Flow ( BiDAF ) model .,fact
rkCi3T3lG_11,The experimental results show that the span based model performs better than the model which generates the answers .,fact
rkCi3T3lG_12,"But the accuracy of both the models is significantly lower than that of their base model ( BiDAF ) on SQuAD , demonstrating the difficulty of the DuoRC dataset .",fact
rkCi3T3lG_13,Strengths : 1 . The data collection process is interesting .,evaluation
rkCi3T3lG_14,The challenges in the proposed dataset as outlined in the paper seem worth pushing for .,evaluation
rkCi3T3lG_15,2 . The paper is well written making it easy to follow .,evaluation
rkCi3T3lG_16,3 . The experiments and analysis presented in the paper are insightful .,evaluation
rkCi3T3lG_17,"Weaknesses : 1 . It would be good if the paper can throw some more light on the comparison between the existing MovieQA dataset and the proposed DuoRC dataset , other than the size .",request
rkCi3T3lG_18,2 . The dataset is motivated as consisting of four challenges ( described in the summary above ) that do not exist in the existing RC datasets .,fact
rkCi3T3lG_19,"However , the paper lacks an analysis on what percentage of questions in the proposed dataset belong to each category of the four challenges .",fact
rkCi3T3lG_20,Such an analysis would helpful to accurately get an estimate of the proportion of these challenges in the dataset .,request
rkCi3T3lG_21,3 . It is not clear from the paper how should the questions which are unanswerable be evaluated .,evaluation
rkCi3T3lG_22,"As in , what should be the ground-truth answer against which the answers should such questions be evaluated .",evaluation
rkCi3T3lG_23,"Clearly , string matching would not work",fact
rkCi3T3lG_24,because a model could say “ do n’t know ” whereas some other model could say “ unanswerable ” .,fact
rkCi3T3lG_25,"So , does the training data have a particular string as the ground truth answer for such questions , so that a model can just be trained to spit out that particular string when it thinks it ca n’t answer the questions ?",non-arg
rkCi3T3lG_26,4 . One of the observations made in the paper is that “ training on one dataset and evaluating on the other results in a drop in the performance . ”,fact
rkCi3T3lG_27,"However , in table 4 , evaluating on Paraphrase RC is better when trained on Self RC as opposed to when trained on Paraphrase RC .",fact
rkCi3T3lG_28,This seems to be in conflict with the observation drawn in the paper .,fact
rkCi3T3lG_29,Could authors please clarify this ?,request
rkCi3T3lG_30,"Also , could authors please throw some light on why this might be happening ?",request
rkCi3T3lG_31,"5 . In the third phase of data collection ( Paraphrase RC ) , was waiting for 2-3 weeks the only step taken in order to ensure that the workers for this stage are different from those in stage 2 , or was something more sophisticated implemented which did not allow a worker who has worked in stage 2 to be able to participate in stage 3 ?",request
rkCi3T3lG_32,"6 . Typo : Dataset section , phrases -- > phases",request
rkCi3T3lG_33,Overall : The challenges proposed in the DuoRC dataset are interesting .,evaluation
rkCi3T3lG_34,The paper is well written,evaluation
rkCi3T3lG_35,and the experiments are interesting .,evaluation
rkCi3T3lG_36,"However , there are some questions ( as mentioned in the Weaknesses section ) which need to be clarified before I can recommend acceptance for the paper .",evaluation
H1asng9lG_0,"This paper introduces a new exploration policy for Reinforcement Learning for agents on the web called "" Workflow Guided Exploration "" .",fact
H1asng9lG_1,Workflows are defined through a DSL unique to the domain .,fact
H1asng9lG_2,"The paper is clear , very well written , and well-motivated .",evaluation
H1asng9lG_3,Exploration is still a challenging problem for RL .,fact
H1asng9lG_4,The workflows remind me of options though in this paper they appear to be hand-crafted .,evaluation
H1asng9lG_5,"In that sense , I wonder if this has been done before in another domain .",non-arg
H1asng9lG_6,The results suggest that WGE sometimes helps but not consistently .,fact
H1asng9lG_7,"While the experiments show that DOMNET improves over Shi et al , that could be explained as not having to train on raw pixels or not enough episodes .",evaluation
HJIPOSAbf_0,The paper develops an interesting approach for solving multi-class classification with softmax loss .,evaluation
HJIPOSAbf_1,"The key idea is to reformulate the problem as a convex minimization of a "" double-sum "" structure via a simple conjugation trick .",evaluation
HJIPOSAbf_2,"SGD is applied to the reformulation : in each step samples a subset of the training samples and labels , which appear both in the double sum .",fact
HJIPOSAbf_3,"The main contributions of this paper are : "" U-max "" idea ( for numerical stability reasons ) and an "" "" proposing an "" implicit SGD "" idea .",evaluation
HJIPOSAbf_4,"Unlike the first review , I see what the term "" exact "" in the title is supposed to mean .",evaluation
HJIPOSAbf_5,I believe this was explained in the paper .,fact
HJIPOSAbf_6,I agree with the second reviewer that the approach is interesting .,evaluation
HJIPOSAbf_7,"However , I also agree with the criticism",evaluation
HJIPOSAbf_8,( double sum formulations exist in the literature ;,fact
HJIPOSAbf_9,comments about experiments ) ;,non-arg
HJIPOSAbf_10,and will not repeat it here .,non-arg
HJIPOSAbf_11,I will stress though that the statement about Newton in the paper is not justified .,fact
HJIPOSAbf_12,Newton method does not converge globally with linear rate .,fact
HJIPOSAbf_13,Cubic regularisation is needed for global convergence .,fact
HJIPOSAbf_14,Local rate is quadratic .,fact
HJIPOSAbf_15,I believe the paper could warrant acceptance if all criticism raised by reviewer 2 is addressed .,evaluation
HJIPOSAbf_16,I apologise for short and late review : I got access to the paper only after the original review deadline .,non-arg
rJMoToYlz_0,The authors present a derivation of previous work of [1] .,fact
rJMoToYlz_1,"In particular they propose the method of using the error signal of a dynamics model as curiosity for exploration , such as [1] , but without any additionaly auxiliary methods .",fact
rJMoToYlz_2,This the author call Curiosity by Bootstrapping Feature ( CBF ) .,fact
rJMoToYlz_3,"In particular they show over a set of auxiliary learning methods ( hindsight ER , inverse dynamics model [1] ) there is not a clear cut edge one method has over the other ( or over using no auxilirary method all , that is CBF ) .",fact
rJMoToYlz_4,Overall I think the novelty is too limited for acceptance .,evaluation
rJMoToYlz_5,"The main point of the authors ( heterogeneous results over different auxilirary learning methods ) , is not suprising at all , and to be expected .",evaluation
rJMoToYlz_6,The method the authors introduce is just a submodule of already published results [1] .,fact
rJMoToYlz_7,"For instance , section 4 discusses challenges related to these class of approaches such as the presence of stochasticity .",fact
rJMoToYlz_8,Had the authors proposed a solution to these challenges that would have benefited the paper greatly .,request
rJMoToYlz_9,"Minor : The light green link color make the paper hard on the eye ,",evaluation
rJMoToYlz_10,I suggest using [hidelinks] for hyperref .,request
rJMoToYlz_11,Figure 2 is very small and hard to read .,evaluation
rJMoToYlz_12,[1] <CIT>,reference
H1wVDrtgM_0,This paper tried to analyze the subspaces of the adversarial examples neighborhood .,fact
H1wVDrtgM_1,"More specifically , the authors used Local Intrinsic Dimensionality to analyze the intrinsic dimensional property of the subspaces .",fact
H1wVDrtgM_2,The characteristics and theoretical analysis of the proposed method are discussed and explained .,fact
H1wVDrtgM_3,This paper helps others to better understand the vulnerabilities of DNNs .,evaluation
SkYNcg5xz_0,The paper addresses the problem of learners forgetting rare states and revisiting catastrophic danger states .,fact
SkYNcg5xz_1,The authors propose to train a predictive ‘ fear model ’ that penalizes states that lead to catastrophes .,fact
SkYNcg5xz_2,The proposed technique is validated both empirically and theoretically .,fact
SkYNcg5xz_3,Experiments show a clear advantage during learning when compared with a vanilla DQN .,fact
SkYNcg5xz_4,"Nonetheless , there are some criticisms than can be made of both the method and the evaluations :",evaluation
SkYNcg5xz_5,The fear radius threshold k_r seems to add yet another hyperparameter that needs tuning .,fact
SkYNcg5xz_6,Judging from the description of the experiments this parameter is important to the performance of the method and needs to be set experimentally .,evaluation
SkYNcg5xz_7,There seems to be no way of a priori determine a good distance,fact
SkYNcg5xz_8,as there is no way to know in advance when a catastrophe becomes unavoidable .,fact
SkYNcg5xz_9,No empirical results on the effect of the parameter are given .,fact
SkYNcg5xz_10,The experimental results support the claim that this technique helps to avoid catastrophic states during initial learning .,fact
SkYNcg5xz_11,"The paper however , also claims to address the longer term problem of revisiting these states once the learner forgets about them ,",fact
SkYNcg5xz_12,since they are no longer part of the data generated by ( close to ) optimal policies .,fact
SkYNcg5xz_13,This problem does not seem to be really solved by this method .,evaluation
SkYNcg5xz_14,"Danger and safe state replay memories are kept , but are only used to train the catastrophe classifier .",fact
SkYNcg5xz_15,"While the catastrophe classifier can be seen as an additional external memory ,",fact
SkYNcg5xz_16,it seems that the learner will still drift away from the optimal policy and then need to be reminded by the classifier through penalties .,fact
SkYNcg5xz_17,"As such the method would n’t prevent catastrophic forgetting ,",fact
SkYNcg5xz_18,it would just prevent the worst consequences by penalizing the agent before it reaches a danger state .,fact
SkYNcg5xz_19,It would therefore be interesting to see some long running experiments and analyse how often catastrophic states ( or those close to them ) are visited .,request
SkYNcg5xz_20,"Overall , the current evaluations focus on performance and give little insight into the behaviour of the method .",evaluation
SkYNcg5xz_21,The paper also does not compare to any other techniques that attempt to deal with catastrophic forgetting and/or the changing state distribution <VAR> .,fact
SkYNcg5xz_22,"In general the explanations in the paper often often use confusing and imprecise language , even in formal derivations , e.g. ‘ if the fear model reaches arbitrarily high accuracy ’ or ‘ if the probability is negligible ’ .",evaluation
SkYNcg5xz_23,It is was n’t clear to me that the properties described in Theorem 1 actually hold .,evaluation
SkYNcg5xz_24,The motivation in the appendix is very informal and no clear derivation is provided .,evaluation
SkYNcg5xz_25,The authors seem to indicate that a minimal return can be guaranteed because the optimal policy spends a maximum of epsilon amount of time in the catastrophic states and the alternative policy simply avoids these states .,fact
SkYNcg5xz_26,"However , as the alternative policy is learnt on a different reward ,",fact
SkYNcg5xz_27,"it can have a very different state distribution , even for the non-catastrophics states .",evaluation
SkYNcg5xz_28,It might attach all its weight to a very poor reward state in an effort to avoid the catastrophe penalty .,fact
SkYNcg5xz_29,It is therefore not clear to me that any claims can be made about its performance without additional assumptions .,evaluation
SkYNcg5xz_30,"It seems that one could construct a counterexample using a 3-state chain problem ( no_reward , danger , goal ) where the only way to get to the single goal state is to incur a small risk of visiting the danger state .",fact
SkYNcg5xz_31,"Any optimal policy would therefore need to spend some time e in the danger state , on average .",fact
SkYNcg5xz_32,A policy that learns to avoid the danger state would then also be unable to reach the goal state and receive rewards .,fact
SkYNcg5xz_33,E.g <VAR> has stationary distribution <VAR> and return <EQN> .,fact
SkYNcg5xz_34,"By adding a sufficiently high penalty , policy pi ~ can learn to avoid the catastrophic state with distribution <VAR> and then gets return <EQN> .",fact
SkYNcg5xz_35,This seems to contradict the theorem .,fact
SkYNcg5xz_36,It was n’t clear what assumptions the authors make to exclude situations like this .,evaluation
SkYNcg5xz_37,[1] <CIT>,reference
SkYNcg5xz_38,[2] <CIT>,reference
r1IWuK2lf_0,The paper presents a method for navigating in an unknown and partially observed environment is presented .,fact
r1IWuK2lf_1,"The proposed approach splits planning into two levels : 1 ) local planning based on the observed space and 2 ) a global planner which receives the local plan , observation features , and access to an addressable memory to decide on which action to select and what to write into memory .",fact
r1IWuK2lf_2,"The contribution of this work is the use of value iteration networks ( VINs ) for local planning on a locally observed map that is fed into a learned global controller that references history and a differential neural computer ( DNC ) , local policy , and observation features select an action and update the memory .",fact
r1IWuK2lf_3,"The core concept of learned local planner providing additional cues for a global , memory-based planner is a clever idea",evaluation
r1IWuK2lf_4,and the thorough analysis clearly demonstrates the benefit of the approach .,evaluation
r1IWuK2lf_5,"The proposed method is tested against three problems : a gridworld , a graph search , and a robot environment .",fact
r1IWuK2lf_6,In each case the proposed method is more performant than the baseline methods .,fact
r1IWuK2lf_7,The ablation study of using LSTM instead of the DNC and the direct comparison of CNN + LSTM support the authors ’ hypothesis about the benefits of the two components of their method .,fact
r1IWuK2lf_8,"While the author ’s compare to DRL methods with limited horizon ( length 4 ) , there is no comparison to memory-based RL techniques .",fact
r1IWuK2lf_9,"Furthermore , a comparison of related memory-based visual navigation techniques on domains for which they are applicable should be considered",request
r1IWuK2lf_10,as such an analysis would illuminate the relative performance over the overlapping portions problem domains,fact
r1IWuK2lf_11,"For example , analysis of the metric map approaches on the grid world or of MACN on their tested environments .",fact
r1IWuK2lf_12,"Prior work in visual navigation in partially observed and unknown environments have used addressable memory ( e.g. , Oh et al. ) and used VINs ( e.g. , Gupta et al. ) to plan as noted .",fact
r1IWuK2lf_13,"In discussing these methods , the authors state that these works are not comparable as they operate strictly on discretized 2d spaces .",fact
r1IWuK2lf_14,"However , it appears to the reviewer that several of these methods can be adapted to higher dimensions and be applicable at least a subclass ( for the euclidean/metric map approaches ) or the full class of the problems ( for Oh et al. ) ,",evaluation
r1IWuK2lf_15,which appears to be capable to solve non-euclidean tasks like the graph search problem .,fact
r1IWuK2lf_16,"If this assessment is correct , the authors should differentiate between these approaches more thoroughly and consider empirical comparisons .",request
r1IWuK2lf_17,The authors should further consider contrasting their approach with “ Neural SLAM ” by Zhang et al .,request
r1IWuK2lf_18,A limitation of the presented method is requirement that the observation “ reveals the labeling of nearby states . ”,evaluation
r1IWuK2lf_19,This assumption holds in each of the examples presented : the neighborhood map in the gridworld and graph examples and the lidar sensor in the robot navigation example .,fact
r1IWuK2lf_20,It would be informative for the authors to highlight this limitation and/or identify how to adapt the proposed method under weaker assumptions such as a sensor that does n’t provide direct metric or connectivity information such as a RGB camera .,request
r1IWuK2lf_21,Many details of the paper are missing and should be included to clarify the approach and ensure reproducible results .,request
r1IWuK2lf_22,The reviewer suggests providing both more details in the main section of the paper and providing the precise architecture including hyperparameters in the supplementary materials section .,request
r1RTd8hgG_0,The proposed method is a classifier that is fair and works in collaboration with an unfair ( but presumably accurate model ) .,evaluation
r1RTd8hgG_1,The novel classifier is the result of the optimisation of a loss function,fact
r1RTd8hgG_2,( composed of a part similar to a logistic regression model and a part being the disparate impact ) .,fact
r1RTd8hgG_3,"Hence , it can be interpreted as a logistic loss with a fairness regularisation .",fact
r1RTd8hgG_4,The results are promising and the applications are very important for the acceptance of ML approaches in the society .,evaluation
r1RTd8hgG_5,"However , I believe that the model could be made more general ( than a fairness regularized logistic loss ) and its theoretical properties studied .",evaluation
r1RTd8hgG_6,"Finally , this paper used uncommon vocabulary ( for the machine learning community )",evaluation
r1RTd8hgG_7,"and it make is difficult to follow sometimes ( for example , the use of a Decision-Maker entity ) .",evaluation
r1RTd8hgG_8,"When reading the submitted paper , it was unclear ( until section 6 ) how deferring could help fairness .",evaluation
r1RTd8hgG_9,"Hence , the structure of the paper could maybe be improved by introducing the cost function earlier in the manuscript ( as a fairness regularised loss ) .",request
r1RTd8hgG_10,"To conclude , although the application is of high interest and the numerical results encouraging ,",evaluation
r1RTd8hgG_11,the methodological approach does not seem to be very novel .,evaluation
r1RTd8hgG_12,Minor comment : - The list of authors of the reference “ Machine bias : theres software … ” apperars incorrectly ( some comma may be missing in the . bib file ),fact
r1RTd8hgG_13,and there is a small typo in the title .,fact
r1RTd8hgG_14,Possible extensions : - The proposed fairness aware loss could be made more general ( and not only in the case of a logistic model ),request
r1RTd8hgG_15,- It could also be generalised to a mixture of biased classifier ( more than on DM ) .,request
Byqj1QtlM_0,"A DeepRL algorithm is presented that represents distributions over Q values , as applied to DDPG , and in conjunction with distributed evaluation across multiple actors , prioritized experience replay , and N-step look-aheads .",fact
Byqj1QtlM_1,"The algorithm is called Distributed Distributional Deep Deterministic Policy Gradient algorithm , D4PG .",fact
Byqj1QtlM_2,"SOTA results are generated for a number of challenging continuous domain learning problems , as compared to benchmarks that include DDPG and PPO , in terms of wall-clock time , and also ( most often ) in terms of sample efficiency .",fact
Byqj1QtlM_3,"pros/cons + the paper provides a thorough investigation of the distributional approach , as applied to difficult continuous action problems , and in conjunction with a set of other improvements ( with ablation tests )",fact
Byqj1QtlM_4,"- the story is a bit mixed in terms of the benefits , as compared to the non-distributional approach , D3PG",evaluation
Byqj1QtlM_5,- it is not clear which of the baselines are covered in detail in the cited paper :,evaluation
Byqj1QtlM_6," Anonymous . Distributed prioritized experience replay . In submission , 2017 .  ,",reference
Byqj1QtlM_7,"i.e. , should readers assume that D3PG already exists and is attributable to this other submission ?",request
Byqj1QtlM_8,"Overall , I believe that the community will find this to be interesting work .",evaluation
Byqj1QtlM_9,Is a video of the results available ?,non-arg
Byqj1QtlM_10,"It seems that the distributional model often does not make much of a difference , as compared to D3PG non-prioritized .",evaluation
Byqj1QtlM_11,"However , sometimes it does make a big difference , i.e. , 3D parkour ; acrobot .",fact
Byqj1QtlM_12,Do the examples where it yields the largest payoff share a particular characteristic ?,non-arg
Byqj1QtlM_13,The benefit of the distributional models is quite different between the 1-step and 5-step versions .,fact
Byqj1QtlM_14,Any ideas why ?,non-arg
Byqj1QtlM_15,"Occasionally , D4PG with N = 1 fails very badly , e.g. , fish , manipulator ( bring ball ) , swimmer .",fact
Byqj1QtlM_16,Why would that be ?,non-arg
Byqj1QtlM_17,Should n't it do at least as well as D3PG in general ?,evaluation
Byqj1QtlM_18,How many atoms are used for the categorical representation ?,non-arg
Byqj1QtlM_19,"As many as [ Bellemare et al. ] , i.e. , 51 ?",non-arg
Byqj1QtlM_20,"How much "" resolution "" is necessary here in order to gain most of the benefits of the distributional representation ?",non-arg
Byqj1QtlM_21,"As far as I understand , V_min and V_max are not the global values , but are specific to the current distribution .",evaluation
Byqj1QtlM_22,Hence the need for the projection .,evaluation
Byqj1QtlM_23,Is that correct ?,non-arg
Byqj1QtlM_24,Would increasing the exploration noise result in a larger benefit for the distributional approach ?,non-arg
Byqj1QtlM_25,Figure 2 : DDPG performs suprisingly poorly in most examples .,evaluation
Byqj1QtlM_26,"Any comments on this , or is DDPG best avoided in normal circumstances for continuous problems ? : - )",evaluation
Byqj1QtlM_27,Is the humanoid stand so easy because of large ( or unlimited ) torque limits ?,non-arg
Byqj1QtlM_28,The wall-clock times are for a cluster with K = 32 cores for Figure 1 ?,non-arg
Byqj1QtlM_29, we utilize a network architecture as specified in Figure 1 which processes the terrain info in order to reduce its dimensionality ,quote
Byqj1QtlM_30,"Figure 1 provides no information about the reduced dimensionality of the terrain representation ,",fact
Byqj1QtlM_31,unless I am somehow failing to see this .,evaluation
Byqj1QtlM_32, the full critic architecture is completed by attaching a critic head as defined in Section A ,quote
Byqj1QtlM_33,"I could find no further documenation in the paper with regard to the "" head "" or a separate critic for the "" head "" .",evaluation
Byqj1QtlM_34,It is not clear to me why multiple critics are needed .,evaluation
Byqj1QtlM_35,Do you have an intuition as to why prioritized replay might be reducing performance in many cases ?,non-arg
Bk1-V2plG_0,"The paper proposes a technique for training quantized neural networks , where the precision ( number of bits ) varies per layer and is learned in an end-to-end fashion .",fact
Bk1-V2plG_1,"The idea is to add two terms to the loss , one representing quantization error , and the other representing the number of discrete values the quantization can support ( or alternatively the number of bits used ) .",fact
Bk1-V2plG_2,Updates are made to the parameter representing the # of bits via the sign of its gradient .,fact
Bk1-V2plG_3,Experiments are conducted using a LeNet-inspired architecture on MNIST and CIFAR10 .,fact
Bk1-V2plG_4,"Overall , the idea is interesting , as providing an end-to-end trainable technique for distributing the precision across layers of a network would indeed be quite useful .",evaluation
Bk1-V2plG_5,"I have a few concerns : First , I find the discussion around the training methodology insufficient .",evaluation
Bk1-V2plG_6,"Inherently , the objective is discontinuous since # of bits is a discrete parameter .",fact
Bk1-V2plG_7,This is worked around by updating the parameter using the sign of its gradient .,fact
Bk1-V2plG_8,This is assuming the local linear approximation given by the derivative is accurate enough one integer away ;,fact
Bk1-V2plG_9,"this may or may not be true ,",evaluation
Bk1-V2plG_10,but it 's not clear and there is little discussion of whether this is reasonable to assume .,evaluation
Bk1-V2plG_11,It 's also difficult for me to understand how this interacts with the other terms in the objective ( quantization error and loss ) .,evaluation
Bk1-V2plG_12,"We 'd like the number of bits parameter to trade off between accuracy ( at least in terms of quantization error , and ideally overall loss as well ) and precision .",fact
Bk1-V2plG_13,But it 's not at all clear that the gradient of either the loss or the quantization error w.r.t. the number of bits will in general suggest increasing the number of bit ( thus requiring the bit regularization term ) .,evaluation
Bk1-V2plG_14,This will clearly not be the case when the continuous weights coincide with the quantized values for the current bit setting .,evaluation
Bk1-V2plG_15,"More generally , the direction of the gradient will be highly dependent on the specific setting of the current weights .",fact
Bk1-V2plG_16,"It 's unclear to me how effectively accuracy and precision are balanced by this training strategy ,",evaluation
Bk1-V2plG_17,and there is n't any discussion of this point either .,fact
Bk1-V2plG_18,I would be less concerned about the above points if I found the experiments compelling .,evaluation
Bk1-V2plG_19,"Unfortunately , although I am quite sympathetic to the argument that state of the art results or architectures are n't necessary for a paper of this kind ,",evaluation
Bk1-V2plG_20,the results on MNIST and CIFAR10 are so poor that they give me some concern about how the training was performed and whether the results are meaningful .,evaluation
Bk1-V2plG_21,Performance on MNIST in the 7-11 % test error range is comparable to a simple linear logistic regression model ; for a CNN that is extremely bad .,evaluation
Bk1-V2plG_22,"Similarly , 40 % error on CIFAR10 is worse than what some very simple fully connected models can achieve .",fact
Bk1-V2plG_23,"Overall , while I like the and think the goal is good ,",evaluation
Bk1-V2plG_24,"I think the motivation and discussion for the training methodology is insufficient , and the empirical work is concerning .",evaluation
Bk1-V2plG_25,I ca n't recommend acceptance .,evaluation
SkaNEl9xM_0,Summary of paper and review : The paper presents the instability issue of training GANs for semi-supervised learning .,fact
SkaNEl9xM_1,"Then , they propose to essentially utilize a wgan for semi-supervised learning .",fact
SkaNEl9xM_2,"The novelty of the paper is minor ,",evaluation
SkaNEl9xM_3,since similar approaches have been done before .,fact
SkaNEl9xM_4,"The analysis is poor ,",evaluation
SkaNEl9xM_5,"the text seems to contain mistakes ,",fact
SkaNEl9xM_6,and the results do n't seem to indicate any advantage or promise of the proposed algorithm .,fact
SkaNEl9xM_7,Detailed comments : - Unless I 'm grossly mistaken the loss function ( 2 ) is clearly wrong .,evaluation
SkaNEl9xM_8,There is a cross-entropy term used by Salimans et al. clearly missing .,fact
SkaNEl9xM_9,"- As well , if equation ( 4 ) is referring to feature matching , the expectation should be inside the norm and not outside",fact
SkaNEl9xM_10,"( this amounts to matching random specific random fake examples to specific random real examples , an imbalanced form of MMD ) .",fact
SkaNEl9xM_11,"- Theorem 2.1 is an almost literal rewrite of Theorem 2.4 of [ 1 ] , without proper attribution .",evaluation
SkaNEl9xM_12,"Furthermore , Theorem 2.1 is not sufficient to demonstrate existence of this issues .",evaluation
SkaNEl9xM_13,This is why [ 1 ] provides an extensive batch of targeted experiments to verify this assumptions .,fact
SkaNEl9xM_14,Analogous experiments are clearly missing .,fact
SkaNEl9xM_15,A detailed analysis of these assumptions and its implications are missing .,fact
SkaNEl9xM_16,"- In section 3 , the authors propose a minor variation of the Improved GAN approach by using a wgan on the unsupervised part of the loss .",evaluation
SkaNEl9xM_17,"Remarkably similar algorithms ( where the two discriminators are two separate heads ) to this have been done before ( see for example , [ 2 ] , but other approaches exist after that , see for examples papers citing [ 2 ] ) .",evaluation
SkaNEl9xM_18,- Theorem 3.1 is a trivial consequence of Theorem 3 from WGAN .,evaluation
SkaNEl9xM_19,- The experiments leave much to be desired .,evaluation
SkaNEl9xM_20,"It is widely known that MNIST is a bad benchmark at this point ,",evaluation
SkaNEl9xM_21,and that no signal can be established from a minor success in this dataset .,fact
SkaNEl9xM_22,"Furthermore , the results in CIFAR do n't seem to bring any advantage , considering the .1 % difference in accuracy is 1/100 of chance in this dataset .",fact
SkaNEl9xM_23,[1] <CIT>,reference
SkaNEl9xM_24,[2] <CIT>,reference
HyciX9dxM_0,1 ) Summary This paper proposes a recurrent neural network ( RNN ) training formulation for encouraging RNN the hidden representations to contain information useful for predicting future timesteps reliably .,fact
HyciX9dxM_1,The authors propose to train a forward and backward RNN in parallel .,fact
HyciX9dxM_2,The forward RNN predicts forward in time and the backward RNN predicts backwards in time .,fact
HyciX9dxM_3,"While the forward RNN is trained to predict the next timestep ,",fact
HyciX9dxM_4,its hidden representation is forced to be similar to the representation of the backward RNN in the same optimization step .,fact
HyciX9dxM_5,"In experiments , it is shown that the proposed method improves training speed in terms of number of training iterations , achieves 0.8 CIDEr points improvement over baselines using the proposed training , and also achieves improved performance for the task of speech recognition .",fact
HyciX9dxM_6,2 ) Pros : + Novel idea that makes sense for learning a more robust representation for predicting the future and prevent only local temporal correlations learned .,evaluation
HyciX9dxM_7,+ Informative analysis for clearly identifying the strengths of the proposed method and where it is failing to perform as expected .,evaluation
HyciX9dxM_8,+ Improved performance in speech recognition task .,evaluation
HyciX9dxM_9,+ The idea is clearly explained and well motivated .,evaluation
HyciX9dxM_10,"3 ) Cons : Image captioning experiment : In the experimental section , there is an image captioning result in which the proposed method is used on top of two baselines .",fact
HyciX9dxM_11,"This experiment shows improvement over such baselines ,",fact
HyciX9dxM_12,"however , the performance is still worse compared against baselines such as Lu et al , 2017 and Yao et al , 2016 .",fact
HyciX9dxM_13,"It would be optimal if the authors can use their training method on such baselines and show improved performance , or explain why this can not be done .",request
HyciX9dxM_14,"Unconditioned generation experiments : In these experiments , sequential pixel-by-pixel MNIST generation is performed in which the proposed method did not help .",fact
HyciX9dxM_15,"Because of this , two conditioned set ups are performed : 1 ) 25 % of pixels are given before generation , and 2 ) 75 % of pixels are given before generation .",fact
HyciX9dxM_16,"The proposed method performs similar to the baseline in the 25 % case , and better than the baseline in the 75 % case .",fact
HyciX9dxM_17,"For completeness , and to come to a stronger conclusion on how much uncertainty really affects the proposed method , this experiment needs a case in which 50 % of the pixels are given .",request
HyciX9dxM_18,Observing 25 % of the pixels gives almost no information about the identity of the digit,evaluation
HyciX9dxM_19,"and it makes sense that it ’s hard to encode the future ,",evaluation
HyciX9dxM_20,"however , 50 % of the pixels give a good idea of what the digit identity is .",evaluation
HyciX9dxM_21,"If the authors believe that the 50 % case is not necessary , please feel free to explain why .",request
HyciX9dxM_22,"Additional comments : The method is shown to converge faster compared to the baselines ,",fact
HyciX9dxM_23,"however , it is possible that the baseline may finish training faster",fact
HyciX9dxM_24,( the authors do acknowledge the additional computation needed in the backward RNN ) .,fact
HyciX9dxM_25,It would be informative for the research community to see the relationship of training time ( how long it takes in hours ) versus how fast it learns ( iterations taken to learn ) .,request
HyciX9dxM_26,Experiments on RL planning tasks would be interesting to see ( Maybe on a simple/predictable environment ) .,request
HyciX9dxM_27,4 ) Conclusion The paper proposes a method for training RNN architectures to better model the future in its internal state supervised by another RNN modeling the future in reverse .,fact
HyciX9dxM_28,Correctly modeling the future is very important for tasks that require making decisions of what to do in the future based on what we predict from the past .,evaluation
HyciX9dxM_29,"The proposed method presents a possible way of better modeling the future ,",fact
HyciX9dxM_30,"however , some the results do not clearly back up the claim yet .",evaluation
HyciX9dxM_31,The given score will improve if the authors are able to address the stated issues .,evaluation
BkMvqjYgG_0,"This paper focuses on the problem of "" machine teaching "" , i.e. , how to select a good strategy to select training data points to pass to a machine learning algorithm , for faster learning .",fact
BkMvqjYgG_1,"The proposed approach leverages reinforcement learning by defining the reward as how fast the learner learns , and use policy gradient to update the teacher parameters .",fact
BkMvqjYgG_2,"I find the definition of the "" state "" in this case very interesting .",evaluation
BkMvqjYgG_3,The experimental results seem to show that such a learned teacher strategy makes machine learning algorithms learn faster .,fact
BkMvqjYgG_4,Overall I think that this paper is decent .,evaluation
BkMvqjYgG_5,The angle the authors took is interesting ( essentially replacing one level of the bi-level optimization problem in machine teaching works with a reinforcement learning setup ) .,evaluation
BkMvqjYgG_6,"The problem formulation is mostly reasonable ,",evaluation
BkMvqjYgG_7,and the evaluation seems quite convincing .,evaluation
BkMvqjYgG_8,The paper is well-written :,evaluation
BkMvqjYgG_9,I enjoyed the mathematical formulation ( Section 3 ) .,evaluation
BkMvqjYgG_10,"The authors did a good job of using different experiments ( filtration number analysis , and teaching both the same architecture and a different architecture ) to intuitively explain what their method actually does .",evaluation
BkMvqjYgG_11,"At the same time , though , I see several important issues that need to be addressed if this paper is to be accepted .",evaluation
BkMvqjYgG_12,Details below .,non-arg
BkMvqjYgG_13,"1 . As much as I enjoyed reading Section 3 , it is very redundant .",evaluation
BkMvqjYgG_14,"In some cases it is good to outline a powerful and generic framework ( like the authors did here with defining "" teaching "" in a very broad sense , including selecting good loss functions and hypothesis spaces ) and then explain that the current work focuses on one aspect ( selecting training data points ) .",evaluation
BkMvqjYgG_15,"However , I do not see it being the case here .",fact
BkMvqjYgG_16,"In my opinion , selecting good loss functions and hypothesis spaces are much harder problems than data teaching - except maybe when one use a pre-defined set of possible loss functions and select from it .",evaluation
BkMvqjYgG_17,But that is not very interesting,evaluation
BkMvqjYgG_18,"( if you can propose new loss functions , that would be way cooler ) .",evaluation
BkMvqjYgG_19,"I also do not see how to define an intuitive set of "" states "" in that case .",evaluation
BkMvqjYgG_20,"Therefore , I think this section should be shortened .",request
BkMvqjYgG_21,"I also think that the authors should not discuss the general framework and rather focus on "" data teaching "" ,",request
BkMvqjYgG_22,which is the only focus of the current paper .,fact
BkMvqjYgG_23,The abstract and introduction should also be modified accordingly to more honestly reflect the current contributions .,request
BkMvqjYgG_24,"2 . The authors should do a better job at explaining the details of the state definition , especially the student model features and the combination of data and current learner model .",request
BkMvqjYgG_25,3 . There is only one definition of the reward - related to batch number when the accuracy first exceeds a threshold .,fact
BkMvqjYgG_26,"Is accuracy stable , can it drop back down below the threshold in the next epoch ?",request
BkMvqjYgG_27,"The accuracy on a held-out test set is not guaranteed to be monotonically increasing , right ?",request
BkMvqjYgG_28,Is this a problem in practice ( it seems to happen on your curves ) ?,request
BkMvqjYgG_29,What about other potential reward definitions ?,request
BkMvqjYgG_30,And what would they potentially lead to ?,request
BkMvqjYgG_31,4 . Experimental results are averaged over 5 repeated runs,fact
BkMvqjYgG_32,- a bit too small in my opinion .,evaluation
BkMvqjYgG_33,5 . Can the authors show convergence of the teacher parameter \ theta ?,request
BkMvqjYgG_34,"I think it is important to see how fast the teacher model converges , too .",evaluation
BkMvqjYgG_35,"6 . In some of your experiments , every training method converges to the same accuracy after enough training ( Fig. 2b ) , while in others , not quite ( Fig. 2a and 2c ) .",fact
BkMvqjYgG_36,Why is this the case ?,request
BkMvqjYgG_37,Does it mean that you have not run enough iterations for the baseline methods ?,request
BkMvqjYgG_38,"My intuition is that if the learner algorithm is convex , then ultimately they will all get to the same accuracy level , so the task is just to get there quicker .",evaluation
BkMvqjYgG_39,"I understand that since the learner algorithm is an NN ,",fact
BkMvqjYgG_40,this is not the case,fact
BkMvqjYgG_41,- but more explanation is necessary here,request
BkMvqjYgG_42,- does your method also reduces the empirical possibility to get stuck in local minima ?,request
BkMvqjYgG_43,7 . More explanation is needed towards Fig. 4c .,request
BkMvqjYgG_44,"In this case , using a teacher model trained on a harder task ( CIFAR10 ) leads to much improved student training on a simpler task ( MNIST ) .",fact
BkMvqjYgG_45,Why ?,request
BkMvqjYgG_46,"8 . Although in terms of "" effective training data points "" the proposed method outperforms the other methods ,",fact
BkMvqjYgG_47,"in terms of time ( Fig. 5 ) the difference between it and say , NoTeach , is not that significant ( especially at very high desired accuracy ) .",evaluation
BkMvqjYgG_48,More explanation needed here .,request
BkuT3b9ef_0,This paper investigates meta-learning strategy for automated architecture search in the context of RNN .,fact
BkuT3b9ef_1,"To constraint the architecture search space , authors propose a DSL that specifies the RNN recurrent operations .",fact
BkuT3b9ef_2,This DSL allows to explore RNN architectures using either random search or a reinforcement-learning strategy .,fact
BkuT3b9ef_3,Candidate architectures are ranked using a TreeLSTM that tries to predict the architecture performances .,fact
BkuT3b9ef_4,The top-k architectures are then evaluated by fully training them on a given task .,fact
BkuT3b9ef_5,Authors evaluate their approach on PTB/Wikitext 2 language modeling and Multi30k/IWSLT '16 machine translation .,fact
BkuT3b9ef_6,"In both experiments , authors show that their approach obtains competitive results and can sometime outperforms RNN cells such as GRU/LSTM .",fact
BkuT3b9ef_7,"In the PTB experiment , their architecture however underperforms other LSTM variant in the literatures .",fact
BkuT3b9ef_8,- Quality/Clarity The paper is overall well written and pleasant to read .,evaluation
BkuT3b9ef_9,Few details can be clarified .,evaluation
BkuT3b9ef_10,In particular how did you initialize the weight and bias for both the LSTM/GRU baselines and the found architectures ?,request
BkuT3b9ef_11,Is there other works leveraging RNN that report results on the Multi30k/IWSLT datasets ?,request
BkuT3b9ef_12,You state in paragraph 3.2 that human experts can inject the previous best known architecture when training the ranking networks .,fact
BkuT3b9ef_13,Did you use this in the experiments ?,request
BkuT3b9ef_14,"If yes , what was the impact of this online learning strategy on the final results ?",request
BkuT3b9ef_15,- Originality The idea of using DSL + ranking for architecture search seems novel .,evaluation
BkuT3b9ef_16,- Significance Automated architecture search is a promising way to design new networks .,evaluation
BkuT3b9ef_17,"However , it is not clear why the proposed approach is not able to outperforms other LSTM-based architectures on the PTB task .",evaluation
BkuT3b9ef_18,Could the problem arise from the DSL that constraint too much the search space ?,request
BkuT3b9ef_19,It would be nice to have other tasks that are commonly used as benchmark for RNN to see where this approach stand .,request
BkuT3b9ef_20,"In addition , authors propose both a DSL , a random and RL generator and a ranking function .",fact
BkuT3b9ef_21,It would be nice to disentangle the contributions of the different components .,evaluation
BkuT3b9ef_22,"In particular , did the authors compare the random search vs the RL based generator or the performances of the RL-based generator when the ranking network is not used ?",request
BkuT3b9ef_23,"Although authors do show that they outperform NAScell in one setting ,",fact
BkuT3b9ef_24,it would be nice to have an extended evaluation ( using character level PTB for instance ) .,request
BkTXGMKlf_0,This paper proposes a family of first-order stochastic optimization schemes,fact
BkTXGMKlf_1,based on ( 1 ) normalizing ( batches of ) stochastic gradient descents and ( 2 ) choosing from a step size updating scheme .,fact
BkTXGMKlf_2,"The authors argue that iterative first-order optimization algorithms can be interpreted as a choice of an update direction and a step size ,",fact
BkTXGMKlf_3,so they suggest that one should always normalize the gradient when computing the direction and then choose a step size using the normalized gradient .,fact
BkTXGMKlf_4,"The presentation in the paper is clear ,",evaluation
BkTXGMKlf_5,and the exposition is easy to follow .,evaluation
BkTXGMKlf_6,The authors also do a good job of presenting related work and putting their ideas in the proper context .,evaluation
BkTXGMKlf_7,"The authors also test their proposed method on many datasets ,",fact
BkTXGMKlf_8,which is appreciated .,evaluation
BkTXGMKlf_9,"However , I did n't find the main idea of the paper to be particularly compelling .",evaluation
BkTXGMKlf_10,"The proposed technique is reasonable on its own ,",evaluation
BkTXGMKlf_11,but the empirical results do not come with any measure of statistical significance .,fact
BkTXGMKlf_12,"The authors also do not analyze the sensitivity of the different optimization algorithms to hyperparameter choice , opting to only use the default .",fact
BkTXGMKlf_13,"Moreover , some algorithms were used as benchmarks on some datasets but not others .",fact
BkTXGMKlf_14,"For a primarily empirical paper , every state-of-the-art algorithm should be used as a point of comparison on every dataset considered .",request
BkTXGMKlf_15,These factors altogether render the experiments uninformative in comparing the proposed suite of algorithms to state-of-the-art methods .,evaluation
BkTXGMKlf_16,"The theoretical result in the convex setting is also not data-dependent , despite the fact that it is the normalized gradient version of AdaGrad , which does come with a data-dependent convergence guarantee .",fact
BkTXGMKlf_17,"Given the suite of optimization algorithms in the literature and in use today , any new optimization framework should either demonstrate improved ( or at least matching ) guarantees in some common ( e.g. convex ) settings or definitively outperform state-of-the-art methods on problems that are of widespread interest .",request
BkTXGMKlf_18,"Unfortunately , this paper does neither .",fact
BkTXGMKlf_19,"Because of these points , I do not feel the quality , originality , and significance of the work to be high enough to merit acceptance .",evaluation
BkTXGMKlf_20,"Some specific comments : p . 2 : "" adaptive feature-dependent step size has attracted lots of attention "" .",quote
BkTXGMKlf_21,"When you apply feature-dependent step sizes , you are effectively changing the direction of the gradient ,",fact
BkTXGMKlf_22,"so your meta learning formulation , as posed , does n't make as much sense .",evaluation
BkTXGMKlf_23,"p . 2 : "" we hope the resulting methods can benefit from both techniques "" .",quote
BkTXGMKlf_24,What reason do you have to hope for this ?,non-arg
BkTXGMKlf_25,Why should they be complimentary ?,non-arg
BkTXGMKlf_26,"Existing optimization techniques are based on careful design and coupling of gradients or surrogate gradients , with specific learning rate schedules .",fact
BkTXGMKlf_27,Arbitrarily mixing the two does n't seem to be theoretically well-motivated .,evaluation
BkTXGMKlf_28,"p . 2 : "" numerical results shows that normalized gradient always helps to improve the performance of the original methods when the network structure is deep "" .",quote
BkTXGMKlf_29,It would be great to provide some intuition for this .,request
BkTXGMKlf_30,"p . 2 : "" we also provide a convergence proof under this framework when the problem is convex and the stepsize is adaptive "" .",quote
BkTXGMKlf_31,The result that you prove guarantees a <VAR> convergence rate .,fact
BkTXGMKlf_32,"On the other hand , the AdaGrad algorithm guarantees a data-dependent bound that is <VAR>",fact
BkTXGMKlf_33,but can also be much smaller .,fact
BkTXGMKlf_34,This suggests that there is no theoretical motivation to use NGD with an adaptive step size over AdaGrad .,evaluation
BkTXGMKlf_35,"p . 2-3 : "" NGD can find a eps-optimal solution .... when the objective function is quasi-convex . .... extended NGD for upper semi-continuous quasiconvex objective functions ... "" .",quote
BkTXGMKlf_36,This seems like a typo .,evaluation
BkTXGMKlf_37,How are results that go from quasi-convex to upper semi-continuous quasi-convex an extension ?,evaluation
BkTXGMKlf_38,p . 3 : There should be a reference for RMSProp .,request
BkTXGMKlf_39,"p . 3 : "" where each block of parameters <VAR> can be viewed as parameters associated to the ith layer in the network "" .",quote
BkTXGMKlf_40,Why is layer parametrization ( and later on normalization ) a good way idea ?,non-arg
BkTXGMKlf_41,There should be either a reference or an explanation .,request
BkTXGMKlf_42,"p . 4 : "" <EQN> "" .",quote
BkTXGMKlf_43,Should these subscripts be superscripts ?,non-arg
BkTXGMKlf_44,"p . 4 : "" For all the algorithms , we use their default settings . """,quote
BkTXGMKlf_45,"This seems insufficient for an empirical paper ,",evaluation
BkTXGMKlf_46,since most problems often involve some amount of hyperparameter tuning .,fact
BkTXGMKlf_47,How sensitive is each method to the choice of hyperparameters ?,non-arg
BkTXGMKlf_48,What about the impact of initialization ?,non-arg
BkTXGMKlf_49,p . 4-8 : None of the experimental results have error bars or any measure of statistical significance .,fact
BkTXGMKlf_50,"p . 5 : "" <VAR> ... is a variant of the <VAR> method "" .",quote
BkTXGMKlf_51,This method is never motivated .,fact
BkTXGMKlf_52,p . 5-6 : Why are SGD and Adam used for MNIST but not on CIFAR ?,non-arg
BkTXGMKlf_53,"p . 5 : "" we chose the best heyper-paerameter from the 56 layer residual network . """,quote
BkTXGMKlf_54,"Apart from the typos , are these parameters chosen from the training set or the test set ?",non-arg
BkTXGMKlf_55,p . 6 : Why is n't Adam tested on ImageNet ?,non-arg
HkgrJeEgM_0,"This paper studies the question : Why does SGD on deep network is often successful , despite the fact that the objective induces bad local minima ?",fact
HkgrJeEgM_1,The approach in this paper is to study a standard MNN with one hidden layer .,fact
HkgrJeEgM_2,"They show that in an overparametrized regime , where the number of parameters is logarithmically larger than the number of parameters in the input , the ratio between the number of ( bad ) local minima to the number of global minima decays exponentially .",fact
HkgrJeEgM_3,"They show this for a piecewise linear activation function , and input drawn from a standard Normal distribution .",fact
HkgrJeEgM_4,"Their improvement over previous work is that the required overparameterization is fairly moderate , and that the network that they considered is similar to ones used in practice .",evaluation
HkgrJeEgM_5,"This result seems interesting ,",evaluation
HkgrJeEgM_6,"although it is clearly not sufficient to explain even the success on the setting studied in this paper ,",evaluation
HkgrJeEgM_7,since the number of minima of a certain type does not correspond to the probability of the SGD ending in one :,fact
HkgrJeEgM_8,"to estimate the latter , the size of each basin of attraction should be taken into account .",fact
HkgrJeEgM_9,The authors are aware of this point and mention it as a disadvantage .,fact
HkgrJeEgM_10,"However , since this question in general is a difficult one ,",evaluation
HkgrJeEgM_11,any progress might be considered interesting .,evaluation
HkgrJeEgM_12,"Hopefully , in future work it would be possible to also bound the probability of starting in one of the basins of attraction of bad local minima .",evaluation
HkgrJeEgM_13,"The paper is well written and well presented ,",evaluation
HkgrJeEgM_14,"and the limitations of the approach , as well as its advantages over previous work , are clearly explained .",evaluation
HkgrJeEgM_15,"As I am not an expert on the previous works in this field , my judgment relies mostly on this work and its representation of previous work .",non-arg
HkgrJeEgM_16,I did not verify the proofs in the appendix .,non-arg
HyjN-YPlz_0,The manuscript proposes two objective functions based on the manifold assumption as defense mechanisms against adversarial examples .,fact
HyjN-YPlz_1,The two objective functions are based on assigning low confidence values to points that are near or off the underlying ( learned ) data manifold while assigning high confidence values to points lying on the data manifold .,fact
HyjN-YPlz_2,"In particular , for an adversarial example that is distinguishable from the points on the manifold and assigned a low confidence by the model , is projected back onto the designated manifold such that the model assigns it a high confidence value .",fact
HyjN-YPlz_3,The authors claim that the two objective functions proposed in this manuscript provide such a projection onto the desired manifold and assign high confidence for these adversarial points .,fact
HyjN-YPlz_4,"These mechanisms , together with the so-called shell wrapper around the model ( a deep learning model in this case ) will provide the desired defense mechanism against adversarial examples .",evaluation
HyjN-YPlz_5,The manuscript at the current stage seems to be a preliminary work that is not well matured yet .,evaluation
HyjN-YPlz_6,The manuscript is overly verbose and the arguments seem to be weak and not fully developed yet .,evaluation
HyjN-YPlz_7,"More importantly , the experiments are very preliminary and there is much more room to deliver more comprehensive and compelling experiments .",request
BkTles9xM_0,The paper proposes a new evaluation measure for evaluating GANs .,fact
BkTles9xM_1,"Specifically , the paper proposes generating synthetic images using GAN , training a classifier ( for an auxiliary task , not the real vs fake discriminator ) and measuring the performance of this classifier on held out real data .",fact
BkTles9xM_2,"While the idea of using a downstream classification task to evaluate the quality of generative models has been explored before ( e.g. semi-supervised learning ) ,",fact
BkTles9xM_3,I think that this is the first paper to evaluate GANs using such an evaluation metric .,evaluation
BkTles9xM_4,I 'm not super convinced that this is an useful evaluation metric as the absolute number is somewhat to interpret and dependent on the details of the classifier used .,evaluation
BkTles9xM_5,The results in Table 1 change quite a bit depending on the classifier .,evaluation
BkTles9xM_6,It would be useful to add a discussion of the failure modes of the proposed metric .,request
BkTles9xM_7,It seems like a generator which generates samples close to the classification boundary ( but drops examples far away from the boundary ) could still achieve a high score under this metric .,evaluation
BkTles9xM_8,"In the experiments , were different architectures used for different GAN variants ?",non-arg
BkTles9xM_9,I think the mode-collapse evaluation metrics in MR-GAN are worth discussing in Section 2.1,request
BkTles9xM_10,Mode Regularized Generative Adversarial Networks <URL>,reference
B1IwI-2xz_0,This paper proposes an empirical measure of the intrinsic dimensionality of a neural network problem .,fact
B1IwI-2xz_1,"Taking the full dimensionality to be the total number of parameters of the network model , the authors assess intrinsic dimensionality by randomly projecting the network to a domain with fewer parameters ( corresponding to a low-dimensional subspace within the original parameter ) , and then training the original network while restricting the projections of its parameters to lie within this subspace .",fact
B1IwI-2xz_2,Performance on this subspace is then evaluated relative to that over the full parameter space ( the baseline ) .,fact
B1IwI-2xz_3,"As an empirical standard , the authors focus on the subspace dimension that achieves a performance of 90 % of the baseline .",fact
B1IwI-2xz_4,"The authors then test out their measure of intrinsic dimensionality for fully-connected networks and convolutional networks , for several well-known datasets ,",fact
B1IwI-2xz_5,and draw some interesting conclusions .,evaluation
B1IwI-2xz_6,Pros : * This paper continues the recent research trend towards a better characterization of neural networks and their performance .,fact
B1IwI-2xz_7,"The authors show a good awareness of the recent literature ,",evaluation
B1IwI-2xz_8,"and to the best of my knowledge , their empirical characterization of the number of latent parameters is original .",fact
B1IwI-2xz_9,"* The characterization of the number of latent variables is an important one ,",evaluation
B1IwI-2xz_10,and their measure does perform in a way that one would intuitively expect .,evaluation
B1IwI-2xz_11,"For example , as reported by the authors , when training a fully-connected network on the MNIST image dataset , shuffling pixels does not result in a change in their intrinsic dimensionality .",fact
B1IwI-2xz_12,For a convolutional network the observed 3-fold rise in intrinsic dimension is explained by the authors as due to the need to accomplish the classification task while respecting the structural constraints of the convnet .,fact
B1IwI-2xz_13,* The proposed measures seem very practical -,evaluation
B1IwI-2xz_14,"training on random projections uses far fewer parameters than in the original space ( the baseline ) ,",fact
B1IwI-2xz_15,and presumably the cost of determining the intrinsic dimensionality would presumably be only a fraction of the cost of this baseline training .,fact
B1IwI-2xz_16,"* Except for the occasional typo or grammatical error , the paper is well-written and organized .",evaluation
B1IwI-2xz_17,"The issues are clearly identified , for the most part ( but see below ... ) .",evaluation
B1IwI-2xz_18,"Cons : * In the main paper , the authors perform experiments and draw conclusions without taking into account the variability of performance across different random projections .",fact
B1IwI-2xz_19,"Variance should be taken into account explicitly , in presenting experimental results and in the definition and analysis of the empirical intrinsic dimension itself .",request
B1IwI-2xz_20,"How often does a random projection lead to a high-quality solution , and how often does it not ?",non-arg
B1IwI-2xz_21,* The authors are careful to point out that training in restricted subspaces can not lead to an optimal solution for the full parameter domain unless the subspace intersects the optimal solution region ( which in general can not be guaranteed ) .,fact
B1IwI-2xz_22,"In their experiments ( FC networks of varying depths and layer widths for the MNIST dataset ) , between projected and original solutions achieving 90 % of baseline performance , they find an order of magnitude gap in the number of parameters needed .",fact
B1IwI-2xz_23,This calls into question the validity of random projection as an empirical means of categorizing the intrinsic dimensionality of a neural network .,evaluation
B1IwI-2xz_24,* The authors then go on to propose that compression of the network be achieved by random projection to a subspace of dimensionality greater than or equal to the intrinsic dimension .,fact
B1IwI-2xz_25,"However , I do n't think that they make a convincing case for this approach .",evaluation
B1IwI-2xz_26,"Again , variation is the difficulty :",evaluation
B1IwI-2xz_27,two different projective subspaces of the same dimensionality can lead to solutions that are extremely different in character or quality .,fact
B1IwI-2xz_28,"How then can we be sure that our compressed network can be reconstituted into a solution of reasonable quality , even when its dimensionality greatly exceeds the intrinsic dimension ?",evaluation
B1IwI-2xz_29,"* The authors argue for a relationship between intrinsic dimensionality and the minimum description length ( MDL ) of their solution ,",fact
B1IwI-2xz_30,in that the intrinsic dimensionality should serve as an upper bound on the MDL .,fact
B1IwI-2xz_31,However they do n't formally acknowledge that there is no standard relationship between the number of parameters and the actual number of bits needed to represent the model -,fact
B1IwI-2xz_32,"it varies from setting to setting , with some parameters potentially requiring many more bits than others .",fact
B1IwI-2xz_33,"And given this uncertain connection , and given the lack of consideration given to variation in the proposed measure of intrinsic dimensionality , it is hard to accept that "" there is some rigor behind "" their conclusion that LeNet is better than FC networks for classification on MNIST",evaluation
B1IwI-2xz_34,because its empirical intrinsic dimensionality score is lower .,fact
B1IwI-2xz_35,* The experimental validation of their measure of intrinsic dimension could be made more extensive .,request
B1IwI-2xz_36,"In the main paper , they use three image datasets - MNIST , CIFAR-10 and ImageNet .",fact
B1IwI-2xz_37,"In the supplemental information , they report intrinsic dimensions for reinforcement learning and other training tasks on four other sets .",fact
B1IwI-2xz_38,"Overall , I think that this characterization does have the potention to give insights into the performance of neural networks , provided that variation across projections is properly taken into account .",evaluation
B1IwI-2xz_39,"For now , more work is needed .",evaluation
H15qgiFgf_0,"This work identifies a mistake in the existing proof of convergence of Adam ,",fact
H15qgiFgf_1,which is among the most popular optimization methods in deep learning .,evaluation
H15qgiFgf_2,"Moreover , it gives a simple 1-dimensional counterexample with linear losses on which Adam does not converge .",fact
H15qgiFgf_3,"The same issue also affects RMSprop ,",fact
H15qgiFgf_4,which may be viewed as a special case of Adam without momentum .,evaluation
H15qgiFgf_5,"The problem with Adam is that the "" learning rate "" matrices <VAR> are not monotonically decreasing .",fact
H15qgiFgf_6,"A new method , called AMSGrad is therefore proposed , which modifies Adam by forcing these matrices to be decreasing .",fact
H15qgiFgf_7,It is then shown that AMSGrad does satisfy essentially the same convergence bound as the one previously claimed for Adam .,fact
H15qgiFgf_8,Experiments and simulations are provided that support the theoretical analysis .,fact
H15qgiFgf_9,"Apart from some issues with the technical presentation ( see below ) ,",evaluation
H15qgiFgf_10,the paper is well-written .,evaluation
H15qgiFgf_11,"Given the popularity of Adam , I consider this paper to make a very interesting observation .",evaluation
H15qgiFgf_12,I further believe all issues with the technical presentation can be readily addressed .,evaluation
H15qgiFgf_13,"Issues with Technical Presentation : - All theorems should explicitly state the conditions they require instead of referring to "" all the conditions in ( Kingma & Ba , 2015 ) "" .",request
H15qgiFgf_14,- Theorem 2 is a repetition of Theorem 1 ( except for additional conditions ) .,fact
H15qgiFgf_15,"- The proof of Theorem 3 assumes there are no projections ,",fact
H15qgiFgf_16,so this should be stated as part of its conditions .,request
H15qgiFgf_17,"( The claim in footnote 2 that they can be handled seems highly plausible ,",evaluation
H15qgiFgf_18,but you should be up front about the limitations of your results . ),request
H15qgiFgf_19,"- The regret bound Theorem 4 establishes convergence of the optimization method ,",fact
H15qgiFgf_20,so it plays the role of a sanity check .,evaluation
H15qgiFgf_21,"However , it is strictly worse than the regret bound <VAR> for online gradient descent [ Zinkevich ,2003 ] ,",fact
H15qgiFgf_22,so it can not explain why the proposed AMSgrad method might be adaptive .,fact
H15qgiFgf_23,( The method may indeed be adaptive in some sense ;,evaluation
H15qgiFgf_24,I am just saying the * bound * does not express that .,evaluation
H15qgiFgf_25,This is also not a criticism of the current paper ;,non-arg
H15qgiFgf_26,the same remark also applies to the previously claimed regret bound for Adam . ),non-arg
H15qgiFgf_27,- The discussion following Corollary 1 suggests that <VAR> might be much smaller than d G_infty .,evaluation
H15qgiFgf_28,"This is true ,",fact
H15qgiFgf_29,"but we should always expect it to be at least a constant ,",fact
H15qgiFgf_30,"because hat { v } _ { t , i } is monotonically increasing by definition of the algorithm ,",fact
H15qgiFgf_31,so the bound does not get better than <VAR> .,fact
H15qgiFgf_32,"It is also suggested that <VAR> might be much smaller than dG_infty ,",evaluation
H15qgiFgf_33,"but this is very unlikely ,",evaluation
H15qgiFgf_34,"because this term will typically grow like <VAR> , unless the data are extremely sparse ,",evaluation
H15qgiFgf_35,so we should at least expect some dependence on T.,evaluation
H15qgiFgf_36,"- In the proof of Theorem 1 , the initial point is taken to be <EQN> ,",fact
H15qgiFgf_37,"which is perfectly fine ,",evaluation
H15qgiFgf_38,"but it is not "" without loss of generality "" , as claimed .",fact
H15qgiFgf_39,This should be stated in the statement of the Theorem .,request
H15qgiFgf_40,- The proof of Theorem 6 in appendix B only covers <EQN> .,fact
H15qgiFgf_41,"If it is "" easy to show "" that the same construction also works for other epsilon , as claimed , then please provide the proof for general epsilon .",request
H15qgiFgf_42,"Other remarks : - Theoretically , nonconvergence of Adam seems a severe problem .",evaluation
H15qgiFgf_43,Can you speculate on why this issue has not prevented its widespread adoption ?,request
H15qgiFgf_44,Which factors might mitigate the issue in practice ?,request
H15qgiFgf_45,- Please define <VAR>,request
H15qgiFgf_46,- I would recommend sticking with standard linear algebra notation for the sqrt and the inverse of a matrix and simply using <VAR> and <VAR> instead of 1/A and <VAR> .,request
H15qgiFgf_47,"- In theorems 1,2,3 , I would recommend stating the dimension ( <EQN> ) of your counterexamples ,",request
H15qgiFgf_48,which makes them very nice !,evaluation
H15qgiFgf_49,Minor issues : - Check accent on Nicol \ ` o Cesa-Bianchi in bibliography .,request
H15qgiFgf_50,"- Near the end of the proof of Theorem 6 : I believe you mean Adam suffers a "" regret "" instead of a "" loss "" of at least 2C-4 .",request
H15qgiFgf_51,Also <EQN> is trivial in the second but last display .,evaluation
Bk6nbuf-M_0,The authors use deep learning to learn a surrogate model for the motion vector in the advection-diffusion equation that they use to forecast sea surface temperature .,fact
Bk6nbuf-M_1,"In particular , they use a CNN encoder-decoder to learn a motion field , and a warping function from the last component to provide forecasting .",fact
Bk6nbuf-M_2,I like the idea of using deep learning for physical equations .,evaluation
Bk6nbuf-M_3,I would like to see a description of the algorithm with the pseudo-code in order to understand the flow of the method .,request
Bk6nbuf-M_4,I got confused at several points,evaluation
Bk6nbuf-M_5,because it was not clear what was exactly being estimated with the CNN .,evaluation
Bk6nbuf-M_6,Having an algorithmic environment would make the description easier .,fact
Bk6nbuf-M_7,"I know that authors are going to publish the code ,",fact
Bk6nbuf-M_8,but this is not enough at this point of the revision .,evaluation
Bk6nbuf-M_9,Physical processes in Machine learning have been studied from the perspective of Gaussian processes .,fact
Bk6nbuf-M_10,"Just to mention a couple of references “ Linear latent force models using Gaussian processes ” and "" Numerical Gaussian Processes for Time-dependent and Non-linear Partial Differential Equations """,reference
Bk6nbuf-M_11,"In Theorem 2 , do you need to care about boundary conditions for your equation ?",request
Bk6nbuf-M_12,I did n’t see any mention to those in the definition for <VAR> .,fact
Bk6nbuf-M_13,You only mention initial conditions .,fact
Bk6nbuf-M_14,How do you estimate the diffusion parameter D ?,request
Bk6nbuf-M_15,Are you assuming isotropic diffusion ?,request
Bk6nbuf-M_16,Is that realistic ?,evaluation
Bk6nbuf-M_17,Can you provide more details about how you run the data assimilation model in the experiments ?,request
Bk6nbuf-M_18,Did you use your own code ?,non-arg
BkEYMCPlG_0,"The authors present RDA , the Recurrent Discounted Attention unit , that improves upon RWA , the earlier introduced Recurrent Weighted Average unit , by adding a discount factor .",fact
BkEYMCPlG_1,"While the RWA was an interesting idea with bad results ( far worse than the standard GRU or LSTM with standard attention except for hand-picked tasks ) ,",evaluation
BkEYMCPlG_2,the RDA brings it more on-par with the standard methods .,evaluation
BkEYMCPlG_3,"On the positive side , the paper is clearly written and adding discount to RWA , while a small change , is original .",evaluation
BkEYMCPlG_4,"On the negative side , in almost all tasks the RDA is on par or worse than the standard GRU -",evaluation
BkEYMCPlG_5,"except for MultiCopy where it trains faster , but not to better results",fact
BkEYMCPlG_6,and it looks like the difference is between few and very-few training steps anyway .,evaluation
BkEYMCPlG_7,"The most interesting result is language modeling on Hutter Prize Wikipedia ,",evaluation
BkEYMCPlG_8,where RDA very significantly improves upon RWA -,fact
BkEYMCPlG_9,"but again , only matches a standard GRU or LSTM .",fact
BkEYMCPlG_10,"So the results are not strongly convincing ,",evaluation
BkEYMCPlG_11,and the paper lacks any mention of newer work on attention .,fact
BkEYMCPlG_12,"This year strong improvements over state-of-the-art have been achieved using attention for translation ( "" Attention is All You Need "" ) and image classification ( e.g. , Non-local Neural Networks , but also others in ImageNet competition ) .",fact
BkEYMCPlG_13,"To make the evaluation convincing enough for acceptance , RDA should be combined with those models and evaluated more competitively on multiple widely-studied tasks .",request
Bk-lFRWWz_0,The authors propose reducing the number of parameters learned by a deep network by setting up sparse connection weights in classification layers .,fact
Bk-lFRWWz_1,Numerical experiments show that such sparse networks can have similar performance to fully connected ones .,fact
Bk-lFRWWz_2,They introduce a concept of “ scatter ” that correlates with network performance .,fact
Bk-lFRWWz_3,"Although I found the results useful and potentially promising ,",evaluation
Bk-lFRWWz_4,I did not find much insight in this paper .,evaluation
Bk-lFRWWz_5,It was not clear to me why scatter ( the way it is defined in the paper ) would be a useful performance proxy anywhere but the first classification layer .,evaluation
Bk-lFRWWz_6,"Once the signals from different windows are intermixed , how do you even define the windows ?",evaluation
Bk-lFRWWz_7,Minor Second line of Section 2.1 : “ lesser ” - > less or fewer,request
BkYwge9ef_0,"There could be an interesting idea here ,",evaluation
BkYwge9ef_1,but the limitations and applicability of the proposed approach are not clear yet .,evaluation
BkYwge9ef_2,More analysis should be done to clarify its potential .,request
BkYwge9ef_3,"Besides , the paper seriously needs to be reworked .",request
BkYwge9ef_4,"The text in general , but also the notation , should be improved .",request
BkYwge9ef_5,"In my opinion , the authors should explain how to apply their algorithm to more general network architectures , and test it , in particular to convnets .",request
BkYwge9ef_6,An experiment on a modern dataset beyond MNIST would also be a welcome addition .,request
BkYwge9ef_7,Some comments : - The method is present as a fully-connected network training procedure .,fact
BkYwge9ef_8,"But the resulting network is not really fully-connected , but modular .",fact
BkYwge9ef_9,This is clear in Fig. 1 and in the explanation in Sect. 3.1 .,fact
BkYwge9ef_10,The newly added hidden neurons at every iteration do not project to the previous pool of hidden neurons .,fact
BkYwge9ef_11,It should be stressed that the networks end up with this non-conventional “ tiled ” architecture .,fact
BkYwge9ef_12,"Are there studies where the capacity of such networks is investigated , when all the weights are trained concurrently .",non-arg
BkYwge9ef_13,- It was n’t clear to me whether the memory reallocation could be easily implemented in hardware .,evaluation
BkYwge9ef_14,A few references or remarks on this issue would be welcome .,request
BkYwge9ef_15,- The work “ Efficient supervised learning in networks with binary synapses ” by Baldassi et al. ( PNAS 2007 ) should be cited .,request
BkYwge9ef_16,"Although usually ignored by the deep learning community , it actually was a pioneering study on the use of low resolution weights during inference while allowing for auxiliary variables during learning .",fact
BkYwge9ef_17,"- Coming back my main point above , I did n’t really get the discussion on Sect. 5.3 .",evaluation
BkYwge9ef_18,Why did n’t the authors test their algorithm on a convnet ?,non-arg
BkYwge9ef_19,Are there any obstacles in doing so ?,non-arg
BkYwge9ef_20,"It seems quite important to understand this point ,",evaluation
BkYwge9ef_21,as the paper appeals to technical applications and convolution seems hard to sidestep currently .,evaluation
BkYwge9ef_22,- Fig. 3 : xx-axis : define storage efficiency and storage requirement .,request
BkYwge9ef_23,- Fig. 4 : What ’s an RSBL ?,request
BkYwge9ef_24,Acronyms should be defined .,request
BkYwge9ef_25,"- Overall , language and notation should really be refined .",request
BkYwge9ef_26,"I had a hard time reading Algorithm 1 ,",evaluation
BkYwge9ef_27,as the notation is not even defined anywhere .,fact
BkYwge9ef_28,And this problem extends throughout the paper .,fact
BkYwge9ef_29,"For example , just looking at Sect. 4.1 , “ training and testing data x is normalized … ” , if x is not properly defined , it ’s best to omit it ;",request
BkYwge9ef_30,"“ … 2-dimentonal … ” , at least major typos should be scanned and corrected .",request
B1qbgHcxz_0,Summary : This paper proposes a simple recipe to preserve proximity to zero mean for activations in deep neural networks .,fact
B1qbgHcxz_1,"The proposal is to replace the non-linearity in half of the units in each layer with its "" bipolar "" version --",fact
B1qbgHcxz_2,one that is obtained by flipping the function on both axes .,fact
B1qbgHcxz_3,"The technique is tested on deep stacks of recurrent layers , and on convolutional networks with depth of 28 , showing that improved results over the baseline networks are obtained .",fact
B1qbgHcxz_4,Clarity : The paper is easy to read .,evaluation
B1qbgHcxz_5,The plots in Fig. 2 and the appendix are quite helpful in improving presentation .,evaluation
B1qbgHcxz_6,The experimental setups are explained in detail .,evaluation
B1qbgHcxz_7,Quality and significance : The main idea from this paper is simple and intuitive .,evaluation
B1qbgHcxz_8,"However , the experiments to support the idea do not seem to match the motivation of the paper .",fact
B1qbgHcxz_9,"As stated in the beginning of the paper , the motivation behind having close to zero mean activations is that this is expected to speed up training using gradient descent .",fact
B1qbgHcxz_10,"However , the presented results focus on the performance on held-out data instead of improvements in training speed .",fact
B1qbgHcxz_11,This is especially the case for the RNN experiments .,fact
B1qbgHcxz_12,"For the CIFAR-10 experiment , the training loss curves do show faster initial progress in learning .",fact
B1qbgHcxz_13,"However , it is unclear that overall training time can be reduced with the help of this technique .",evaluation
B1qbgHcxz_14,"To evaluate this speed up effect , the dependence on the choice of learning rate and other hyperparameters should also be considered .",request
B1qbgHcxz_15,"Nevertheless , it is interesting to note the result that the proposed approach converts a deep network that does not train into one which does in many cases .",fact
B1qbgHcxz_16,The method appears to improve the training for moderately deep convolutional networks without batch normalization,fact
B1qbgHcxz_17,"( although this is tested on a single dataset ) ,",fact
B1qbgHcxz_18,but is not practically useful yet,evaluation
B1qbgHcxz_19,since the regularization benefits of Batch Normalization are also taken away .,fact
Bk_UdcKxf_0,"* Summary * The paper proposes to use hyper-networks [ Ha et al. 2016 ] for the tuning of hyper-parameters , along the lines of [ Brock et al. 2017 ] .",fact
Bk_UdcKxf_1,"The core idea is to have a side neural network sufficiently expressive to learn the ( large-scale , matrix-valued ) mapping from a given configuration of hyper-parameters to the weights of the model we wish to tune .",fact
Bk_UdcKxf_2,"The paper gives a theoretical justification of its approach ,",fact
Bk_UdcKxf_3,and then describes several variants of its core algorithm which mix the training of the hyper-networks together with the optimization of the hyper-parameters themselves .,fact
Bk_UdcKxf_4,"Finally , experiments based on MNIST illustrate the properties of the proposed approach .",fact
Bk_UdcKxf_5,"While the core idea may appear as appealing ,",evaluation
Bk_UdcKxf_6,the paper suffers from several flaws ( as further detailed afterwards ) :,evaluation
Bk_UdcKxf_7,- Insufficient related work,evaluation
Bk_UdcKxf_8,- Correctness/rigor of Theorem 2.1,fact
Bk_UdcKxf_9,"- Clarity of the paper ( e.g. , Sec. 2.4 )",evaluation
Bk_UdcKxf_10,- Experiments look somewhat artificial,evaluation
Bk_UdcKxf_11,- How scalable is the proposed approach in the perspective of tuning models way larger/more complex than those treated in the experiments ?,evaluation
Bk_UdcKxf_12,"* Detailed comments * - "" ... and training the model to completion . "" and "" This is wasteful , since it trains the model from scratch each time ... "" ( and similar statement in Sec. 2.1 ) :",quote
Bk_UdcKxf_13,Those statements are quite debatable .,evaluation
Bk_UdcKxf_14,"There are lines of work , e.g. , in Bayesian optimization , to model early stopping/learning curves ( e.g. , Domhan2014 , Klein2017 and references therein ) and where training procedures are explicitly resumed ( e.g. , Swersky2014 , Li2016 ) .",fact
Bk_UdcKxf_15,The paper should reformulate its statements in the light of this literature .,request
Bk_UdcKxf_16,"- "" Uncertainty could conceivably be incorporated into the hypernet ... "" .",quote
Bk_UdcKxf_17,"This seems indeed an important point ,",evaluation
Bk_UdcKxf_18,"but it does not appear as clear how to proceed ( e.g. , uncertainty on <VAR> which later needs to propagated to L_val ) ;",evaluation
Bk_UdcKxf_19,could the authors perhaps further elaborate ?,request
Bk_UdcKxf_20,- I am concerned about the rigor/correctness of Theorem 2.1 ;,evaluation
Bk_UdcKxf_21,"for instance , how is the continuity of the best-response exploited ?",evaluation
Bk_UdcKxf_22,"Also , throughout the paper , the argmin is defined as if it was a singleton",fact
Bk_UdcKxf_23,"while in practice it is rather a set-valued mapping ( except if there is a unique minimizer for L_train ( . , lambda ) ,",fact
Bk_UdcKxf_24,which is unlikely to be the case given the nature of the considered neural-net model ) .,evaluation
Bk_UdcKxf_25,"In the same vein , Jensen 's inequality states that <EQN> for some convex function g and random variable X ;",fact
Bk_UdcKxf_26,"how does it precisely translate into the paper 's setting ( convexity , which function g , etc. ) ?",request
Bk_UdcKxf_27,"- Specify in Alg . 1 that "" hyperopt "" refers to a generic hyper-parameter procedure .",request
Bk_UdcKxf_28,- More details should be provided to better understand Sec. 2.4 .,request
Bk_UdcKxf_29,"At the moment , it is difficult to figure out ( and potentially reproduce ) the model which is proposed .",evaluation
Bk_UdcKxf_30,- The training procedure in Sec. 4.2 seems quite ad hoc ;,evaluation
Bk_UdcKxf_31,how sensitive was the overall performance with respect to the optimization strategy ?,request
Bk_UdcKxf_32,"For instance , in 4.2 and 4.3 , different optimization parameters are chosen .",fact
Bk_UdcKxf_33,"- typo : "" weight decay is applied the ... "" -- > "" weight decay is applied to the ... """,fact
Bk_UdcKxf_34,"- "" a standard Bayesian optimization implementation from sklearn "" : Could more details be provided ?",request
Bk_UdcKxf_35,( there does not seem to be implementation there <URL> to the best of my knowledge ),fact
Bk_UdcKxf_36,- The experimental set up looks a bit far-fetched and unrealistic :,evaluation
Bk_UdcKxf_37,"first scalar , than diagonal and finally matrix-weighted regularization schemes .",fact
Bk_UdcKxf_38,"While the first two may be used in practice ,",fact
Bk_UdcKxf_39,the third scheme is not used in practice to the best of my knowledge .,fact
Bk_UdcKxf_40,"- typo : "" fit a hypernet same dataset . "" -- > "" fit a hypernet on the same dataset . """,request
Bk_UdcKxf_41,- ( Franceschi2017 ) could be added to the related work section .,request
Bk_UdcKxf_42,* References * <CIT>,reference
Bk_UdcKxf_43,<CIT>,reference
Bk_UdcKxf_44,<CIT>,reference
Bk_UdcKxf_45,<CIT>,reference
Bk_UdcKxf_46,<CIT>,reference
HJNeoqYNG_0,"This paper focuses on the problem of "" machine teaching "" ,",fact
HJNeoqYNG_1,"i.e. , how to select a good strategy to select training data points to pass to a machine learning algorithm , for faster learning .",fact
HJNeoqYNG_2,"The proposed approach leverages reinforcement learning by defining the reward as how fast the learner learns ,",fact
HJNeoqYNG_3,and use policy gradient to update the teacher parameters .,fact
HJNeoqYNG_4,"I find the definition of the "" state "" in this case very interesting .",evaluation
HJNeoqYNG_5,The experimental results seem to show that such a learned teacher strategy makes machine learning algorithms learn faster .,evaluation
HJNeoqYNG_6,Overall I think that this paper is decent .,evaluation
HJNeoqYNG_7,The angle the authors took is interesting ( essentially replacing one level of the bi-level optimization problem in machine teaching works with a reinforcement learning setup ) .,evaluation
HJNeoqYNG_8,"The problem formulation is mostly reasonable ,",evaluation
HJNeoqYNG_9,and the evaluation seems quite convincing .,evaluation
HJNeoqYNG_10,The paper is well-written :,evaluation
HJNeoqYNG_11,I enjoyed the mathematical formulation ( Section 3 ) .,evaluation
HJNeoqYNG_12,"The authors did a good job of using different experiments ( filtration number analysis , and teaching both the same architecture and a different architecture ) to intuitively explain what their method actually does .",evaluation
HJNeoqYNG_13,"At the same time , though , I see several important issues that need to be addressed if this paper is to be accepted .",evaluation
HJNeoqYNG_14,"Details below . 1 . As much as I enjoyed reading Section 3 , it is very redundant .",evaluation
HJNeoqYNG_15,In some cases it is good to outline a powerful and generic framework,request
HJNeoqYNG_16,"( like the authors did here with defining "" teaching "" in a very broad sense , including selecting good loss functions and hypothesis spaces )",fact
HJNeoqYNG_17,and then explain that the current work focuses on one aspect ( selecting training data points ) .,request
HJNeoqYNG_18,"However , I do not see it being the case here .",evaluation
HJNeoqYNG_19,"In my opinion , selecting good loss functions and hypothesis spaces are much harder problems than data teaching - except maybe when one use a pre-defined set of possible loss functions and select from it .",evaluation
HJNeoqYNG_20,But that is not very interesting,evaluation
HJNeoqYNG_21,"( if you can propose new loss functions , that would be way cooler ) .",request
HJNeoqYNG_22,"I also do not see how to define an intuitive set of "" states "" in that case .",evaluation
HJNeoqYNG_23,"Therefore , I think this section should be shortened .",request
HJNeoqYNG_24,"I also think that the authors should not discuss the general framework and rather focus on "" data teaching "" ,",request
HJNeoqYNG_25,which is the only focus of the current paper .,evaluation
HJNeoqYNG_26,The abstract and introduction should also be modified accordingly to more honestly reflect the current contributions .,request
HJNeoqYNG_27,"2 . The authors should do a better job at explaining the details of the state definition ,",request
HJNeoqYNG_28,especially the student model features and the combination of data and current learner model .,request
HJNeoqYNG_29,3 . There is only one definition of the reward – related to batch number when the accuracy first exceeds a threshold .,fact
HJNeoqYNG_30,"Is accuracy stable ,",non-arg
HJNeoqYNG_31,can it drop back down below the threshold in the next epoch ?,request
HJNeoqYNG_32,"The accuracy on a held-out test set is not guaranteed to be monotonically increasing , right ?",non-arg
HJNeoqYNG_33,Is this a problem in practice ( it seems to happen on your curves ) ?,non-arg
HJNeoqYNG_34,What about other potential reward definitions ?,non-arg
HJNeoqYNG_35,And what would they potentially lead to ?,non-arg
HJNeoqYNG_36,4 . Experimental results are averaged over 5 repeated runs,fact
HJNeoqYNG_37,- a bit too small in my opinion .,evaluation
HJNeoqYNG_38,5 . Can the authors show convergence of the teacher parameter theta ?,request
HJNeoqYNG_39,"I think it is important to see how fast the teacher model converges , too .",request
HJNeoqYNG_40,"6 . In some of your experiments , every training method converges to the same accuracy after enough training ( Fig. 2b ) , while in others , not quite ( Fig. 2a and 2c ) .",fact
HJNeoqYNG_41,Why is this the case ?,non-arg
HJNeoqYNG_42,Does it mean that you have not run enough iterations for the baseline methods ?,non-arg
HJNeoqYNG_43,"My intuition is that if the learner algorithm is convex , then ultimately they will all get to the same accuracy level ,",evaluation
HJNeoqYNG_44,so the task is just to get there quicker .,evaluation
HJNeoqYNG_45,"I understand that since the learner algorithm is an NN , this is not the case –",evaluation
HJNeoqYNG_46,but more explanation is necessary here –,request
HJNeoqYNG_47,does your method also reduces the empirical possibility to get stuck in local minima ?,non-arg
HJNeoqYNG_48,7 . More explanation is needed towards Fig. 4c .,request
HJNeoqYNG_49,"In this case , using a teacher model trained on a harder task ( CIFAR10 ) leads to much improved student training on a simpler task ( MNIST ) . Why ?",non-arg
HJNeoqYNG_50,"8 . Although in terms of "" effective training data points "" the proposed method outperforms the other methods ,",fact
HJNeoqYNG_51,"in terms of time ( Fig. 5 ) the difference between it and say , NoTeach , is not that significant ( especially at very high desired accuracy ) .",evaluation
HJNeoqYNG_52,More explanation needed here .,request
HJNeoqYNG_53,Read the rebuttal and revision and slightly increased my rating .,non-arg
HygXOMDxf_0,The authors propose an approach to dynamically generating filters in a CNN based on the input image .,fact
HygXOMDxf_1,"The filters are generated as linear combinations of a basis set of filters , based on features extracted by an auto-encoder .",fact
HygXOMDxf_2,"The authors test the approach on recognition tasks on three datasets : MNIST , MTFL ( facial landmarks ) and CIFAR10 , and show a small improvement over baselines without dynamic filters .",fact
HygXOMDxf_3,Pros : 1 ) I have not seen this exact approach proposed before .,evaluation
HygXOMDxf_4,2 ) There method is evaluated on three datasets and two tasks : classification and facial landmark detection .,fact
HygXOMDxf_5,"Cons : 1 ) The authors are not the first to propose dynamically generating filters ,",fact
HygXOMDxf_6,and they clearly mention that the work of De Brabandere et al. is closely related .,fact
HygXOMDxf_7,"Yet , there is no comparison to other methods for dynamic weight generation .",fact
HygXOMDxf_8,"2 ) Related to that , there is no ablation study ,",fact
HygXOMDxf_9,so it is unclear if the authors ’ contributions are useful .,evaluation
HygXOMDxf_10,"I appreciate the analysis in Tables 1 and 2 ,",evaluation
HygXOMDxf_11,but this is not sufficient .,evaluation
HygXOMDxf_12,Why the need for the autoencoder - why ca n’t the whole network be trained end-to-end on the goal task ?,request
HygXOMDxf_13,"Why generate filters as linear combination - is this just for computational reasons , or also accuracy ?",request
HygXOMDxf_14,This should be analyzed empirically .,request
HygXOMDxf_15,3 ) The experiments are somewhat substandard :,evaluation
HygXOMDxf_16,"- On MNIST the authors use a tiny poorly-performance network ,",evaluation
HygXOMDxf_17,and it is no surprise that one can beat it with a bigger dynamic filter network .,evaluation
HygXOMDxf_18,- The MTFL experiments look most convincing,evaluation
HygXOMDxf_19,"( although this might be because I am not familiar with SoTA on the dataset ) ,",evaluation
HygXOMDxf_20,"but still there is no control for the number of parameters ,",fact
HygXOMDxf_21,and the performance improvements are not huge,evaluation
HygXOMDxf_22,"- On CIFAR10 - there is a marginal improvement in performance ,",evaluation
HygXOMDxf_23,"which , as the authors admit , can also be reached by using a deeper model .",fact
HygXOMDxf_24,The baseline models are far from SoTA,evaluation
HygXOMDxf_25,"- the authors should look at more modern architecture such as AllCNN ( not particularly new or good , but very simple ) , ResNet , wide ResNet , DenseNet , etc .",request
HygXOMDxf_26,"As a comment , I do n’t think classification is a good task for showcasing such an architecture",evaluation
HygXOMDxf_27,- classification is already working extremely well .,evaluation
HygXOMDxf_28,"Many other tasks - for instance , detection , tracking , few-shot learning - seem much more promising .",evaluation
HygXOMDxf_29,"To conclude , the authors propose a new approach to learning convolutional networks with dynamic input-conditioned filters .",fact
HygXOMDxf_30,"Unfortunately , the authors fail to demonstrate the value of the proposed method .",evaluation
HygXOMDxf_31,I therefore recommend rejection .,evaluation
BJ0qmr9xf_0,"The paper solves the problem of how to do autonomous resets ,",fact
BJ0qmr9xf_1,which is an important problem in real world RL .,evaluation
BJ0qmr9xf_2,"The method is novel ,",evaluation
BJ0qmr9xf_3,"the explanation is clear ,",evaluation
BJ0qmr9xf_4,and has good experimental results .,evaluation
BJ0qmr9xf_5,"Pros : 1 . The approach is simple , solves a task of practical importance , and performs well in the experiments .",evaluation
BJ0qmr9xf_6,"2 . The experimental section performs good ablation studies wrt fewer reset thresholds , reset attempts , use of ensembles .",fact
BJ0qmr9xf_7,"Cons : 1 . The method is evaluated only for 3 tasks , which are all in simulation , and on no real world tasks .",fact
BJ0qmr9xf_8,"Additional tasks could be useful , especially for qualitative analysis of the learned reset policies .",request
BJ0qmr9xf_9,"2 . It seems that while the method does reduce hard resets ,",fact
BJ0qmr9xf_10,it would be more convincing if it can solve tasks which a model without a reset policy couldnt .,evaluation
BJ0qmr9xf_11,"Right now , the methods without the reset policy perform about equally well on final reward .",fact
BJ0qmr9xf_12,3 . The method wont be applicable to RL environments where we will need to take multiple non-invertible actions to achieve the goal ( an analogy would be multiple levels in a game ) .,fact
BJ0qmr9xf_13,"In such situations , one might want to use the reset policy to go back to intermediate “ start ” states from where we can continue again , rather than the original start state always .",evaluation
BJ0qmr9xf_14,"Conclusion/Significance : The approach is a step in the right direction ,",evaluation
BJ0qmr9xf_15,and further refinements can make it a significant contribution to robotics work .,evaluation
HJSqWxjez_0,The authors consider a Neural Network where the neurons are treated as rational agents .,fact
HJSqWxjez_1,"In this model , the neurons must pay to observe the activation of neurons upstream .",fact
HJSqWxjez_2,"Thus , each individual neuron seeks to maximize the sum of payments it receives from other neurons minus the cost for observing the activations of other neurons ( plus an external reward for success at the task ) .",fact
HJSqWxjez_3,"While this is an interesting idea on its surface ,",evaluation
HJSqWxjez_4,"the paper suffers from many problems in clarity , motivation , and technical presentation .",evaluation
HJSqWxjez_5,It would require very major editing to be fit for publication .,evaluation
HJSqWxjez_6,The major problem with this paper is its clarity .,evaluation
HJSqWxjez_7,See detailed comments below for problems just in the introduction .,non-arg
HJSqWxjez_8,"More generally , the paper is riddled with non sequiturs .",evaluation
HJSqWxjez_9,The related work section mentions Generative Adversarial Nets .,fact
HJSqWxjez_10,"As far as I can tell , this paper has nothing to do with GANs .",evaluation
HJSqWxjez_11,"The Background section introduces notation for POMDPs , never to be used again in the entirety of the paper , before launching into a paragraph about apoptosis in glial cells .",fact
HJSqWxjez_12,There is also a general lack of attention to detail .,evaluation
HJSqWxjez_13,"For example , the entire network receives an external reward ( R_t ^ { ex } ) , presumably for its performance on some task .",fact
HJSqWxjez_14,This reward is dispersed to the the individual agents who receive individual external rewards ( R _ { it } ^ { ex } ) .,fact
HJSqWxjez_15,It is never explained how this reward is allocated even in the authors ’ own experiments .,fact
HJSqWxjez_16,The authors state that all units playing NOOP is an equilibrium .,fact
HJSqWxjez_17,"While this is certainly believable/expected ,",evaluation
HJSqWxjez_18,"such a result would depend on the external rewards R _ { it } ^ { ex } , the observation costs \ sigma _ { jit } , and the network topology .",fact
HJSqWxjez_19,None of this is discussed .,fact
HJSqWxjez_20,The authors discuss Pareto optimality without ever formally describing what multi-objective function defines this supposed Pareto boundary .,fact
HJSqWxjez_21,"This is pervasive throughout the paper ,",evaluation
HJSqWxjez_22,and is detrimental to the reader ’s understanding .,evaluation
HJSqWxjez_23,"While this might be lost because of the clarity problems described above ,",evaluation
HJSqWxjez_24,the model itself is also never really motivated .,fact
HJSqWxjez_25,Why is this an interesting problem ?,evaluation
HJSqWxjez_26,There are many ways to create rational incentives for neurons in a neural net .,fact
HJSqWxjez_27,Why is paying to observe activations the one chosen here ?,evaluation
HJSqWxjez_28,"The neuroscientific motivation is not very convincing to me , considering that ultimately these neurons have to hold an auction .",fact
HJSqWxjez_29,Is there an economic motivation ?,request
HJSqWxjez_30,Is it just a different way to train a NN ?,request
HJSqWxjez_31,Detailed Comments : “ In the of NaaA ” = > remove “ of ” ?,request
HJSqWxjez_32,“ passing its activation to the unit as cost ”,quote
HJSqWxjez_33,= > Unclear .,evaluation
HJSqWxjez_34,What does this mean ?,request
HJSqWxjez_35,“ performance decreases if we naively consider units as agents ”,quote
HJSqWxjez_36,= > Performance on what ?,request
HJSqWxjez_37,“ . . we demonstrate that the agent obeys to maximize its counterfactual return as the Nash Equilibrium “,quote
HJSqWxjez_38,"= > Perhaps , this should be rewritten as “ Agents maximize their counterfactual return in equilibrium .",request
HJSqWxjez_39,"“ Subsequently , we present that learning counterfactual return leads the model to learning optimal topology ”",quote
HJSqWxjez_40,= > Do you mean “ maximizing ” instead of learning .,request
HJSqWxjez_41,Optimal with respect to what task ?,request
HJSqWxjez_42,“ pure-randomly ” = > “ randomly ”,request
HJSqWxjez_43,“ with adaptive algorithm ” = > “ with an adaptive algorithm ”,request
HJSqWxjez_44,“ the connection ” = > “ connections ”,request
HJSqWxjez_45,"“ In game theory , the outcome maximizing overall reward is named Pareto optimality . ”",quote
HJSqWxjez_46,= > This is simply incorrect .,fact
rkSREOYgM_0,This paper proposes a device placement algorithm to place operations of tensorflow on devices .,fact
rkSREOYgM_1,Pros : 1 . It is a novel approach which trains the placement end to end .,evaluation
rkSREOYgM_2,2 . The experiments are solid to demonstrate this method works very well .,evaluation
rkSREOYgM_3,3 . The writing is easy to follow .,evaluation
rkSREOYgM_4,4 . This would be a very useful tool for the community if open sourced .,evaluation
rkSREOYgM_5,"Cons : 1 . It is not very clear in the paper whether the training happens for each model yielding separate agents , or a shared agent is trained and used for all kinds of models .",evaluation
rkSREOYgM_6,The latter would be more exciting .,evaluation
rkSREOYgM_7,"The adjacency matrix varies size for different graphs ,",fact
rkSREOYgM_8,so I guess a separate agent is trained for each graph ?,non-arg
rkSREOYgM_9,"However , if the agent is not shared , why not just use integer to represent each operation in the graph ,",request
rkSREOYgM_10,since overfitting would be more desirable in this case .,fact
rkSREOYgM_11,2 . Averaging the embedding is hard to understand especially for the output sizes and number of outputs .,evaluation
rkSREOYgM_12,3 . It is not clear how the adjacency information is used .,evaluation
Bk9Z3ZQlG_0,"The paper proposes LSD-NET , an active vision method for object classification .",fact
Bk9Z3ZQlG_1,"In the proposed method , based on a given view of an object , the algorithm can decide to either classify the object or to take a discrete action step which will move the camera in order to acquire a different view of the object .",fact
Bk9Z3ZQlG_2,Following this procedure the algorithm iteratively moves around the object until reaching a maximum number of allowed moves or until a object view favorable for classification is reached .,fact
Bk9Z3ZQlG_3,The main contribution of the paper is a hierarchical action space that distinguishes between camera-movement actions and classification actions .,fact
Bk9Z3ZQlG_4,"At the top-level of the hierarchy , the algorithm decides whether to perform a movement or a classification - type action .",fact
Bk9Z3ZQlG_5,"At the lower-level , the algorithm either assign a specific class label ( for the case of classification actions ) or performs a camera movement ( for the case of camera-movement actions ) .",fact
Bk9Z3ZQlG_6,This hierarchical action space results in reduced bias towards classification actions .,fact
Bk9Z3ZQlG_7,Strong Points - The content is clear and easy to follow .,evaluation
Bk9Z3ZQlG_8,- The proposed method achieves competitive performance w.r.t. existing work .,evaluation
Bk9Z3ZQlG_9,Weak Points - Some aspects of the proposed method could have been evaluated better .,evaluation
Bk9Z3ZQlG_10,- A deeper evaluation/analysis of the proposed method is missing .,evaluation
Bk9Z3ZQlG_11,Overall the proposed method is sound and the paper has a good flow and is easy to follow .,evaluation
Bk9Z3ZQlG_12,"The proposed method achieves competitive results ,",evaluation
Bk9Z3ZQlG_13,"and up to some extent , shows why it is important to have the proposed hierarchical action space .",evaluation
Bk9Z3ZQlG_14,My main concerns with this manuscript are the following :,non-arg
Bk9Z3ZQlG_15,In some of the tables a LSTM variant ? of the proposed method is mentioned .,fact
Bk9Z3ZQlG_16,However it is never introduced properly in the text .,evaluation
Bk9Z3ZQlG_17,Can you indicate how this LSTM-based method differs from the proposed method ?,request
Bk9Z3ZQlG_18,"At the end of Section 5.2 the manuscript states : "" In comparison to other methods , our method is agnostic of the starting point i.e. it can start randomly on any image and it would get similar testing accuracies . """,quote
Bk9Z3ZQlG_19,This suggests that the method has been evaluated over different trials considering different random initializations .,evaluation
Bk9Z3ZQlG_20,"However , this is unclear based on the evaluation protocol presented in Section 5 .",evaluation
Bk9Z3ZQlG_21,"If this is not the case , perhaps this is an experiment that should be conducted .",request
Bk9Z3ZQlG_22,"In Section 3.2 it is mentioned that different from typical deep reinforcement learning methods , the proposed method uses a deeper AlexNet-like network .",fact
Bk9Z3ZQlG_23,"In this context , it would be useful to drop a comment on the computation costs added in training/testing by this deeper model .",request
Bk9Z3ZQlG_24,Table 3 shows the number of correctly and wrongly classified objects as a function of the number of steps taken .,fact
Bk9Z3ZQlG_25,"Here we can notice that around 50 % of the objects are in the step 1 and 12 ,",fact
Bk9Z3ZQlG_26,"which as correctly indicated by the manuscript , suggests that movement does not help for those cases .",evaluation
Bk9Z3ZQlG_27,Would it be possible to have more class-specific ( or classes grouped into intermediate categories ) visualization of the results ?,request
Bk9Z3ZQlG_28,This would provide a better insight of what is going on and when exactly actions related to camera movements really help to get better classification performance .,evaluation
Bk9Z3ZQlG_29,"On the presentation side , I would recommend displaying the content of Table 3 in a plot .",request
Bk9Z3ZQlG_30,This may display the trends more clearly .,evaluation
Bk9Z3ZQlG_31,"Moreover , I would recommend to visualize the classification accuracy as a function of the step taken by the method .",request
Bk9Z3ZQlG_32,"In this regard , a deeper analysis of the effect of the proposed hierarchical action space is a must .",request
Bk9Z3ZQlG_33,I would encourage the authors to address the concerns raised on my review .,request
ry-q9ZOlf_0,"This paper proposes a simple modification to the standard alternating stochastic gradient method for GAN training , which stabilizes training , by adding a prediction step .",fact
ry-q9ZOlf_1,"This is a clever and useful idea ,",evaluation
ry-q9ZOlf_2,and the paper is very well written .,evaluation
ry-q9ZOlf_3,"The proposed method is very clearly motivated , both intuitively and mathematically ,",evaluation
ry-q9ZOlf_4,and the authors also provide theoretical guarantees on its convergence behavior .,fact
ry-q9ZOlf_5,I particularly liked the analogy with the damped harmonic oscillator .,evaluation
ry-q9ZOlf_6,The experiments are well designed and provide clear evidence in favor of the usefulness of the proposed technique .,evaluation
ry-q9ZOlf_7,I believe that the method proposed in this paper will have a significant impact in the area of GAN training .,evaluation
ry-q9ZOlf_8,"I have only one minor question : in the prediction step , why not use a step size , say <EQN> , such that the "" amount of predition "" may be adjusted ?",non-arg
BkYfM_Rgz_0,It is clear that the problem studied in this paper is interesting .,evaluation
BkYfM_Rgz_1,"However , after reading through the manuscript , it is not clear to me what are the real contributions made in this paper .",evaluation
BkYfM_Rgz_2,I also failed to find any rigorous results on generalization bounds .,evaluation
BkYfM_Rgz_3,"In this case , I can not recommend the acceptance of this paper .",evaluation
H1W1OsYxG_0,"The paper introduces a method for learning graph representations ( i.e. , vector representations for graphs ) .",fact
H1W1OsYxG_1,An existing node embedding method is used to learn vector representations for the nodes .,fact
H1W1OsYxG_2,The node embeddings are then projected into a 2-dimensional space by PCA .,fact
H1W1OsYxG_3,The 2-dimensional space is binned using an imposed grid structure .,fact
H1W1OsYxG_4,The value for a bin is the ( normalized ) number of nodes falling into the corresponding region .,fact
H1W1OsYxG_5,The idea is simple and easily explained in a few minutes .,evaluation
H1W1OsYxG_6,That is an advantage .,evaluation
H1W1OsYxG_7,"Also , the experimental results look quite promising .",evaluation
H1W1OsYxG_8,It seems that the methods outperforms existing methods for learning graph representations .,evaluation
H1W1OsYxG_9,The problem with the approach is that it is very ad-hoc .,evaluation
H1W1OsYxG_10,There are several ( existing ) ideas of how to combine node representations into a representation for the entire graph .,fact
H1W1OsYxG_11,"For instance , averaging the node embeddings is something that has shown promising results in previous work .",evaluation
H1W1OsYxG_12,"Since the methods is so ad-hoc ( node2vec - > PCA - > discretized density map - > CNN architecure ) and since a theoretical understanding of why the approach works is missing , it is especially important to compare your method more thoroughly to simpler methods .",request
H1W1OsYxG_13,"Again , pooling operations ( average , max , etc. ) on the learned node2vec embeddings are examples of simpler alternatives .",evaluation
H1W1OsYxG_14,The experimental results are also not explained thoroughly enough .,evaluation
H1W1OsYxG_15,"For instance , since two runs of node2vec will give you highly varying embeddings ( depending on the initialization ) , you will have to run node2vec several times to reduce the variance of your resulting discretized density maps .",evaluation
H1W1OsYxG_16,How many times did you run node2vec on each graph ?,non-arg
BJQGTw5lM_0,This manuscript explores the idea of adding noise to the adversary 's play in GAN dynamics over an RKHS .,fact
BJQGTw5lM_1,"This is equivalent to adding noise to the gradient update , using the duality of reproducing kernels .",fact
BJQGTw5lM_2,"Unfortunately , the evaluation here is wholly unsatisfactory to justify the manuscript 's claims .",evaluation
BJQGTw5lM_3,"No concrete practical algorithm specification is given ( only a couple of ideas to inject noise listed ) ,",fact
BJQGTw5lM_4,"only a qualitative one on a 2-dimensional latent space in MNIST , and an inconclusive one using the much-doubted Parzen window KDE method .",fact
BJQGTw5lM_5,"The idea as stated in the abstract and introduction may well be worth pursuing ,",evaluation
BJQGTw5lM_6,but not on the evidence provided by the rest of the manuscript .,evaluation
HJv0cb5xG_0,This work addresses an important and outstanding problem : accurate long-term forecasting using deep recurrent networks .,fact
HJv0cb5xG_1,"The technical approach seems well motivated , plausible , and potentially a good contribution ,",evaluation
HJv0cb5xG_2,but the experimental work has numerous weaknesses which limit the significance of the work in current form .,evaluation
HJv0cb5xG_3,"For one , the 3 datasets tested are not established as among the most suitable , well-recognized benchmarks for evaluating long-term forecasting .",fact
HJv0cb5xG_4,"It would be far more convincing if the author ’s used well-established benchmark data , for which existing best methods have already been well-tuned to get their best results .",request
HJv0cb5xG_5,"Otherwise , the reader is left with concerns that the author ’s may not have used the best settings for the baseline method results reported , which indeed is a concern here ( see below ) .",evaluation
HJv0cb5xG_6,"One weakness with the experiments is that it is not clear that they were fair to RNN or LSTM ,",evaluation
HJv0cb5xG_7,"for example , in terms of giving them the same computation as the TT-RNNs .",fact
HJv0cb5xG_8,Section Hyper-parameter Analysis ” on page 7 explains that they determined best TT rank and lags via grid search .,fact
HJv0cb5xG_9,"But presumably larger values for rank and lag require more computation ,",evaluation
HJv0cb5xG_10,"so to be fair to RNN and LSTM they should be given more computation as well , for example allowing them more hidden units than TT-RNNs get , so that overall computation cost is the same for all 3 methods .",request
HJv0cb5xG_11,"As far as this reviewer can tell , the authors offer no experiments to show that a larger number of units for RNN or LSTM would not have helped them in improving long-term forecasting accuracies ,",fact
HJv0cb5xG_12,so this seems like a very serious and plausible concern .,evaluation
HJv0cb5xG_13,"Also , on page 6 the authors say that they tried ARMA but that it performed about 5 % worse than LSTM , and thus dismissing direct comparisons of ARMA against TT-RNN .",fact
HJv0cb5xG_14,But they are unclear whether they gave ARMA as much hyper-parameter tuning ( e.g. for number of lags ) via grid search as their proposed TT-RNN benefited from .,evaluation
HJv0cb5xG_15,"Again , the concern here is that the experiments are plausibly not being fair to all methods equally .",evaluation
HJv0cb5xG_16,"So , due to the weaknesses in the experimental work ,",evaluation
HJv0cb5xG_17,this work seems a bit premature .,evaluation
HJv0cb5xG_18,It should more clearly establish that their proposed TT-RNN are indeed performing well compared to existing SOTA .,request
Bk-6h6Txz_0,"The article "" Contextual Explanation Networks "" introduces the class of models which learn the intermediate explanations in order to make final predictions .",fact
Bk-6h6Txz_1,"The contexts can be learned by , in principle , any model including neural networks ,",fact
Bk-6h6Txz_2,while the final predictions are supposed to be made by some simple models like linear ones .,fact
Bk-6h6Txz_3,The probabilistic model allows for the simultaneous training of explanation and prediction parts as opposed to some recent post-hoc methods .,fact
Bk-6h6Txz_4,"The experimental part of the paper considers variety of experiments , including classification on MNIST , CIFAR-10 , IMDB and also some experiments on survival analysis .",fact
Bk-6h6Txz_5,"I should note , that the quality of the algorithm is in general similar to other methods considered ( as expected ) .",fact
Bk-6h6Txz_6,"However , while in some cases the CEN algorithm is slightly better , in other cases it appears to sufficiently loose , see for example left part of Figure 3 ( b ) for MNIST data set .",fact
Bk-6h6Txz_7,It would be interesting to know the explanation .,request
Bk-6h6Txz_8,"Also , it would be interesting to have more examples of qualitative analysis to see , that the learned explanations are really useful .",request
Bk-6h6Txz_9,"I am a bit worried , that while we have interpretability with respect to intermediate features , these features theirselves might be very hard to interpret .",evaluation
Bk-6h6Txz_10,"To sum up , I think that the general idea looks very natural and the results are quite supportive .",evaluation
Bk-6h6Txz_11,"However , I do n't feel myself confident enough in this area of research to make strong conclusion on the quality of the paper .",non-arg
Hy9zmitlG_0,This paper presents a novel method for spike based learning that aims at reducing the needed computation during learning and testing when classifying temporal redundant data .,fact
Hy9zmitlG_1,This approach extends the method presented on Arxiv on Sigma delta quantized networks,fact
Hy9zmitlG_2,"( Peter O’Connor and Max Welling . Sigma delta quantized networks . arXiv preprint arXiv : 1611.02024 , 2016b . ) .",reference
Hy9zmitlG_3,"Overall , the paper is interesting and promising ;",evaluation
Hy9zmitlG_4,only a few works tackle the problem of learning with spikes showing the potential advantages of such form of computing .,fact
Hy9zmitlG_5,"The paper , however , is not flawless .",evaluation
Hy9zmitlG_6,"The authors demonstrate the method on just two datasets , and effectively they show results of training only for Feed-Forward Neural Nets",fact
Hy9zmitlG_7,"( the authors claim that “ the entire spiking network end-to-end works ” referring to their pre-trained VGG19 ,",fact
Hy9zmitlG_8,but this paper presents only training for the three top layers ) .,fact
Hy9zmitlG_9,"Furthermore , even if suitable datasets are not available , the authors could have chosen to train different architectures .",request
Hy9zmitlG_10,The first dataset is the well-known benchmark MNIST also presented in a customized Temporal-MNIST .,fact
Hy9zmitlG_11,"Although it is a common base-line , some choices are not clear :",evaluation
Hy9zmitlG_12,why using a FFNN instead that a CNN which performs better on this dataset ;,request
Hy9zmitlG_13,how data is presented in terms of temporal series –,request
Hy9zmitlG_14,this applies to the Temporal MNIST too ;,request
Hy9zmitlG_15,why performances for Temporal MNIST – which should be a more suitable dataset — are worse than for the standard MNIST ;,request
Hy9zmitlG_16,what is the meaning of the right column of Figure 5,request
Hy9zmitlG_17,since it ’s just a linear combination of the GOps results .,fact
Hy9zmitlG_18,"For the second dataset , some points are not clear too :",evaluation
Hy9zmitlG_19,why the labels and the pictures seem not to match ( in appendix E ) ;,request
Hy9zmitlG_20,why there are more training iterations with spikes w.r.t. the not-spiking case .,request
Hy9zmitlG_21,"Overall , the paper is mathematically sound ,",evaluation
Hy9zmitlG_22,except for the “ future updates ” meaning which probably deserves a clearer explanation .,request
Hy9zmitlG_23,"Moreover , I do n’t see why the learning rule equations ( 14-15 ) are described in the appendix ,",evaluation
Hy9zmitlG_24,while they are referred constantly in the main text .,fact
Hy9zmitlG_25,The final impression is that the problem of the dynamical range of the hidden layer activations is not fully resolved by the empirical solution described in Appendix D :,evaluation
Hy9zmitlG_26,perhaps this problem affects CCNs more than FFN .,evaluation
Hy9zmitlG_27,"Finally , there are some minor issues here and there",evaluation
Hy9zmitlG_28,( the authors show quite some lack of attention for just 7 pages ) :,evaluation
Hy9zmitlG_29,- Two times “ get ” in “ we get get a decoding scheme ” in the introduction ;,fact
Hy9zmitlG_30,- Two times “ update ” in “ our true update update as ” in Sec. 2.6 ;,fact
Hy9zmitlG_31,- Pag3 correct the capital S in 2.3.1,request
Hy9zmitlG_32,- Pag4 Figure 1 increase font size ( also for Figure2 ) ;,request
Hy9zmitlG_33,close bracket after Equation 3 ;,request
Hy9zmitlG_34,N ( number of spikes ) is not defined,fact
Hy9zmitlG_35,- Pag5 “ one-hot ” or “ onehot ” ;,request
Hy9zmitlG_36,"- in the inline equation the sum goes from n = 1 to S , while in eq . ( 8 ) it goes from n = 1 to N ;",fact
Hy9zmitlG_37,- Eq ( 10 ) ( 11 ) ( 12 ) and some lines have a typo ( a \ cdot ) just before some of the ws ;,evaluation
Hy9zmitlG_38,- Pag6 k _ { beta } is not defined in the main text ;,fact
Hy9zmitlG_39,- Pag7 there are two “ so that ” in 3.1 ;,fact
Hy9zmitlG_40,capital letter “ It used 32x10 ^ 12 . . ” ;,fact
Hy9zmitlG_41,"beside , here , why do not report the difference in computation w.r.t. not-spiking nets ?",request
Hy9zmitlG_42,- Pag7 in 3.2 “ discussed in 1 ” is section 1 ?,request
Hy9zmitlG_43,"- Pag14 Appendix E , why the labels do n’t match the pictures ;",request
Hy9zmitlG_44,"- Pag14 Appendix F , explain better the architecture used for this experiment .",request
SJbxHpnHz_0,Summary ------- This paper proposes a generative model of symbolic ( MIDI ) melody in western popular music .,fact
SJbxHpnHz_1,"The model uses an LSTM architecture to map sequences of chord symbols and structural identifiers ( e.g. , verse or chorus ) to predicted note sequences which constitute the melody line .",fact
SJbxHpnHz_2,"The key innovation proposed in this work is to jointly encode note symbols along with timing and duration information to form musical "" words "" from which melodies are composed .",evaluation
SJbxHpnHz_3,"The proposed model compares compared favorably to prior work in listener preference and Turing-test studies , and performs .",fact
SJbxHpnHz_4,"Quality ------- Overall , I found the paper interesting , and the provided examples generated by the model sound relatively good .",evaluation
SJbxHpnHz_5,"The quantitative evaluations seem promising ,",evaluation
SJbxHpnHz_6,though difficult to interpret fully due to a lack of provided detail ( see below ) .,evaluation
SJbxHpnHz_7,"Apart from clarification issues enumerated below , where the paper could be most substantially improved is in the evaluation of the various ingredients of the model .",evaluation
SJbxHpnHz_8,"Many ideas are presented with some abstract motivation ,",fact
SJbxHpnHz_9,but there is no comparative evaluation to demonstrate what happens when any one piece is removed from the system .,fact
SJbxHpnHz_10,"Some examples : - How important is the "" song part "" contextual input ?",request
SJbxHpnHz_11,- What happens if the duration or timing information is not encoded with the note ?,request
SJbxHpnHz_12,- How important is the pitch range regularization ?,request
SJbxHpnHz_13,"Since the authors claim these ideas as novel ,",fact
SJbxHpnHz_14,I would expect to see more evaluation of their independent impact on the resulting system .,request
SJbxHpnHz_15,"Without such an evaluation , it is difficult to take any general lessons away from this paper .",evaluation
SJbxHpnHz_16,"Clarity ------- While the main ideas of this paper are presented clearly ,",evaluation
SJbxHpnHz_17,I found the details difficult to follow .,evaluation
SJbxHpnHz_18,"Specifically , the following points need to be substantially clarified : - The note encoding described in Section 3.1 : "" w_i = ( p_i , t_i , l_i ) "" describes the pitch , timing , and duration of the i ' th chord , but it is not explained how time and duration are represented .",request
SJbxHpnHz_19,"Since these are derived from MIDI , I would expect either ticks or seconds -- or maybe a tempo-normalized variant --",request
SJbxHpnHz_20,"but Figure 2 suggests staff notation , which is not explicitly coded in MIDI .",fact
SJbxHpnHz_21,Please explain precisely how the data is represented .,request
SJbxHpnHz_22,"- Also in Section 3 , several references are made to a "" previous "" model , but no citation is given .",request
SJbxHpnHz_23,Was this a specific published work ?,non-arg
SJbxHpnHz_24,- Equation 1 is missing a variable ( j ) for the range of the summation .,fact
SJbxHpnHz_25,It took a few passes for me to parse what was going on here .,non-arg
SJbxHpnHz_26,"One could easily mistake it for summing over i to describe partial subsequences ,",evaluation
SJbxHpnHz_27,but I do n't think this is what is intended .,evaluation
SJbxHpnHz_28,"- "" ... our model does not have to consider intervals that do not contain notes "" --",quote
SJbxHpnHz_29,"this contradicts the implication of Figure 2b , where a rest is explicitly notated in the generated sequence .",fact
SJbxHpnHz_30,Since MIDI does not explicitly encode rests,fact
SJbxHpnHz_31,"( they must be inferred from the absence of note events ) ,",fact
SJbxHpnHz_32,"I 'd suggest wording this more carefully , and being more explicit about what is produced by the model and what is notational embellishment for expository purposes .",request
SJbxHpnHz_33,"- Equation 2 describes the LSTM gate equations ,",fact
SJbxHpnHz_34,but there is no concrete description of the model architecture used in this paper .,fact
SJbxHpnHz_35,How are the hidden states mapped to note predictions ?,request
SJbxHpnHz_36,What is the loss function and optimizer ?,request
SJbxHpnHz_37,These details are necessary to facilitate replication .,evaluation
SJbxHpnHz_38,- Equation 3 and the accompanying text implies that song part states ( x_i ) are conditionally independent given the current chord state ( z_i ) .,fact
SJbxHpnHz_39,Is that correct ?,evaluation
SJbxHpnHz_40,"If so , it seems like a strange choice ,",evaluation
SJbxHpnHz_41,since I would expect a part state to persist across multiple chord state transitions .,evaluation
SJbxHpnHz_42,Please explain this part in more detail .,request
SJbxHpnHz_43,Also a typo in the second factor : p ( z_N | z _ { N-1 } ) should be p ( z_n | z _ { n-1 } ) ; likewise p ( x_n | z_N ) .,request
SJbxHpnHz_44,- The regularization penalty ( Alg . 1 ) is also difficult to follow .,evaluation
SJbxHpnHz_45,"Is S derived from P by viterbi decoding , or independent ( point-wise argmax ) decoding ?",request
SJbxHpnHz_46,"What exactly is the "" E "" that results in the derivative at step 8 , and why does the derivative for p_i depend on the total sum C ?",request
SJbxHpnHz_47,"This all seems non-obvious , and worth describing in more detail since it seems critical to the performance of the model .",evaluation
SJbxHpnHz_48,"- Table 2 : what does "" # samples "" mean in this context ?",request
SJbxHpnHz_49,"Why is it different from "" # songs "" ?",request
SJbxHpnHz_50,- Section 4.2 : the description of the evaluation suggests that the proposed model 's output was always played before the baseline .,fact
SJbxHpnHz_51,Is that correct ?,request
SJbxHpnHz_52,"If so , does that bias the results ?",request
SJbxHpnHz_53,"- Section 4.2 : are the examples provided to listeners just the melody lines , or full mixes on top of the input chord sequence ?",request
SJbxHpnHz_54,"It 's unclear from the text ,",evaluation
SJbxHpnHz_55,and it seems like a relevant detail to correctly assess the fairness of the comparison to the baselines .,fact
SJbxHpnHz_56,- Section 4.2 : how many generated examples were included in this evaluation ?,request
SJbxHpnHz_57,"Should this instead be in or out of key , since the tuning is presumably fixed by MIDI synthesis ?",request
SJbxHpnHz_58,"Originality ----------- As far as I know , the proposed method is novel , though strongly related to ( cited ) prior work .",evaluation
SJbxHpnHz_59,"The key idea seems to be encoding of notes and properties as analogous to "" words "" .",fact
SJbxHpnHz_60,"I find this analogy a little bit of a stretch ,",evaluation
SJbxHpnHz_61,"since even with timing and duration included , it 's hard to argue that a single note event has semantic content in the way that a word does .",evaluation
SJbxHpnHz_62,"A little more development of this idea , and some more concrete motivation for the specific choices of which properties to include , would go a long way in strengthening the paper .",evaluation
SJbxHpnHz_63,Significance ------------ The significance of this work is difficult to assess without independent evaluation of the proposed novel components .,evaluation
B1Fe0Zqxz_0,The paper presents a way to regularize a sequence generator by making the hidden states also predict the hidden states of an RNN working backward .,fact
B1Fe0Zqxz_1,"Applied to sequence-to-sequence networks , the approach requires training one encoder , and two separate decoders , that generate the target sequence in forward and reversed orders .",fact
B1Fe0Zqxz_2,A penalty term is added that forces an agreement between the hidden states of the two decoders .,fact
B1Fe0Zqxz_3,"During model evaluation only the forward decoder is used , with the backward operating decoder discarded .",fact
B1Fe0Zqxz_4,"The method can be interpreted to generalize other recurrent network regularizers , such as putting an L2 loss on the hidden states .",fact
B1Fe0Zqxz_5,"Experiments indicate that the approach is most successful when the regularized RNNs are conditional generators , which emit sequences of low entropy , such as decoders of a seq2seq speech recognition network .",fact
B1Fe0Zqxz_6,"Negative results were reported when the proposed regularization technique was applied to language models , whose output distribution has more entropy .",fact
B1Fe0Zqxz_7,"The proposed regularization is evaluated with positive results on a speech recognition task and on an image captioning task , and with negative results ( no improvement , but also no deterioration ) on a language modeling and sequential MNIST digit generation tasks .",fact
B1Fe0Zqxz_8,I have one question about baselines : is the proposed approach better than training to forward generators and force an agreement between them ( in the spirit of the concurrent ICLR submission <URL> - ) ?,non-arg
B1Fe0Zqxz_9,"Also , would using the backward RNN , e.g. for rescoring , bring another advantage ?",non-arg
B1Fe0Zqxz_10,"In other words , what is ( and is there ) a gap between an ensemble of a forward and backward rnn and the forward-rnn only , but trained with the state-matching penalty ?",non-arg
B1Fe0Zqxz_11,Quality : The proposed approach is well motivated,evaluation
B1Fe0Zqxz_12,and the experiments show the limits of applicability range of the technique .,fact
B1Fe0Zqxz_13,Clarity : The paper is clearly written .,evaluation
B1Fe0Zqxz_14,Originality : The presented idea seems novel .,evaluation
B1Fe0Zqxz_15,"Significance : The method may prove to be useful to regularize recurrent networks ,",evaluation
B1Fe0Zqxz_16,however I would like to see a comparison with ensemble methods .,request
B1Fe0Zqxz_17,"Also , as the authors note the method seems to be limited to conditional sequence generators .",fact
B1Fe0Zqxz_18,"Pros and cons : Pros : the method is simple to implement ,",evaluation
B1Fe0Zqxz_19,the paper lists for what kind of datasets it can be used .,fact
B1Fe0Zqxz_20,"Cons : the method needs to be compared with typical ensembles of models going only forward in time ,",request
B1Fe0Zqxz_21,it may turn that it using the backward RNN is not necessary,evaluation
S1VwmoFxz_0,"In this paper , the authors investigate variance reduction techniques for agents with multi-dimensional policy outputs , in particular when they are conditionally independent ( ' factored ' ) .",fact
S1VwmoFxz_1,"With the increasing focus on applying RL methods to continuous control problems and RTS type games , this is an important problem",evaluation
S1VwmoFxz_2,and this technique seems like an important addition to the RL toolbox .,evaluation
S1VwmoFxz_3,"The paper is well written , the method is easy to implement ,",evaluation
S1VwmoFxz_4,and the algorithm seems to have clear positive impact on the presented experiments .,evaluation
S1VwmoFxz_5,- The derivations in pages 4-6 are somewhat disconnected from the rest of the paper :,evaluation
S1VwmoFxz_6,"the optimal baseline derivation is very standard ( even if adapted to the slightly different situation situated here ) ,",fact
S1VwmoFxz_7,"and for reasons highlighted by the authors in this paper , they are not often used ;",fact
S1VwmoFxz_8,"the ' marginalized ' baseline is more common , and indeed , the authors adopt this one as well .",fact
S1VwmoFxz_9,In light of this ( and of the paper being quite a bit over the page limit ) -,fact
S1VwmoFxz_10,is this material ( 4.2 - > 4.4 ) mostly not better suited for the appendix ?,request
S1VwmoFxz_11,Same for section 4.6 ( which I believe is not used in the experiments ) .,evaluation
S1VwmoFxz_12,- The experimental section is very strong ;,evaluation
S1VwmoFxz_13,"regarding the partial observability experiments , assuming actions are here factored as well , I could see four baselines",evaluation
S1VwmoFxz_14,"( two choices for whether the baseline has access to the goal location or not , and two choices for whether the baseline has access to the vector <VAR> .",fact
S1VwmoFxz_15,It 's not clear which two baselines are depicted in 5b -,evaluation
S1VwmoFxz_16,is it possible to disentangle the effect of providing <VAR> and the location of the hole to the baseline ?,request
S1VwmoFxz_17,( side note : it is an interesting idea to include information not available to the agent as input to the baseline though it does feel a bit ' iffy ' ;,evaluation
S1VwmoFxz_18,"the agent requires information to train ,",fact
S1VwmoFxz_19,but is not provided the information to act .,fact
S1VwmoFxz_20,"Out of curiosity , is it intended as an experiment to verify the need for better baselines ?",non-arg
S1VwmoFxz_21,Or as a ' fair ' training procedure ? ),non-arg
S1VwmoFxz_22,- Minor : in equation 2 - is the correct exponent not t ' ?,evaluation
S1VwmoFxz_23,"Also since <VAR> is define with a scaling <VAR> ( to make it an actual distribution ) ,",fact
S1VwmoFxz_24,I believe the definition of <VAR> should also be multiplied by <VAR> ( as well as equation 2 ) .,request
BytyNwclz_0,This paper presents an analysis of the communication systems that arose when neural network based agents played simple referential games .,fact
BytyNwclz_1,"The set up is that a speaker and a listener engage in a game where both can see a set of possible referents ( either represented symbolically in terms of features , or represented as simple images ) and the speaker produces a message consisting of a sequence of numbers while the listener has to make the choice of which referent the speaker intends .",fact
BytyNwclz_2,"This is a set up that has been used in a large amount of previous work ,",evaluation
BytyNwclz_3,and the authors summarize some of this work .,fact
BytyNwclz_4,"The main novelty in this paper is the choice of models to be used by speaker and listener ,",evaluation
BytyNwclz_5,which are based on LSTMs and convolutional neural networks .,fact
BytyNwclz_6,"The results show that the agents generate effective communication systems ,",fact
BytyNwclz_7,and some analysis is given of the extent to which these communications systems develop compositional properties,fact
BytyNwclz_8,– a question that is currently being explored in the literature on language creation .,fact
BytyNwclz_9,"This is an interesting question ,",evaluation
BytyNwclz_10,and it is nice to see worker playing modern neural network models to his question and exploring the properties of the solutions of the phone .,evaluation
BytyNwclz_11,"However , there are also a number of issues with the work .",evaluation
BytyNwclz_12,1 . One of the key question is the extent to which the constructed communication systems demonstrate compositionality .,evaluation
BytyNwclz_13,The authors note that there is not a good quantitative measure of this .,fact
BytyNwclz_14,"However , this is been the topic of much research of the literature and language evolution .",evaluation
BytyNwclz_15,"This work has resulted in some measures that could be applied here ,",request
BytyNwclz_16,see for example Carr et al. ( 2016 ) : <URL>,reference
BytyNwclz_17,2 . In general the results occurred be more quantitative .,request
BytyNwclz_18,In section 3.3.2 it would be nice to see statistical tests used to evaluate the claims .,request
BytyNwclz_19,Minimally I think it is necessary to calculate a null distribution for the statistics that are reported .,request
BytyNwclz_20,3 . As noted above the main novelty of this work is the use of contemporary network models .,evaluation
BytyNwclz_21,"One of the advantages of this is that it makes it possible to work with more complex data stimuli , such as images .",evaluation
BytyNwclz_22,"However , unfortunately the image example that is used is still very artificial being based on a small set of synthetically generated images .",evaluation
BytyNwclz_23,"Overall , I see this as an interesting piece of work that may be of interest to researchers exploring questions around language creation and language evolution ,",evaluation
BytyNwclz_24,"but I think the results require more careful analysis and the novelty is relatively limited , at least in the way that the results are presented here .",evaluation
Sk6i-Szbz_0,This paper proposes to use Cross-Corpus training for biomedical relationship extraction from text .,fact
Sk6i-Szbz_1,"- Many wording issues , like citation formats , grammar mistakes , missing words , e.g. , Page 2 : it as been",fact
Sk6i-Szbz_2,- The description of the methods should be improved .,request
Sk6i-Szbz_3,"For instance , why the input has only two entities ?",request
Sk6i-Szbz_4,"In many biomedical sentences , there are more than two entities .",fact
Sk6i-Szbz_5,How can the proposed two models handle these cases ?,non-arg
Sk6i-Szbz_6,- The paper just presents to train on a larger labeled corpus and test on a task with a smaller labeled set .,fact
Sk6i-Szbz_7,Why is this novel ?,evaluation
Sk6i-Szbz_8,Nothing is novel in the deep models ( CNN and TreeLSTM ) .,evaluation
Sk6i-Szbz_9,"- Missing refs , like : <CIT>",fact
B1fZIQcxM_0,The paper is not anonymized .,evaluation
B1fZIQcxM_1,"In page 2 , the first line , the authors revealed [ 15 ] is a self-citation",fact
B1fZIQcxM_2,and [ 15 ] is not anonumized in the reference list .,fact
Bkp-xJ5xf_0,This paper presents a so-called cross-view training for semi-supervised deep models .,fact
Bkp-xJ5xf_1,Experiments were conducted on various data sets,fact
Bkp-xJ5xf_2,and experimental results were reported .,fact
Bkp-xJ5xf_3,Pros : * Studying semi-supervised learning techniques for deep models is of practical significance .,evaluation
Bkp-xJ5xf_4,Cons : * The novelty of this paper is marginal .,evaluation
Bkp-xJ5xf_5,The use of unlabeled data is in fact a self-training process .,fact
Bkp-xJ5xf_6,Leveraging the sub-regions of the image to improve performance is not new and has been widely-studied in image classification and retrieval .,fact
Bkp-xJ5xf_7,* The proposed approach suffers from a technical weakness or flaw .,evaluation
Bkp-xJ5xf_8,"For the self-labeled data , the prediction of each view is enforced to be same as the assigned self-labeling .",fact
Bkp-xJ5xf_9,"However , since each view related to a sub-region of the image ( especially when the model is not so deep ) , it is less likely for this region to contain the representation of the concepts",evaluation
Bkp-xJ5xf_10,"( e.g. , some local region of an image with a horse may exhibit only grass ) ;",evaluation
Bkp-xJ5xf_11,"enforcing the prediction of this view to be the same self-labeled concepts ( e.g , “ horse ” ) may drive the prediction away from what it should be",evaluation
Bkp-xJ5xf_12,"( e. . g , it will make the network to predict grass as horse ) .",fact
Bkp-xJ5xf_13,Such a flaw may affect the final performance of the proposed approach .,evaluation
Bkp-xJ5xf_14,* The word “ view ” in this paper is misleading .,evaluation
Bkp-xJ5xf_15,The “ view ” in this paper is corresponding to actually sub-regions in the images,fact
Bkp-xJ5xf_16,"* The experimental results indicate that the proposed approach fails to perform better than the compared baselines in table 2 , which reduces the practical significance of the proposed approach .",evaluation
BJ3VB6_xG_0,"The paper proposes an interesting alternative to recent approaches to learning from logged bandit feedback , and validates their contribution in a reasonable experimental comparison .",fact
BJ3VB6_xG_1,The clarity of writing can be improved,request
BJ3VB6_xG_2,"( several typos in the manuscript , notation used before defining , missing words , poorly formatted citations , etc. ) .",fact
BJ3VB6_xG_3,Implementing the approach using recent f-GANs is an interesting contribution and may spur follow-up work .,evaluation
BJ3VB6_xG_4,There are several lingering concerns about the approach ( detailed below ) that detract from the quality of their contributions .,evaluation
BJ3VB6_xG_5,"[ Major ] In Lemma 1 , <VAR> is used before defining it .",fact
BJ3VB6_xG_6,"Crucially , additional assumptions on <VAR> are necessary ( e.g. <EQN> for all z.",request
BJ3VB6_xG_7,"If not , a trivial counter-example is : set <VAR> >> 1 for all z and Lemma 1 is violated ) .",fact
BJ3VB6_xG_8,It is unclear how crucially this additional assumption is required in practice,evaluation
BJ3VB6_xG_9,( their expts with Hamming losses clearly do not satisfy such an assumption ) .,evaluation
BJ3VB6_xG_10,"[ Minor ] Typo : Section 3.2 , first equation ; the integral equals <VAR> .",fact
BJ3VB6_xG_11,"[ Crucial ! ] Eqn10 : Expected some justification on why it is fruitful to * lower-bound * the divergence term ,",request
BJ3VB6_xG_12,which contributes to an * upper-bound * on the true risk .,fact
BJ3VB6_xG_13,[ Crucial ! ] Algorithm1 : How is the condition of the while loop checked in a tractable manner ?,request
BJ3VB6_xG_14,"[ Minor ] Typos : Initilization -> Initialization , Varitional -> Variational",request
BJ3VB6_xG_15,"[ Major ] Expected an additional "" baseline "" in the expts -- Supervised but with the neural net policy architecture",request
BJ3VB6_xG_16,( NN approaches outperforming Supervised on LYRL dataset was baffling before realizing that Supervised is implemented using a linear CRF ) .,evaluation
BJ3VB6_xG_17,"[ Major ] Is there any guidance for picking the new regularization hyper-parameters ( or at least , a sensible range for them ) ?",request
BJ3VB6_xG_18,"[ Minor ] The derived bounds depend on M ,",fact
BJ3VB6_xG_19,an a priori upper bound on the Renyi divergence between the logging policy and any new policy .,fact
BJ3VB6_xG_20,It 's unclear that such a bound can be tractably guessed,evaluation
BJ3VB6_xG_21,"( in contrast , prior work uses an upper bound on the importance weight",fact
BJ3VB6_xG_22,-- which is simply 1 / ( Min action selection prob . by logging policy ) ) .,fact
rJfo7HsxG_0,The paper proposes a VAE inference network for a non-parametric topic model .,fact
rJfo7HsxG_1,The model on page 4 is confusing to me,evaluation
rJfo7HsxG_2,"since this is a topic model ,",fact
rJfo7HsxG_3,"so document-specific topic distributions are required ,",fact
rJfo7HsxG_4,but what is shown is only stick-breaking for a mixture model .,fact
rJfo7HsxG_5,"From what I can tell , the model itself is not new , only the fact that a VAE is used to approximate the posterior .",evaluation
rJfo7HsxG_6,"In this case , if the model is nonparametric , then comparing with Wang , et al ( 2011 ) seems the most relevant non-deep approach .",evaluation
rJfo7HsxG_7,"Given the factorization used in that paper , the q distributions are provably optimal by the standard method .",evaluation
rJfo7HsxG_8,"Therefore , something must be gained by the VAE due to a non-factorized q.",evaluation
rJfo7HsxG_9,This would be best shown by comparing with the corresponding non-deep version of the model rather than LDA and other deep models .,request
B10Nn-jlf_0,The authors consider new attacks for generating adversarial samples against neural networks .,fact
B10Nn-jlf_1,"In particular , they are interested in approximating gradient-based white-box attacks such as FGSM in a black-box setting by estimating gradients from queries to the classifier .",fact
B10Nn-jlf_2,"They assume that the attacker is able to query , for any example x , the vector of probabilities <VAR> corresponding to each class .",fact
B10Nn-jlf_3,Given such query access it ’s trivial to estimate the gradients of p using finite differences .,fact
B10Nn-jlf_4,"As a consequence one can implement FGSM using these estimates assuming cross-entropy loss , as well as a logit-based loss .",fact
B10Nn-jlf_5,They consider both iterative and single-step FGSM attacks in the targeted ( i.e. the adversary ’s goal is to switch the example ’s label to a specific alternative label ) and un-targeted settings ( any mislabelling is a success ) .,fact
B10Nn-jlf_6,"They compare themselves to transfer black-box attacks , where the adversary trains a proxy model and generates the adversarial sample by running a white-box attack on that model .",fact
B10Nn-jlf_7,"For a number of target classifiers on both MNIST and CIFAR-10 , they show that these attacks outperform the transfer-based attacks , and are comparable to white-box attacks , while maintaining low distortion on the attack samples .",fact
B10Nn-jlf_8,"One drawback of estimating gradients using finite differences is that the number of queries required scales with the dimensionality of the examples ,",fact
B10Nn-jlf_9,which can be prohibitive in the case of images .,fact
B10Nn-jlf_10,"They therefore describe two practical approaches for query reduction — one based on random feature grouping , and the other on PCA ( which requires access to training data ) .",fact
B10Nn-jlf_11,"They once again demonstrate the effectiveness of these methods across a number of models and datasets , including models deploying adversarially trained defenses .",fact
B10Nn-jlf_12,"Finally , they demonstrate compelling real-world deployment against Clarifai classification models designed to flag “ Not Safe for Work ” content .",fact
B10Nn-jlf_13,"Overall , the paper provides a very thorough experimental examination of a practical black-box attack that can be deployed against real-world systems .",evaluation
B10Nn-jlf_14,"While there are some similarities with Chen et al. with respect to utilizing finite-differences to estimate gradients , I believe the work is still valuable for its very thorough experimental verification , as well as the practicality of their methods .",evaluation
B10Nn-jlf_15,The authors may want to be more explicit about their claim in the Related Work section that the running time of their attack is “ 40x ” less than that of Chen et al .,request
B10Nn-jlf_16,"While this is believable , there is no running time comparison in the body of the paper .",fact
ry_xOQ5ef_0,This paper creates adversarial images by imposing a flow field on an image such that the new spatially transformed image fools the classifier .,fact
ry_xOQ5ef_1,"They minimize a total variation loss in addition to the adversarial loss to create perceptually plausible adversarial images ,",fact
ry_xOQ5ef_2,this is claimed to be better than the normal L2 loss functions .,fact
ry_xOQ5ef_3,"Experiments were done on MNIST , CIFAR-10 , and ImageNet ,",fact
ry_xOQ5ef_4,which is very useful to see that the attack works with high dimensional images .,evaluation
ry_xOQ5ef_5,"However , some numbers on ImageNet would be helpful",request
ry_xOQ5ef_6,as the high resolution of it make it potentially different than the low-resolution MNIST and CIFAR .,fact
ry_xOQ5ef_7,It is a bit concerning to see some parts of Fig. 2 .,evaluation
ry_xOQ5ef_8,Some of Fig. 2 ( especially ( b ) ) became so dotted that it no longer seems an adversarial that a human eye can not detect .,fact
ry_xOQ5ef_9,And model B in the appendix looks pretty much like a normal model .,evaluation
ry_xOQ5ef_10,"It might needs some experiments , either human studies , or to test it against an adversarial detector , to ensure that the resulting adversarials are still indeed adversarials to the human eye .",request
ry_xOQ5ef_11,Another good thing to run would be to try the 3x3 average pooling restoration mechanism in the following paper :,request
ry_xOQ5ef_12,"Xin Li , Fuxin Li . Adversarial Examples Detection in Deep Networks with Convolutional Filter Statistics . ICCV 2017 .",reference
ry_xOQ5ef_13,to see whether this new type of adversarial example can still be restored by a 3x3 average pooling the image,fact
ry_xOQ5ef_14,"( I suspect that this is harder to restore by such a simple method than the previous FGSM or OPT-type , but we need some numbers ) .",evaluation
ry_xOQ5ef_15,I also do n't think FGSM and OPT are this bad in Fig. 4 .,evaluation
ry_xOQ5ef_16,Are the authors sure that if more regularization are used these 2 methods no longer fool the corresponding classifiers ?,non-arg
ry_xOQ5ef_17,I like the experiment showing the attention heat maps for different attacks .,evaluation
ry_xOQ5ef_18,"This experiment shows that the spatial transforming attack ( stAdv ) changes the attention of the classifier for each target class , and is robust to adversarially trained Inception v3 unlike other attacks like FGSM and CW .",fact
ry_xOQ5ef_19,I would likely upgrade to a 7 if those concerns are addressed .,non-arg
SJQVdQ5lG_0,This paper describes an extension to the recently introduced Transformer networks which shows better convergence properties and also improves results on standard machine translation benchmarks .,fact
SJQVdQ5lG_1,This is a great paper,evaluation
SJQVdQ5lG_2,-- it introduces a relatively simple extension of Transformer networks which only adds very few parameters and speeds up convergence and achieves better results .,fact
SJQVdQ5lG_3,It would have been good to also add a motivation for doing this,request
SJQVdQ5lG_4,"( for example , this idea can be interpreted as having a variable number of attention heads which can be blended in and out with a single learned parameter , hence making it easier to use the parameters where they are needed ) .",fact
SJQVdQ5lG_5,"Also , it would be interesting to see how important the concatenation weight and the addition weight are relative to each other --",request
SJQVdQ5lG_6,do you possibly get the same results even without the concatenation weight ?,request
SJQVdQ5lG_7,A suggested improvement : Please check the references in the introduction and see if you can find earlier ones --,request
SJQVdQ5lG_8,"for example , language modeling with RNNs has been done for a very long time , not just since 2017 which are the ones you list ;",fact
SJQVdQ5lG_9,similar for speech recognition etc. ( which probably has been done since 1993 ! ) .,fact
S1KIF7olf_0,This paper presents an empirical study of whether data augmentation can be a substitute for explicit regularization of weight decay and dropout .,fact
S1KIF7olf_1,It is a well written and well organized paper .,evaluation
S1KIF7olf_2,"However , overall I do not find the authors ’ premises and conclusions to be well supported by the results and",evaluation
S1KIF7olf_3,would suggest further investigations .,request
S1KIF7olf_4,In particular : a ) Data augmentation is a very domain specific process and limits of augmentation are often not clear .,evaluation
S1KIF7olf_5,"For example , in financial data or medical imaging data it is often not clear how data augmentation should be carried out and how much is too much .",evaluation
S1KIF7olf_6,On the other hand model regularization is domain agnostic,fact
S1KIF7olf_7,"( has to be tuned for each task , but the methodology is consistent and well known ) .",fact
S1KIF7olf_8,Thus advocating that data augmentation can universally replace explicit regularization does not seem correct .,fact
S1KIF7olf_9,b ) I find the results to be somewhat inconsistent .,evaluation
S1KIF7olf_10,"For example , on CIFAR-10 , for 100 % data regularization + augmentation is better than augmentation alone for both models ,",evaluation
S1KIF7olf_11,whereas for 80 % data augmentation alone seems to be better .,evaluation
S1KIF7olf_12,"Similarly on CIFAR-100 the WRN model shows mixed trends ,",fact
S1KIF7olf_13,and this model is significantly better than the All-CNN model in performance .,fact
S1KIF7olf_14,These results also seem inconsistent with authors statement,evaluation
S1KIF7olf_15,“ … and conclude that data augmentation alone - without any other explicit regularization techniques - can achieve the same performance to higher as regularized models … ”,quote
BkH22ZFxG_0,This paper proposes two regularization terms to encourage learning disentangled representations .,fact
BkH22ZFxG_1,One term is applied to weight parameters of a layer just like weight decay .,fact
BkH22ZFxG_2,"The other is applied to the activations of the target layer ( e.g. , the penultimate layer ) .",fact
BkH22ZFxG_3,The core part of both regularization terms is a compound hinge loss of which the input is the KL divergence between two softmax-normalized input arguments .,fact
BkH22ZFxG_4,Experiments demonstrate the proposed regularization terms are helpful in learning representations which significantly facilitate clustering performance .,evaluation
BkH22ZFxG_5,Pros : ( 1 ) This paper is clearly written and easy to follow .,evaluation
BkH22ZFxG_6,( 2 ) Authors proposed multiple variants of the regularization term which cover both supervised and unsupervised settings .,fact
BkH22ZFxG_7,"( 3 ) Authors did a variety of classification experiments ranging from time serials , image and text data .",fact
BkH22ZFxG_8,Cons : ( 1 ) The design choice of the compound hinge loss is a bit arbitrary .,evaluation
BkH22ZFxG_9,KL divergence is a natural similarity measure for probability distribution .,evaluation
BkH22ZFxG_10,"However , it seems that authors use softmax to force the weights or the activations of neural networks to be probability distributions just for the purpose of using KL divergence .",fact
BkH22ZFxG_11,"Have you compared with other choices of similarity measure , e.g. , cosine similarity ?",request
BkH22ZFxG_12,I think the comparison as an additional experiment would help explain the design choice of the proposed function .,evaluation
BkH22ZFxG_13,"( 2 ) In the binary classification experiments , it is very strange to almost randomly group several different classes of images into the same category .",evaluation
BkH22ZFxG_14,"I would suggest authors look into datasets where the class hierarchy is already provided , e.g. , ImageNet or a combination of several fine-grained image classification datasets .",request
BkH22ZFxG_15,"Additionally , I have the following questions : ( 1 ) I am curious how the proposed method compares to other competitors in terms of the original classification setting , e.g. , 10-class classification accuracy on CIFAR10 .",request
BkH22ZFxG_16,"( 2 ) What will happen for the multi-layer loss if the network architecture is very large such that you can not use large batch size , e.g. , less than 10 ?",request
BkH22ZFxG_17,"( 3 ) In drawing figure 2 and 3 , if the nonlinear activation function is not ReLU , how would you exam the same behavior ?",request
BkH22ZFxG_18,Have you tried multi-class classification for the case “ without proposed loss component ” and does the similar pattern still happen or not ?,request
BkH22ZFxG_19,"Some typos : ( 1 ) In introduction , “ when the cosine between the vectors 1 ” should be “ when the cosine between the vectors is 1 ” .",request
BkH22ZFxG_20,"( 2 ) In section 4.3 , “ we used the DBPedia ontology dataset dataset ” should be “ we used the DBPedia ontology dataset ” .",request
BkH22ZFxG_21,I would like to hear authors ’ feedback on the issues I raised .,non-arg
SkHg5PQxf_0,"While I acknowledge that training generative models with binary latent variables is hard ,",evaluation
SkHg5PQxf_1,I 'm not sure this paper really makes valuable progress in this direction .,evaluation
SkHg5PQxf_2,"The only results that seem promising are those on binarized MNIST , for the non-convolutional architecture ,",evaluation
SkHg5PQxf_3,and this setting is n't particularly exciting .,evaluation
SkHg5PQxf_4,All other experiments seem to suggest that the proposed model/algorithm is behind the state of the art .,fact
SkHg5PQxf_5,"Moreover , the proposed approach is fairly incremental , compared to existing work on RWS , VIMCO , etc .",evaluation
SkHg5PQxf_6,"So while this work seem to have been seriously and thoughtfully executed ,",evaluation
SkHg5PQxf_7,I think it falls short of the ICLR acceptance bar .,evaluation
BkEcWHKlf_0,Pros : 1 . It provided theoretic analysis why larger feature norm is preferred in feature representation learning .,fact
BkEcWHKlf_1,2 . A new regularization method ( feature incay ) is proposed .,fact
BkEcWHKlf_2,Cons : It seems there is not much comparison between this proposed method and the concurrent work,fact
BkEcWHKlf_3, COCO ( Liu et al. ( 2017c ) )  .,reference
B1ZlEVXyf_0,"Summary ======== The authors present CLEVER , an algorithm which consists in evaluating the ( local ) Lipschitz constant of a trained network around a data point .",fact
B1ZlEVXyf_1,This is used to compute a lower-bound on the minimal perturbation of the data point needed to fool the network .,fact
B1ZlEVXyf_2,"The method proposed in the paper already exists for classical function ,",fact
B1ZlEVXyf_3,they only transpose it to neural networks .,fact
B1ZlEVXyf_4,"Moreover , the lower bound comes from basic results in the analysis of Lipschitz continuous functions .",fact
B1ZlEVXyf_5,Clarity ===== The paper is clear and well-written .,evaluation
B1ZlEVXyf_6,Originality ========= This idea is not new :,evaluation
B1ZlEVXyf_7,"if we search for "" Lipschitz constant estimation "" in google scholar , we get for example Wood , G. R. , and B. P. Zhang . "" Estimation of the Lipschitz constant of a function . "" ( 1996 )",fact
B1ZlEVXyf_8,"which presents a similar algorithm ( i.e. , estimation of the maximum slope with reverse Weibull ) .",evaluation
B1ZlEVXyf_9,"Technical quality ============== The main theoretical result in the paper is the analysis of the lower-bound on \ delta , the smallest perturbation to apply on a data point to fool the network .",fact
B1ZlEVXyf_10,This result is obtained almost directly by writing the bound on Lipschitz-continuous function <EQN> where <EQN> .,evaluation
B1ZlEVXyf_11,Comments : - Lemma 3.1 : why citing Paulavicius and Zilinskas for the definition of Lipschitz continuity ?,evaluation
B1ZlEVXyf_12,"Moreover , a Lipschitz-continuous function does not need to be differentiable at all ( e.g. <VAR> is Lipschitz with constant 1 but sharp at <EQN> ) .",fact
B1ZlEVXyf_13,"Indeed , this constant can be easier obtained if the gradient exists ,",evaluation
B1ZlEVXyf_14,but this is not a requirement .,fact
B1ZlEVXyf_15,- ( Flaw ? ) Theorem 3.2 : This theorem works for fixed target-class,fact
B1ZlEVXyf_16,since <EQN> for fixed g.,fact
B1ZlEVXyf_17,"However , once <EQN> , this theorem is not clear with the constant Lq .",evaluation
B1ZlEVXyf_18,"Indeed , the function g should be <EQN> .",request
B1ZlEVXyf_19,"Thus its Lipschitz constant is different , potentially equal to <EQN> , where <VAR> is the Lipschitz constant of <VAR> .",fact
B1ZlEVXyf_20,"If the theorem remains unchanged after this modification , you should clarify the proof .",request
B1ZlEVXyf_21,"Otherwise , the theorem will work with the maximum over all Lipschitz constants but the theoretical result will be weakened .",fact
B1ZlEVXyf_22,- Theorem 4.1 : I do not see the purpose of this result in this paper .,evaluation
B1ZlEVXyf_23,This should be better motivated .,request
B1ZlEVXyf_24,"Numerical experiments ==================== Globally , the numerical experiments are in favor of the presented method .",evaluation
B1ZlEVXyf_25,"The authors should also add information about the time it takes to compute the bound , the evolution of the bound in function of the number of samples and the distribution of the relative gap between the lower-bound and the best adversarial example .",request
B1ZlEVXyf_26,"Moreover , the numerical experiments look to be realized in the context of targeted attack .",evaluation
B1ZlEVXyf_27,"To show the real effectiveness of the approach , the authors should also show the effectiveness of the lower-bound in the context of non-targeted attack .",request
S15xOyjgf_0,This paper proposes an evolutionary algorithm for solving the variational E step in expectation-maximization algorithm for probabilistic models with binary latent variables .,fact
S15xOyjgf_1,"This is done by ( i ) considering the bit-vectors of the latent states as genomes of individuals , and by ( ii ) defining the fitness of the individuals as the log joint distribution of the parameters and the latent space .",fact
S15xOyjgf_2,Pros : The paper is well written and the methodology presented is largely clear .,evaluation
S15xOyjgf_3,"Cons : While the reviewer is essentially fine with the idea of the method ,",evaluation
S15xOyjgf_4,the reviewer is much less convinced of the empirical study .,evaluation
S15xOyjgf_5,There is no comparison with other methods such as Monte carlo sampling .,fact
S15xOyjgf_6,It is not clear how computationally Evolutionary EM performs comparing to Variational EM algorithm,evaluation
S15xOyjgf_7,and there is neither experimental results nor analysis for the computational complexity of the proposed model .,fact
S15xOyjgf_8,The datasets used in the experiments are quite old .,evaluation
S15xOyjgf_9,The reviewer is concerned that these datasets may not be representative of real problems .,evaluation
S15xOyjgf_10,The applicability of the method is quite limited .,evaluation
S15xOyjgf_11,"The proposed model is only applicable for the probabilistic models with binary latent variables ,",fact
S15xOyjgf_12,hence it can not be applied to more realistic complex model with real-valued latent variables .,fact
Bysyyl5eM_0,This paper proposed to jointly train a multilingual skip-gram model and a cross-lingual sentence similarity model to construct sentence embeddings .,fact
Bysyyl5eM_1,They used cross-lingual classification tasks for evaluation .,fact
Bysyyl5eM_2,This idea is fairly simple but interesting .,evaluation
Bysyyl5eM_3,Their results on some language pairs showed that the joint training is effective,fact
Bysyyl5eM_4,( results on table 1 showed that sent-LSTM worked best with <EQN> ) .,fact
Bysyyl5eM_5,The downside of this paper is that their results could not outperform state-of-the-art results .,evaluation
Bysyyl5eM_6,"Some detailed comments : - The authors should weaken some of the statements , e.g. ‘ since our multilingual skip-gram and cross-lingual sentence similarity models are trained jointly , they can inform each other through the shared word embedding layer and promote the compositionality of learned word embeddings at training time ’ .",request
Bysyyl5eM_7,"Actually , there are no experimental results and evidences in this paper supporting this statement .",fact
Bysyyl5eM_8,- I do n’t see that ‘ Amenable to Multi-task modeling ’ is a contribution of this paper .,evaluation
Bysyyl5eM_9,The authors should report additional experimental results to prove this statement .,request
rydWCNKxz_0,Summary : This paper empirically studies adversarial perturbations dx and what the effects are of adversarial training ( AT ) with respect to shared ( dx fools for many x ) and singular ( only for a single x ) perturbations .,fact
rydWCNKxz_1,Experiments use a ( previously published ) iterative fast-gradient-sign-method and use a Resnet on CIFAR .,fact
rydWCNKxz_2,The authors conclude that in this experimental setting : - AT seems to defend models against shared dx 's .,fact
rydWCNKxz_3,"- This is visible on universal perturbations ,",fact
rydWCNKxz_4,which become less effective as more AT is applied .,fact
rydWCNKxz_5,"- AT decreases the effectiveness of adversarial perturbations , e.g . AT decreases the number of adversarial perturbations that fool both an input x and x with e.g. a contrast change .",fact
rydWCNKxz_6,"- Singular perturbations are easily detected by a detector model ,",fact
rydWCNKxz_7,as such perturbations do n't change much when applying AT .,fact
rydWCNKxz_8,Pro : - Paper addresses an important problem : qualitative / quantitative understanding of the behavior of adversarial perturbations is still lacking .,fact
rydWCNKxz_9,- The visualizations of universal perturbations as they change during AT are nice .,evaluation
rydWCNKxz_10,- The basic observation wrt the behavior of AT is clearly communicated .,evaluation
rydWCNKxz_11,"Con : - The experiments performed are interesting directions , although unfocused and rather limited in scope .",evaluation
rydWCNKxz_12,"For instance , does the same phenomenon happen for different datasets ?",request
rydWCNKxz_13,Different models ?,request
rydWCNKxz_14,- What happens when we use adversarial attacks different from FGSM ?,request
rydWCNKxz_15,Do we get similar results ?,request
rydWCNKxz_16,- The papers lacks a more in-depth theoretical analysis .,evaluation
rydWCNKxz_17,Is there a principled reason AT+FGSM defends against universal perturbations ?,request
rydWCNKxz_18,"Overall : - As is , it seems to me the paper lacks a significant central message ( due to limited and unfocused experiments ) or significant new theoretical insight into the effect of AT .",evaluation
rydWCNKxz_19,A number of questions addressed are interesting starting points towards a deeper understanding of * how * the observations can be explained and more rigorous empirical investigations .,evaluation
rydWCNKxz_20,Detailed : -,non-arg
ryT2f8KgM_0,"This paper continues a trend of incremental improvements to Wasserstein GANs ( WGAN ) ,",fact
ryT2f8KgM_1,where the latter were proposed in order to alleviate the difficulties encountered in training GANs .,fact
ryT2f8KgM_2,"Originally , Arjovsky et al . [ 1 ] argued that the Wasserstein distance was superior to many others typically used for GANs .",fact
ryT2f8KgM_3,"An important feature of WGANs is the requirement for the discriminator to be 1-Lipschitz ,",evaluation
ryT2f8KgM_4,which [ 1 ] achieved simply by clipping the network weights .,fact
ryT2f8KgM_5,"Recently , Gulrajani et al. [ 2 ] proposed a gradient penalty "" encouraging "" the discriminator to be 1-Lipschitz .",fact
ryT2f8KgM_6,"However , their approach estimated continuity on points between the generated and the real samples ,",fact
ryT2f8KgM_7,and thus could fail to guarantee Lipschitz-ness at the early training stages .,fact
ryT2f8KgM_8,The paper under review overcomes this drawback by estimating the continuity on perturbations of the real samples .,fact
ryT2f8KgM_9,"Together with various technical improvements , this leads to state-of-the-art practical performance both in terms of generated images and in semi-supervised learning .",fact
ryT2f8KgM_10,"In terms of novelty , the paper provides one core conceptual idea followed by several tweaks aimed at improving the practical performance of GANs .",fact
ryT2f8KgM_11,The key conceptual idea is to perturb each data point twice and use a Lipschitz constant to bound the difference in the discriminator ’s response on the perturbed points .,fact
ryT2f8KgM_12,The proposed method is used in eq . ( 6 ) together with the gradient penalty from [ 2 ] .,fact
ryT2f8KgM_13,The authors found that directly perturbing the data with Gaussian noise led to inferior results,fact
ryT2f8KgM_14,and therefore propose to perturb the hidden layers using dropout .,fact
ryT2f8KgM_15,For supervised learning they demonstrate less overfitting for both MNIST and CIFAR 10 .,fact
ryT2f8KgM_16,They also extend their framework to the semi-supervised setting of Salismans et al 2016 and report improved image generation .,fact
ryT2f8KgM_17,The authors do an excellent comparative job in presenting their experiments .,evaluation
ryT2f8KgM_18,"They compare numerous techniques ( e.g. , Gaussian noise , dropout ) and demonstrates the applicability of the approach for a wide range of tasks .",fact
ryT2f8KgM_19,"They use several criteria to evaluate their performance ( images , inception score , semi-supervised learning , overfitting , weight histogram ) and compare against a wide range of competing papers .",fact
ryT2f8KgM_20,Where the paper could perhaps be slightly improved is writing clarity .,request
ryT2f8KgM_21,"In particular , the discussion of M and M ' is vital to the point of the paper ,",evaluation
ryT2f8KgM_22,but could be written in a more transparent manner .,request
ryT2f8KgM_23,The same goes for the semi-supervised experiment details and the CIFAR-10 augmentation process .,request
ryT2f8KgM_24,"Finally , the title seems uninformative .",evaluation
ryT2f8KgM_25,"Almost all progress is incremental ,",evaluation
ryT2f8KgM_26,"and the authors modestly give credit to both [ 1 ] and [ 2 ] ,",fact
ryT2f8KgM_27,but the title is neither memorable nor useful in expressing the novel idea .,evaluation
ryT2f8KgM_28,[1] <CIT>,reference
ryT2f8KgM_29,[2] <CIT>,reference
HyvZDmueM_0,This paper proposed a new optimization framework for semi-supervised learning based on derived inversion scheme for deep neural networks .,fact
HyvZDmueM_1,The numerical experiments show a significant improvement in accuracy of the approach .,evaluation
B1g5pBTxz_0,"The article "" Do GANs Learn the Distribution ? Some Theory and Empirics "" considers the important problem of quantifying whether the distributions obtained from generative adversarial networks come close to the actual distribution of images .",fact
B1g5pBTxz_1,The authors argue that GANs in fact generate the distributions with fairly low support .,fact
B1g5pBTxz_2,The proposed approach relies on so-called birthday paradox,fact
B1g5pBTxz_3,which allows to estimate the number of objects in the support by counting number of matching ( or very similar ) pairs in the generated sample .,fact
B1g5pBTxz_4,This test is expected to experimentally support the previous theoretical analysis by Arora et al. ( 2017 ) .,fact
B1g5pBTxz_5,The further theoretical analysis is also performed showing that for encoder-decoder GAN architectures the distributions with low support can be very close to the optimum of the specific ( BiGAN ) objective .,fact
B1g5pBTxz_6,The experimental part of the paper considers the CelebA and CIFAR-10 datasets .,fact
B1g5pBTxz_7,We definitely see many very similar images in fairly small sample generated .,evaluation
B1g5pBTxz_8,"So , the general claim is supported .",evaluation
B1g5pBTxz_9,"However , if you look closely at some pictures , you can see that they are very different though reported as similar .",fact
B1g5pBTxz_10,"For example , some deer or truck pictures .",fact
B1g5pBTxz_11,"That 's why I would recommend to reevaluate the results visually ,",request
B1g5pBTxz_12,which may lead to some change in the number of near duplicates and consequently the final support estimates .,evaluation
B1g5pBTxz_13,"To sum up , I think that the general idea looks very natural and the results are supportive .",evaluation
B1g5pBTxz_14,"On theoretical side , the results seem fair ( though I did n't check the proofs )",evaluation
B1g5pBTxz_15,"and , being partly based on the previous results of Arora et al. ( 2017 ) , clearly make a step further .",evaluation
BktJHw_lM_0,The paper discusses a setting in which an existing dataset/trained model is augmented/refined by adding additional datapoints .,fact
BktJHw_lM_1,"Issues of how to price the new data are discussed in a high level , abstract way , and arguments against retrieving the new data for free or encrypting it are presented .",fact
BktJHw_lM_2,"Overall , the paper is of an expository nature ,",evaluation
BktJHw_lM_3,"discussing high-level ideas rather than actually implementing them ,",fact
BktJHw_lM_4,and does not experimentally or theoretically substantiate any of its claims .,fact
BktJHw_lM_5,This makes the technical contribution rather shallow .,evaluation
BktJHw_lM_6,"Interesting questions do arise , such as how to assess the value of new data and how to price datapoints ,",fact
BktJHw_lM_7,but these questions are never addressed ( neither theoretically nor empirically ) .,fact
BktJHw_lM_8,"Though main points are valid ,",evaluation
BktJHw_lM_9,"the paper is also rife with informal statements and logical jumps ,",evaluation
BktJHw_lM_10,perhaps due to the expository/high-level approach taken in discussing these issues .,fact
BktJHw_lM_11,Detailed comments : The ( informal ) information theoretic argument has a few holes .,evaluation
BktJHw_lM_12,"The claim is roughly that every datapoint ( ~ 1Mbyte image ) contributes ~ 1M bits of changes in a model ,",fact
BktJHw_lM_13,which can be quite revealing .,evaluation
BktJHw_lM_14,"As a result , there is no benefit from encrypting the datapoint , as the mapping from inputs to changes is insecure ( in an information-theoretic sense ) in itself .",fact
BktJHw_lM_15,This assumes that every step of stochastic gradient descent ( one step per image ) is done in the clear ;,fact
BktJHw_lM_16,this is not what one would consider secure in cryptography literature .,evaluation
BktJHw_lM_17,A secure function evaluation ( SFE ) would encrypt the data and the computation in an end-to-end fashion ;,fact
BktJHw_lM_18,"in particular , it would only reveal the final outcome of SGD over all images in the dataset without revealing any intermediate steps .",fact
BktJHw_lM_19,"Presuming that the new dataset is large ( i.e. , having N images ) , the "" information theoretic "" limit becomes ~ N x 1Mbyte inputs for ~ 1M function outputs ( the finally-trained model ) .",fact
BktJHw_lM_20,"In this sense , this argument that "" encryption is hopeless "" is somewhat brittle .",evaluation
BktJHw_lM_21,"Encryption-issues aside , the paper would have been much stronger if it spent more effort in formalizing or evaluating different methods for assessing the value of data .",request
BktJHw_lM_22,"The authors approach this by treating the ML algorithm as a blackbox , and using influence functions ( a la Bastani 2017 ) to assess the impact of different inputs on the finally trained model",fact
BktJHw_lM_23,"( again , this is proposed but not implemented/explored/evaluated in any way ) .",fact
BktJHw_lM_24,"This is a design choice , but it is not obvious .",evaluation
BktJHw_lM_25,There is extensive literature in statistics and machine learning on the areas of experimental design and active learning .,fact
BktJHw_lM_26,"Both are active , successful research areas , and both can be provide tools to formally reason about the value of data/labels not yet seen ;",evaluation
BktJHw_lM_27,the paper summarily ignores this literature .,evaluation
BktJHw_lM_28,"Examples of imprecise/informal statements : "" The fairness in the pricing is highly questionable """,quote
BktJHw_lM_29, implicit contracts get difficult to verify ,quote
BktJHw_lM_30, The fairness in the pricing is dubious ,quote
BktJHw_lM_31," As machine learning models become more and more complicated , its ( sic ) capability can outweigh the privacy guarantees encryption gives us ",quote
BktJHw_lM_32," as an image classifier 's model architecture changes , all the data would need to be collected and purchased again ",quote
BktJHw_lM_33, Interpretability solutions aim to alleviate the notoriety of reasonability of neural networks ,quote
S1ZbRMqlM_0,"The paper suggests taking GloVe word vectors , adjust them , and then use a non-Euclidean similarity function between them .",fact
S1ZbRMqlM_1,"The idea is tested on very small data sets ( 80 and 50 examples , respectively ) .",fact
S1ZbRMqlM_2,"The proposed techniques are a combination of previously published steps ,",fact
S1ZbRMqlM_3,and the new algorithm fails to reach state-of-the-art on the tiny data sets .,fact
S1ZbRMqlM_4,"It is n't clear what the authors are trying to prove ,",evaluation
S1ZbRMqlM_5,nor whether they have successfully proven what they are trying to prove .,evaluation
S1ZbRMqlM_6,Is the point that GloVe is a bad algorithm ?,non-arg
S1ZbRMqlM_7,That these steps are general ?,non-arg
S1ZbRMqlM_8,"If the latter , then the experimental results are far weaker than what I would find convincing .",evaluation
S1ZbRMqlM_9,Why not try on multiple different word embeddings ?,request
S1ZbRMqlM_10,What happens if you start with random vectors ?,non-arg
S1ZbRMqlM_11,What happens when you try a bigger data set or a more complex problem ?,non-arg
HJfRKPFeM_0,SUMMARY . The paper presents an extension of word2vec for structured features .,fact
HJfRKPFeM_1,"The authors introduced a new compatibility function between features and , as in the skipgram approach , they propose a variation of negative sampling to deal with structured features .",fact
HJfRKPFeM_2,The learned representation of features is tested on a recommendation-like task .,fact
HJfRKPFeM_3,---------- OVERALL JUDGMENT The paper is not clear,evaluation
HJfRKPFeM_4,and thus I am not sure what I can learn from it .,evaluation
HJfRKPFeM_5,From what is written on the paper I have trouble to understand the definition of the model the authors propose and also an actual NLP task where the representation induced by the model can be useful .,evaluation
HJfRKPFeM_6,"For this reason , I would suggest the authors make clear with a more formal notation , and the use of examples , what the model is supposed to achieve .",request
HJfRKPFeM_7,"---------- DETAILED COMMENTS When the authors refer to word2vec is not clear if they are referring to skipgram or cbow algorithm ,",evaluation
HJfRKPFeM_8,please make it clear .,request
HJfRKPFeM_9,"Bottom of page one : "" a positive example is ' semantic ' "" ,",quote
HJfRKPFeM_10,"please , use another expression to describe observable examples ,",request
HJfRKPFeM_11,' semantic ' does not make sense in this context .,evaluation
HJfRKPFeM_12,"Levi and Goldberg ( 2014 ) do not say anything about factorization machines ,",fact
HJfRKPFeM_13,could the authors clarify this point ?,request
HJfRKPFeM_14,"Equation ( 4 ) , what do i and j stand for ?",request
HJfRKPFeM_15,what does \ beta represent ?,request
HJfRKPFeM_16,is it the embedding vector ?,request
HJfRKPFeM_17,How is this formula related to skipgram or cbow ?,request
HJfRKPFeM_18,The introduction of structured deep-in factorization machine should be more clear with examples that give the intuition on the rationale of the model .,request
HJfRKPFeM_19,"The experimental section is rather poor ,",evaluation
HJfRKPFeM_20,"first , the authors only compare themselves with word2ve ( cbow ) ,",fact
HJfRKPFeM_21,it is not clear what the reader should learn from the results the authors got .,evaluation
HJfRKPFeM_22,"Finally , the most striking flaw of this paper is the lack of references to previous works on word embeddings and feature representation ,",evaluation
HJfRKPFeM_23,I would suggest the author check and compare themselves with previous work on this topic .,request
HJZM5e9eM_0,"Summary This article considers neural networks over time-series , defined as a succession of convolutions and fully-connected layers with Leaky ReLU activations .",fact
HJZM5e9eM_1,The authors provide relatively general conditions for transformations described by such networks to admit a Lipschitz-continuous inverse .,evaluation
HJZM5e9eM_2,They extend these results to the case where the first layer is a convolution with irregular sampling .,fact
HJZM5e9eM_3,"Finally , they show that the first convolutional filters can be chosen so as to represent a discrete wavelet transform , and provide some numerical experiments .",fact
HJZM5e9eM_4,"Main remarks While the introduction seemed promising ,",evaluation
HJZM5e9eM_5,"and I enjoyed the writing style ,",evaluation
HJZM5e9eM_6,I was disappointed with this article .,evaluation
HJZM5e9eM_7,( 1 ) There are many mistakes in the mathematical statements .,fact
HJZM5e9eM_8,"First , in Theorem 1.1 , I do not think that <VAR> is a non-linear frame ,",evaluation
HJZM5e9eM_9,because I do not see why it should be of the form of Definition 1.2 ( what would be the functions psi_n ? ) .,evaluation
HJZM5e9eM_10,"For the same reason , I also do not understand Theorem 1.2 .",evaluation
HJZM5e9eM_11,"In Proof 1.4 , the line of equalities after « Also with the Plancherel formula » is , in my opinion , not true ,",evaluation
HJZM5e9eM_12,because the <VAR> norm of a product of functions is not the product of the <VAR> norms of the functions .,fact
HJZM5e9eM_13,"It also seems to me that Theorem 1.3 , from [ Benedetto , 1992 ] , is incorrect :",fact
HJZM5e9eM_14,"it is not the limit of <VAR> that must be larger than 2R , but the limit of <VAR> ( with N_n the number of t_i 's that belong to the interval [ - n ; n ] ) ,",fact
HJZM5e9eM_15,"and there must probably be a compatibility condition between <VAR> and R_1 , not only between <VAR> and R.",fact
HJZM5e9eM_16,"In Proposition 1.6 , I think that the equality should be a strict inequality .",evaluation
HJZM5e9eM_17,"Additionally , I do not say that Proof 2.1 is not true ,",non-arg
HJZM5e9eM_18,but the fact that the undersampling by a factor 2 does not prevent the operator from being a frame should be justified .,request
HJZM5e9eM_19,"( 2 ) The authors do not justify , in the introduction , why admitting a continuous inverse should be a crucial criterion of quality for the representation described by a neural network .",fact
HJZM5e9eM_20,"Additionally , the existence of this continous inverse relies on the fact that the non-linearity that is used is a Leaky ReLU ,",fact
HJZM5e9eM_21,"which looks a bit like "" cheating "" to me ,",evaluation
HJZM5e9eM_22,"because the Lipschitz constant of the inverse of a Leaky ReLU , although finite , is large ,",fact
HJZM5e9eM_23,"so it seems to me that cascading several layers with Leaky ReLUs could encode a transformation with strictly positive , but still very poor frame bounds .",evaluation
HJZM5e9eM_24,"( 3 ) I also do not understand why having "" orthogonal outputs "" , as in Section 2 , is really desirable ;",evaluation
HJZM5e9eM_25,I think that it should be better justified .,request
HJZM5e9eM_26,"Also , there are probably other ways to achieve orthogonality than using wavelets in the first layer ,",evaluation
HJZM5e9eM_27,"so the fact that wavelets achieve orthogonality does not really justify why using wavelets in the first layer is a good choice , compared to other filters .",evaluation
HJZM5e9eM_28,"( 4 ) I had understood in the introduction that the authors would explain how to define a ( good ) deep representation for data of the form <VAR> , where each x_n would be the value of a time series at instant t_n , with the t_n non-uniformly spaced .",evaluation
HJZM5e9eM_29,"But all the representations considered in the article seem to be applicable to functions in <VAR> only ( like in Theorem 1.4 and Theorem 2.2 ) , and not to sequences <VAR> .",evaluation
HJZM5e9eM_30,There is something that I did not get here .,non-arg
HJZM5e9eM_31,"Minor remarks - Fourth paragraph , third line : "" this generalization frames "" ?",evaluation
HJZM5e9eM_32,"- Last paragraph before "" Contributions & Organization "" : "" that that "" .",request
HJZM5e9eM_33,- Paragraph about notations : it seems to me that what is defined as <VAR> is denoted as <VAR> after the introduction .,evaluation
HJZM5e9eM_34,"- Last line of this paragraph : <VAR> should be <VAR> , and <VAR> .",request
HJZM5e9eM_35,"- I think "" smooth "" could be replaced by "" continuous """,request
HJZM5e9eM_36,( smoothness implies a notion of differentiability ) .,evaluation
HJZM5e9eM_37,"- Paragraph before Proposition 1.1 : \ sqrt { s } is not defined , and "" is supported "" should be "" are supported "" .",request
HJZM5e9eM_38,- Theorem 1.1 : the f_k should be phi_k .,request
HJZM5e9eM_39,"- Definition 1.4 : "" piece-linear "" -> "" piecewise linear "" ?",request
HJZM5e9eM_40,- Lemma 1.2 and Proof 1.4 : there are indices missing to \ tilde h and \ tilde g.,request
HJZM5e9eM_41,"- Proof 1.4 : "" and finally "" -> "" And finally "" .",request
HJZM5e9eM_42,- Proof 1.5 : I do not understand the grammatical structure of the second sentence .,evaluation
HJZM5e9eM_43,- Proposition 1.4 : the definition of a RNN is the same as definition 1.2 ( except for the frame bounds ) ;,fact
HJZM5e9eM_44,I do not see why such transformations should model RNNs .,evaluation
HJZM5e9eM_45,"- Paragraph before Proposition 1.5 : "" in , formation "" .",non-arg
HJZM5e9eM_46,- Proposition 1.6 : it should be said on which space the frame is injective .,request
HJZM5e9eM_47,"- On page 8 , "" Lipschitz "" is erroneously written ( twice ) .",fact
HJZM5e9eM_48,"- Proposition 1.7 : "" ProjW , l "" ?",non-arg
HJZM5e9eM_49,"- Definition 2.1 : in the "" nested "" property , I think that the inclusion should be the other way around .",request
HJZM5e9eM_50,"- Before Theorem 2.1 , the sentence "" Such Riesz basis is proven "" is unclear to me .",evaluation
HJZM5e9eM_51,"- Theorem 2.1 : "" filters convolution filters "" .",non-arg
HJZM5e9eM_52,- I think the architecture described in Theorem 2.2 could be clarified ;,request
HJZM5e9eM_53,I am not exactly sure where all the arrows start from .,evaluation
HJZM5e9eM_54,"- First line of Subsection 2.3 : "" . is always "" -> "" is always "" .",request
HJZM5e9eM_55,"- First paragraph of Subsection 3.2 : "" the the "" .",non-arg
HJZM5e9eM_56,- Paragraph 3.2 : could the previous algorithms developed for this dataset be described in slightly more detail ?,request
HJZM5e9eM_57,"I also do not understand the meaning of "" must solely leverage the temporal structure "" .",evaluation
HJZM5e9eM_58,"- I think that the section about numerical experiments could be slightly rewritten , so that the architecture used in each experiment is clearer .",request
HJZM5e9eM_59,"In Paragraph 3.2 in particular , I did not get why the architecture presented in Figure 6 has far fewer parameters than the one in Figure 5 ;",evaluation
HJZM5e9eM_60,it would help if the authors clearly precised how many parameters each layer contains .,request
HJZM5e9eM_61,"- Conclusion : "" we can to "" -> "" we can "" .",request
HJZM5e9eM_62,- Definition 4.1 : <VAR> -> <VAR> .,request
H1JAev9gz_0,The authors present 3 architectures for learning representations of programs from execution traces .,fact
H1JAev9gz_1,"In the variable trace embedding , the input to the model is given by a sequence of variable values .",fact
H1JAev9gz_2,The state trace embedding combines embeddings for variable traces using a second recurrent encoder .,fact
H1JAev9gz_3,The dependency enforcement embedding performs element-wise multiplication of embeddings for parent variables to compute the input of the GRU to compute the new hidden state of a variable .,fact
H1JAev9gz_4,The authors evaluate their architectures on the task of predicting error patterns for programming assignments from Microsoft DEV204 .1 X ( an introduction to C# offered on edx ) and problems on the Microsoft CodeHunt platform .,fact
H1JAev9gz_5,They additionally use their embeddings to decrease the search time for the Sarfgen program repair system .,fact
H1JAev9gz_6,This is a fairly strong paper .,evaluation
H1JAev9gz_7,The proposed models make sense,evaluation
H1JAev9gz_8,"and the writing is for the most part clear ,",evaluation
H1JAev9gz_9,though there are a few places where ambiguity arises :,evaluation
H1JAev9gz_10,"- The variable "" Evidence "" in equation ( 4 ) is never defined .",fact
H1JAev9gz_11,"- The authors refer to "" predicting the error patterns "" ,",fact
H1JAev9gz_12,but again do n't define what an error pattern is .,fact
H1JAev9gz_13,"The appendix seems to suggest that the authors are simply performing multilabel classification based on a predefined set of classes of errors ,",evaluation
H1JAev9gz_14,is this correct ?,non-arg
H1JAev9gz_15,- It is not immediately clear from Figures 3 and 4 that the architectures employed are in fact recurrent .,evaluation
H1JAev9gz_16,"- Figure 5 seems to suggest that dependencies are only enforced at points in a program where assignment is performed for a variable ,",evaluation
H1JAev9gz_17,is this correct ?,non-arg
H1JAev9gz_18,"Assuming that the authors can address these clarity issues , I would in principle be happy for the paper to appear .",evaluation
ryU7ZMsgf_0,This paper presents a reparametrization of the perturbation applied to features in adversarial examples based attacks .,fact
ryU7ZMsgf_1,It tests this attack variation on against Inception-family classifiers on ImageNet .,fact
ryU7ZMsgf_2,It shows some experimental robustness to JPEG encoding defense .,fact
ryU7ZMsgf_3,"Specifically about the method : Instead of perturbating a feature x_i by delta_i , as in other attacks , with delta_i in range [ - Delta_i , Delta_i ] , they propose to perturbate <VAR> , which is recentered in the domain of x_i through a heuristic ( ( x_i ± Delta_i + domain boundary that would be clipped ) / 2 ) , and have a similar heuristic for computing a <VAR> .",fact
ryU7ZMsgf_4,"Instead of perturbating <VAR> directly by delta_i , they compute the perturbed x by <VAR> ,",fact
ryU7ZMsgf_5,so they follow the gradient of loss to misclassify w.r.t. r ( instead of delta ) .,fact
ryU7ZMsgf_6,+ / - : + The presentation of the method is clear .,evaluation
ryU7ZMsgf_7,+ ImageNet is a good dataset to benchmark on .,evaluation
ryU7ZMsgf_8,- ( ! ) The ( ensemble ) white-box attack is effective,evaluation
ryU7ZMsgf_9,"but the results are not compared to anything else , e.g. it could be compared to ( vanilla ) FGSM nor C&W .",fact
ryU7ZMsgf_10,"- The other attack demonstrated is actually a grey-box attack ,",fact
ryU7ZMsgf_11,"as 4 out of the 5 classifiers are known , they are attacking the 5th ,",fact
ryU7ZMsgf_12,but in particular all the 5 classifiers are Inception-family models .,fact
ryU7ZMsgf_13,"- The experimental section is a bit sloppy at times ( e.g. enumerating more than what is actually done , starting at 3.1.1 . ) .",evaluation
ryU7ZMsgf_14,- The results on their JPEG approximation scheme seem too explorative ( early in their development ) to be properly compared .,evaluation
ryU7ZMsgf_15,"I think that the paper need some more work , in particular to make more convincing experiments that the benefit lies in CIA ( baselines comparison ) , and that it really is robust across these defenses shown in the paper .",request
S1GVQk5gG_0,This paper is about rethinking how to use encoder-decoder architectures for representation learning when the training objective contains a similarity between the decoder output and the encoding of something else .,fact
S1GVQk5gG_1,"For example , for the skip-thought RNN encoder-decoder that encodes a sentence and decodes neighboring sentences : rather than use the final encoder hidden state as the representation of the sentence , the paper uses some function of the decoder ,",fact
S1GVQk5gG_2,since the training objective is to maximize each dot product between a decoder hidden state and the embedding of a context word .,fact
S1GVQk5gG_3,"If dot product ( or cosine similarity ) is going to be used as the similarity function for the representation , then it makes more sense , the paper argues , to use the decoder hidden state ( s ) as the representation of the input sentence .",fact
S1GVQk5gG_4,The paper considers both averaging and concatenating hidden states .,fact
S1GVQk5gG_5,"One difficulty here is that the neighboring sentences are typically not available in downstream tasks ,",fact
S1GVQk5gG_6,"so the paper runs the decoder to produce a predicted sentence one word-at-a-time , using the predicted words as inputs to the decoder RNNs .",fact
S1GVQk5gG_7,Then those decoder RNN hidden states are used via averaging or concatenation,fact
S1GVQk5gG_8,as the representation of a sentence in downstream tasks .,fact
S1GVQk5gG_9,"This paper is a source of contributions ,",fact
S1GVQk5gG_10,but I think in its current form it is not yet ready for publication .,evaluation
S1GVQk5gG_11,Pros : I think it makes sense to pay attention to the training objective when deciding how to use the model for downstream tasks .,evaluation
S1GVQk5gG_12,I like the empirical investigation of combining RNN and BOW encoders and decoders .,evaluation
S1GVQk5gG_13,The experimental results show that a single encoder-decoder model can be trained and then two different functions of it can be used at test time for different kinds of tasks ( RNN-RNN for supervised transfer and RNN-RNN-mean for unsupervised transfer ) .,fact
S1GVQk5gG_14,I think this is an interesting result .,evaluation
S1GVQk5gG_15,Cons : I have several concerns .,evaluation
S1GVQk5gG_16,The first relate to the theoretical arguments and their empirical support .,evaluation
S1GVQk5gG_17,"Regarding the theoretical arguments : First , the paper discusses the notion of an "" optimal representation space "" and describes the argument as theoretical ,",fact
S1GVQk5gG_18,but I do n't see much of a theoretical argument here .,fact
S1GVQk5gG_19,"As far as I can tell , the paper does not formally define its terms or define in what sense the representation space is "" optimal "" .",fact
S1GVQk5gG_20,"I can only find heuristic statements like those in the paragraph in Sec 3.2 that begins "" These observations ... "" .",fact
S1GVQk5gG_21,"What exactly is meant formally by statements like "" any model where the decoder is log-linear with respect to the encoder "" or "" that distance is optimal with respect to the model ’s objective "" ?",non-arg
S1GVQk5gG_22,"It seems like the paper may want to start with formal definitions of an encoder and a decoder , then define what is meant by a "" decoder that is log-linear with respect to the encoder "" , and define what it means for a distance to be optimal with respect to a training objective .",request
S1GVQk5gG_23,"That seems necessary in order to provide the foundation to make any theoretical statement about choices for encoders , decoders , and training objectives .",evaluation
S1GVQk5gG_24,"I am still not exactly sure what that theoretical statement might look like ,",evaluation
S1GVQk5gG_25,but maybe defining the terms would help the authors get started in heading toward the goal of defining a statement to prove .,request
S1GVQk5gG_26,"Second , the paper 's theoretical story seems to diverge almost immediately from the choices used in the model and experimental procedure .",evaluation
S1GVQk5gG_27,"For example , in Sec. 3.2 , it is stated that cosine similarity "" is the appropriate similarity measure in the case of log-linear decoders . """,fact
S1GVQk5gG_28,But the associated footnote ( footnote 2 ) seems to admit a contradiction here by noting that actually the appropriate similarity measure is dot product :,fact
S1GVQk5gG_29," Evidently , the correct measure is actually the dot product . ",quote
S1GVQk5gG_30,This is a bit confusing .,evaluation
S1GVQk5gG_31,"It also raises a question : If cosine similarity will be used later for computing similarity , then why not try using cosine similarity in place of dot product in the model ?",evaluation
S1GVQk5gG_32,"That is , replace "" u_w \ cdot h_i "" in Eq . ( 2 ) with "" cos ( u_w , h_i ) "" .",fact
S1GVQk5gG_33,"If the paper 's story is correct ( and if I understand the ideas correctly ) , training with cosine similarity should work better than training with dot product ,",fact
S1GVQk5gG_34,because the similarity function used during training is more similar to that used in testing .,fact
S1GVQk5gG_35,This seems like a natural experiment to try .,request
S1GVQk5gG_36,Other natural experiments would be to vary both the similarity function used in the model during training and the similarity function used at test time .,evaluation
S1GVQk5gG_37,The authors ' claims could be validated if the optimal choices always use the same choice for the training and test-time similarity functions .,fact
S1GVQk5gG_38,"That is , if Euclidean distance is used during training , then will Euclidean distance be the best choice at test time ?",fact
S1GVQk5gG_39,Another example of the divergence lies in the use of the skip-thought decoder on downstream tasks .,fact
S1GVQk5gG_40,"Since the decoder hidden states depend on neighboring sentences and these are considered to be unavailable at test time ,",fact
S1GVQk5gG_41,"the paper "" unrolls "" the decoder for several steps by using it to predict words which are then used as inputs on the next time step .",fact
S1GVQk5gG_42,"To me , this is a potentially very significant difference between training and testing .",evaluation
S1GVQk5gG_43,"Since much of the paper is about reconciling training and testing conditions in terms of the representation space and similarity function ,",evaluation
S1GVQk5gG_44,this difference feels like a divergence from the theoretical story .,fact
S1GVQk5gG_45,It is only briefly mentioned at the end of Sec. 3.3 and then discussed again later in the experiments section .,fact
S1GVQk5gG_46,I think this should be described in more detail in Section 3.3,request
S1GVQk5gG_47,because it is an important note about how the model will be used in practice .,evaluation
S1GVQk5gG_48,"It would be nice to be able to quantify the impact ( of unrolling the decoder with predicted words ) by , for example , using the decoder on a downstream evaluation dataset that has neighboring sentences in it .",request
S1GVQk5gG_49,"Then the actual neighboring sentences can be used as inputs to the decoder when it is unrolled , which would be closer to the training conditions",request
S1GVQk5gG_50,and we could empirically see the difference .,request
S1GVQk5gG_51,Perhaps there is an evaluation dataset with ordered sentences so that the authors could empirically compare using real vs predicted inputs to the decoder on a downstream task ?,non-arg
S1GVQk5gG_52,The above experiments might help to better connect the experiments section with the theoretical arguments .,evaluation
S1GVQk5gG_53,"Other concerns , including more specific points , are below : Sec. 2 : When describing inferior performance of RNN-based models on unsupervised sentence similarity tasks , the paper states : "" While this shortcoming of SkipThought and RNN-based models in general has been pointed out , to the best of our knowledge , it has never been systematically addressed in the literature before . """,fact
S1GVQk5gG_54,The authors may want to check Wieting & Gimpel ( 2017 ) ( and its related work ) which investigates the inferiority of LSTMs compared to word averaging for unsupervised sentence similarity tasks .,request
S1GVQk5gG_55,They found that averaging the encoder hidden states can work better than using the final encoder hidden state ;,fact
S1GVQk5gG_56,the authors may want to try that as well .,request
S1GVQk5gG_57,"Sec. 3.2 : When describing FastSent , the paper includes "" Due to the model 's simplicity , it is particularly fast to train and evaluate , yet has shown state-of-the-art performance in unsupervised similarity tasks ( Hill et al. , 2015 ) . """,fact
S1GVQk5gG_58,"I do n't think it makes much sense to cite the SimLex-999 paper in this context ,",evaluation
S1GVQk5gG_59,as that is a word similarity task and that paper does not include any results of FastSent .,fact
S1GVQk5gG_60,Maybe the Hill et al ( 2016 ) FastSent citation was meant instead ?,non-arg
S1GVQk5gG_61,"But in that case , I do n't think it is quite accurate to make the claim that FastSent is SOTA on unsupervised similarity tasks .",fact
S1GVQk5gG_62,"In the original FastSent paper ( Hill et al. , 2016 ) , FastSent is not as good as CPHRASE or "" DictRep BOW + embs "" on average across the unsupervised sentence similarity evaluations .",fact
S1GVQk5gG_63,FastSent is also not as good as sent2vec from Pagliardini et al ( 2017 ) or charagram-phrase from Wieting et al. ( 2016 ) .,fact
S1GVQk5gG_64,"Sec. 3.3 : In describing skip-thought , the paper states : "" While computationally complex , it is currently the state-of-the-art model for supervised transfer tasks ( Hill et al. , 2016 ) . """,fact
S1GVQk5gG_65,"I do n't think it is accurate to state that skip-thought is still state-of-the-art for supervised transfer tasks , in light of recent work ( Conneau et al. , 2017 ; Gan et al. , 2017 ) .",fact
S1GVQk5gG_66,"Sec. 3.3 : When discussing averaging the decoder hidden states , the paper states : "" Intuitively , this corresponds to destroying the word order information the decoder has learned . """,fact
S1GVQk5gG_67,I 'm not sure this strong language can be justified here .,evaluation
S1GVQk5gG_68,Is there any evidence to suggest that averaging the decoder hidden states will destroy word order information ?,request
S1GVQk5gG_69,"The hidden states may be representing word order information in a way that is robust to averaging , i.e. , in a way such that the average of the hidden states can still lead to the reconstruction of the word order .",fact
S1GVQk5gG_70,Sec. 4 : What does it mean to use an RNN encoder and a BOW decoder ?,request
S1GVQk5gG_71,"This seems to be a strongly-performing setting and competitive with RNN-mean ,",evaluation
S1GVQk5gG_72,but I do n't know exactly what this means .,evaluation
S1GVQk5gG_73,"Minor things : Sec. 3.1 : When defining v_w , it would be helpful to make explicit that it 's in \ mathbb { R } ^ d.",request
S1GVQk5gG_74,"Sec. 4 : For TREC question type classification , I think the correct citation should be Li & Roth ( 2002 ) instead of Vorhees ( 2002 ) .",request
S1GVQk5gG_75,"Sec. 5 : I think there 's a typo in the following sentence : "" Our results show that , for example , the raw encoder output for SkipThought ( RNN-RNN ) achieves strong performance on supervised transfer , whilst its mean decoder output ( RNN-mean ) achieves strong performance on supervised transfer . """,request
S1GVQk5gG_76,"I think "" unsupervised "" was meant in the latter mention .",request
S1GVQk5gG_77,"References : Conneau , A. , Kiela , D. , Schwenk , H. , Barrault , L. , & Bordes , A. ( 2017 ) . Supervised Learning of Universal Sentence Representations from Natural Language Inference Data . EMNLP .",reference
S1GVQk5gG_78,"Gan , Z. , Pu , Y. , Henao , R. , Li , C. , He , X. , & Carin , L. ( 2017 ) . Learning generic sentence representations using convolutional neural networks . EMNLP .",reference
S1GVQk5gG_79,"Li , X. , & Roth , D. ( 2002 ) . Learning question classifiers . COLING .",reference
S1GVQk5gG_80,"Pagliardini , M. , Gupta , P. , & Jaggi , M. ( 2018 ) . Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features . arXiv preprint arXiv : 1703.02507 .",reference
S1GVQk5gG_81,"Wieting , J. , Bansal , M. , Gimpel , K. , & Livescu , K. ( 2016 ) . Charagram : Embedding words and sentences via character n-grams . EMNLP .",reference
S1GVQk5gG_82,"Wieting , J. , & Gimpel , K. ( 2017 ) . Revisiting Recurrent Networks for Paraphrastic Sentence Embeddings . ACL .",reference
BympCwwgf_0,"This paper presents a method to cope with adversarial examples in classification tasks , leveraging a generative model of the inputs .",fact
BympCwwgf_1,"Given an accurate generative model of the input , this approach first projects the input onto the manifold learned by the generative model",fact
BympCwwgf_2,( the idea being that inputs on this manifold reflect the non-adversarial input distribution ) .,fact
BympCwwgf_3,This projected input is then used to produce the classification probabilities .,fact
BympCwwgf_4,The authors test their method on various adversarially constructed inputs ( with varying degrees of noise ) .,fact
BympCwwgf_5,Questions/Comments : - I am interested in unpacking the improvement of Defense-GAN over the MagNet auto-encoder based method .,evaluation
BympCwwgf_6,Is the MagNet auto-encoder suffering lower accuracy because the projection of an adversarial image is based on an encoding function that is learned only on true data ?,non-arg
BympCwwgf_7,"If the decoder from the MagNet approach were treated purely as a generative model , and the same optimization-based projection approach ( proposed in this work ) was followed , would the results be comparable ?",non-arg
BympCwwgf_8,"- Is there anything special about the GAN approach , versus other generative approaches ?",non-arg
BympCwwgf_9,"- In the black-box vs. white-box scenarios , can the attacker know the GAN parameters ?",non-arg
BympCwwgf_10,"Is that what is meant by the "" defense network "" ( in experiments bullet 2 ) ?",non-arg
BympCwwgf_11,- How computationally expensive is this approach take compared to MagNet or other adversarial approaches ?,non-arg
BympCwwgf_12,Quality : The method appears to be technically correct .,evaluation
BympCwwgf_13,Clarity : This paper clearly written ;,evaluation
BympCwwgf_14,both method and experiments are presented well .,evaluation
BympCwwgf_15,Originality : I am not familiar enough with adversarial learning to assess the novelty of this approach .,non-arg
BympCwwgf_16,Significance : I believe the main contribution of this method is the optimization-based approach to project onto a generative model 's manifold .,evaluation
BympCwwgf_17,"I think this kernel has the potential to be explored further ( e.g. computational speed-up , projection metrics ) .",evaluation
HJecicqxG_0,"In conventional boosting methods , one puts a weight on each sample .",fact
HJecicqxG_1,The wrongly classified samples get large weights such that in the next round those samples will be more likely to get right .,fact
HJecicqxG_2,Thus the learned weak learner at this round will make different mistakes .,fact
HJecicqxG_3,This idea however is difficult to be applied to deep learning with a large amount of data .,evaluation
HJecicqxG_4,This paper instead designed a new boosting method which puts large weights on the category with large error in this round .,fact
HJecicqxG_5,In other words samples in the same category will have the same weight,fact
HJecicqxG_6,Error bound is derived .,fact
HJecicqxG_7,Experiments show its usefulness,fact
HJecicqxG_8,though experiments are limited,evaluation
rJOVWxjez_0,"The authors describe a new defense mechanism against adversarial attacks on classifiers ( e.g. , FGSM ) .",fact
rJOVWxjez_1,"They propose utilizing Generative Adversarial Networks ( GAN ) ,",fact
rJOVWxjez_2,"which are usually used for training generative models for an unknown distribution ,",fact
rJOVWxjez_3,but have a natural adversarial interpretation .,evaluation
rJOVWxjez_4,"In particular , a GAN consists of a generator NN G which maps a random vector z to an example x , and a discriminator NN D which seeks to discriminate between an examples produced by G and examples drawn from the true distribution .",fact
rJOVWxjez_5,"The GAN is trained to minimize the max min loss of D on this discrimination task , thereby producing a G ( in the limit ) whose outputs are indistinguishable from the true distribution by the best discriminator .",fact
rJOVWxjez_6,"Utilizing a trained GAN , the authors propose the following defense at inference time .",fact
rJOVWxjez_7,"Given a sample x ( which has been adversarially perturbed ) , first project x onto the range of G by solving the minimization problem <EQN> .",fact
rJOVWxjez_8,This is done by SGD .,fact
rJOVWxjez_9,Then apply any classifier trained on the true distribution on the resulting <EQN> .,fact
rJOVWxjez_10,"In the case of existing black-box attacks , the authors argue ( convincingly ) that the method is both flexible and empirically effective .",fact
rJOVWxjez_11,"In particular , the defense can be applied in conjunction with any classifier ( including already hardened classifiers ) , and does not assume any specific attack model .",fact
rJOVWxjez_12,"Nevertheless , it appears to be effective against FGSM attacks , and competitive with adversarial training specifically to defend against FGSM .",fact
rJOVWxjez_13,The authors provide less-convincing evidence that the defense is effective against white-box attacks .,evaluation
rJOVWxjez_14,"In particular , the method is shown to be robust against FGSM , RAND+FGSM , and CW white-box attacks .",fact
rJOVWxjez_15,"However , it is not clear to me that the method is invulnerable to novel white-box attacks .",evaluation
rJOVWxjez_16,"In particular , it seems that the attacker can design an x which projects onto some desired <VAR> ( using some other method entirely ) , which then fools the classifier downstream .",fact
rJOVWxjez_17,"Nevertheless , the method is shown to be an effective tool for hardening any classifier against existing black-box attacks",fact
rJOVWxjez_18,( which is arguably of great practical value ) .,evaluation
rJOVWxjez_19,It is novel and should generate further research with respect to understanding its vulnerabilities more completely .,evaluation
rJOVWxjez_20,"Minor Comments : The sentence starting “ Unless otherwise specified … ” at the top of page 7 is confusing given the actual contents of Tables 1 and 2 , which are clarified only by looking at Table 5 in the appendix .",evaluation
rJOVWxjez_21,This should be fixed .,request
B1B3e0Oef_0,This work introduces a particular parametrization of a stochastic policy ( a uniform mixture of deterministic policies ) .,fact
B1B3e0Oef_1,"They find this parametrization , when trained with stochastic value gradient outperforms DDPG on several OpenAI gym benchmarks .",fact
B1B3e0Oef_2,This paper unfortunately misses many significant pieces of prior work training stochastic policies .,evaluation
B1B3e0Oef_3,The most relevant is [ 1 ] which should definitely be cited .,request
B1B3e0Oef_4,The algorithm here can be seen as SVG ( 0 ) with a particular parametrization of the policy .,fact
B1B3e0Oef_5,"However , numerous other works have examined stochastic policies including [ 2 ] ( A3C which also used the Torcs environment ) and [ 3 ] .",fact
B1B3e0Oef_6,"The wide use of stochastic policies in prior work makes the introductory explanation of the potential benefits for stochastic policies distracting ,",evaluation
B1B3e0Oef_7,instead the focus should be on the particular choice and benefits of the particular stochastic parametrization chosen here and the choice of stochastic value gradient as a training method ( as opposed to many on-policy methods ) .,request
B1B3e0Oef_8,"The empirical comparison is also hampered by only comparing with DDPG ,",evaluation
B1B3e0Oef_9,there are numerous stochastic policy algorithms that have been compared on these environments .,fact
B1B3e0Oef_10,"Additionally , the DDPG performance here is lower for several environments than the results reported in Henderson et al. 2017 ( cited in the paper , table 2 here , table 3 Henderson )",fact
B1B3e0Oef_11,which should be explained .,request
B1B3e0Oef_12,"While this particular parametrization may provide some benefits , the lack of engagement with relevant prior work and other stochastic baselines significant limits the impact of this work and makes assessing its significance difficult .",evaluation
B1B3e0Oef_13,This work would benefit from careful copyediting .,request
B1B3e0Oef_14,[1] <CIT>,reference
B1B3e0Oef_15,[2] <CIT>,reference
B1B3e0Oef_16,[3] <CIT>,reference
rkejdYtxz_0,Summary : This work is about model evaluation for molecule generation and design .,fact
rkejdYtxz_1,"19 benchmarks are proposed , small data sets are expanded to a large , standardized data set",fact
rkejdYtxz_2,and it is explored how to apply new RL techniques effectively for molecular design .,fact
rkejdYtxz_3,"on the positive side : The paper is well written , quality and clarity of the work are good .",evaluation
rkejdYtxz_4,The work provides a good overview about how to apply new reinforcement learning techniques for sequence generation .,evaluation
rkejdYtxz_5,"It is investigated how several RL strategies perform on a large , standardized data set .",fact
rkejdYtxz_6,"Different RL models like Hillclimb-MLE , PPO , GAN , A2C are investigated and discussed .",fact
rkejdYtxz_7,An implementation of 19 suggested benchmarks of relevance for de novo design will be provided as open source as an OpenAI Gym .,fact
rkejdYtxz_8,on the negative side : There is no new novel contribution on the methods side .,evaluation
rkejdYtxz_9,minor comments : Section 2.1 . see Fig. 2 — > see Fig. 1,request
rkejdYtxz_10,page 4just before equation 8 : the the,request
r1Kg9atxz_0,"The authors extend the approach proposed in the "" Reverse Curriculum Learning for Reinforcement Learning "" paper by adding a discriminator that gives a bonus reward to a state based on how likely it thinks the current policy is to reach the goal from said state .",fact
r1Kg9atxz_1,The discriminator is a potentially interesting mechanism to approximate multi-step backups in sparse-reward environments .,evaluation
r1Kg9atxz_2,"The approach of this paper seems severely severely limited by the assumptions made by the authors , mainly assuming a deterministic environment , known goal states and the ability to sample anywhere in the state space .",evaluation
r1Kg9atxz_3,"Some of these assumptions may be reasonable in domains such as robotics ,",evaluation
r1Kg9atxz_4,but they seem very restrictive in the domains like the games considered in the paper .,evaluation
r1Kg9atxz_5,"Additional Comments : - The authors demonstrate some benefits of using Tendency rewards ,",fact
r1Kg9atxz_6,but made little attempt to explain why it leads to accelerated learning .,evaluation
r1Kg9atxz_7,Results are pure performance results .,fact
r1Kg9atxz_8,- The authors should probably structure the tendency reward as potential based instead of using the Gaussian kernel hack they introduce in section 4.2,request
r1Kg9atxz_9,- Presentation : There are several mistakes and formatting issues in References,fact
r1Kg9atxz_10,- Assumption 2 transformations - > transitions ?,request
r1Kg9atxz_11,- Need to add assumption 3 : advance knowledge of goal state,request
r1Kg9atxz_12,"- the use of gamma as a scale factor in equation 2 is confusion ,",evaluation
r1Kg9atxz_13,it was already introduced as the discount factor ( which is default notation in RL ) .,fact
r1Kg9atxz_14,It also is n't clear what the notation r_f denotes ( is it the same as r ^ f in appendix ? ) .,evaluation
r1Kg9atxz_15,- It is nice to see that the authors compare their method with alternative approaches .,evaluation
r1Kg9atxz_16,"Unfortunately , the proposed method does not seem to offer many benefits .",evaluation
H18ZJWAgG_0,Summary The paper is well-written,evaluation
H18ZJWAgG_1,but does not make deep technical contributions and does not present a comprehensive evaluation or highly insightful empirical results .,evaluation
H18ZJWAgG_2,"Abstract / Intro I get the entire focus of the paper is some variant of Pac-Man which has received attention in the RL literature for Atari games ,",evaluation
H18ZJWAgG_3,"but for the most part the impressive advances of previous Atari/RL papers are in the setting that the raw video is provided as input ,",fact
H18ZJWAgG_4,which is much different than solving the underlying clean mathematically abstracted problem ( as a grid world with obstacles ) as done here and evident in the videos .,evaluation
H18ZJWAgG_5,Further it is honestly hard for me to be strongly motivated about a paper that focuses on the need to decompose Pac-man into sub-agents/advisor value functions .,evaluation
H18ZJWAgG_6,Section 2 Another historically well-cited paper for MDP decomposition :,evaluation
H18ZJWAgG_7,<CIT>,reference
H18ZJWAgG_8,Section 3 Is the additive reward decomposition a required part of the problem specification ?,request
H18ZJWAgG_9,"It seems so , i.e. , there is no obvious method for automatically decomposing a monolithic reward function over advisors .",evaluation
H18ZJWAgG_10,"Section 4 * Egocentric : Definition 1 : Sure , the problem will have local optima ( attractors ) when decomposed suboptimally",fact
H18ZJWAgG_11,-- I 'm not sure what new insight we 've gained from this analysis ...,evaluation
H18ZJWAgG_12,it is a general problem with any function approximation scheme that does not guarantee that the rank ordering of actions for a state is preserved .,fact
H18ZJWAgG_13,"* Agnostic Other than approximating some type of myopic rollout , I really do n't see why this approach would be reasonable ?",evaluation
H18ZJWAgG_14,I am surprised it works at all,evaluation
H18ZJWAgG_15,though my guess is that this could simply be an artifact of evaluating on a single domain with a specific structure .,evaluation
H18ZJWAgG_16,* Empathic This appears to be the key contribution,evaluation
H18ZJWAgG_17,though related work certainly infringes on its novelty .,evaluation
H18ZJWAgG_18,Is this paper then an empirical evaluation of previous methods in a single Pac-man grid world variant ?,evaluation
H18ZJWAgG_19,I wonder if the theory of DEC-MDPs would have any relevance for novel analysis here ?,request
H18ZJWAgG_20,Section 5 I 'm disappointed that the authors only evaluate on a single domain ;,evaluation
H18ZJWAgG_21,presumably the empathic approach has applications beyond Pac-Man ?,evaluation
H18ZJWAgG_22,The fact that empathic generally performs better is not at all surprising .,evaluation
H18ZJWAgG_23,"The fact that a modified discount factor for egocentric can also perform well is not surprising given that lower discount factors have often been shown to improve approximated MDP solutions , e.g. ,",evaluation
H18ZJWAgG_24,<CIT>,reference
H18ZJWAgG_25,"*** Side note : The following part is somewhat orthogonal to the review above in that I would not expect the authors to address this on revision , * but * at the same time I think it provides a connection to the special case of concurrent action decomposition into advisors ,",non-arg
H18ZJWAgG_26,which could potentially provide a high impact direction of application for this work,evaluation
H18ZJWAgG_27,"( i.e. , concurrent problems are hard and show up in numerous operations research problems covering inventory control , logistics , epidemic response ) .",evaluation
H18ZJWAgG_28,"For the special case that each advisor is assigned to one action in a factored space of concurrent actions , the egocentric algorithm would be very close to the Hindsight approximation in Section 6 of this paper ( including an additive decomposition of rewards ) :",evaluation
H18ZJWAgG_29,<CIT>,reference
H18ZJWAgG_30,This simple algorithm is hard to beat,evaluation
H18ZJWAgG_31,"for the following reason that connects some details of your egocentric and empathic settings : rather than decomposing a concurrent MDP into independent problems per concurrent action , the optimization of each action ( by each advisor ) is done in sequence ( advisors are ordered ) and gets to condition on the previously selected advisor actions .",fact
H18ZJWAgG_32,So it provides an alternate paradigm where advisors actually get to see and condition their policy on what other advisors are doing .,fact
H18ZJWAgG_33,"In my own work comparing optimal concurrent solutions to this approach , I have found this approach to be near-optimal and much more efficient to solve since it exploits decomposition .",non-arg
H18ZJWAgG_34,Why is this relevant to this work ?,non-arg
H18ZJWAgG_35,Because ( a ) it suggests another variant of the advisor decomposition that at least makes sense in the case of concurrent actions ( and perhaps shared actions though this would require some extension ),fact
H18ZJWAgG_36,and ( b ) it suggests there are more options than just the full egocentric and empathic settings in this important class of concurrent action problems that are necessarily solved in practice for large action spaces by some form of decomposition .,fact
H18ZJWAgG_37,"This could be an interesting direction for future exploration of the ideas in this work , where there might be additional technical novelty and more space for empirical contributions and observations .",evaluation
Hkcb6tG-M_0,"The paper proposes a technique for quantizing the weights of a neural network , with bit-depth/precision varying on a per-parameter basis .",fact
Hkcb6tG-M_1,The main idea is to minimize the number of bits used in the quantization while constraining the loss to remain below a specified upper bound .,fact
Hkcb6tG-M_2,"This is achieved by formulating an upper bound on the number of bits used via a set of "" tolerances "" ;",fact
Hkcb6tG-M_3,this upper bound is then minimized while estimating any increase in loss using a first order Taylor approximation .,fact
Hkcb6tG-M_4,I have a number of questions and concerns about the proposed approach .,evaluation
Hkcb6tG-M_5,"First , at a high level , there are many details that are n't clear from the text .",evaluation
Hkcb6tG-M_6,"Quantization has some bookkeeping associated with it : In a per-parameter quantization setup it will be necessary to store not just the quantized parameter , but also the number of bits used in the quantization ( takes e.g. 4-5 extra bits ) , and there will be some metadata necessary to encode how the quantized value should be converted back to floating point ( e.g. , for 8-bit quantization of a layer of weights , usually the min and max are stored ) .",fact
Hkcb6tG-M_7,"From Algorithm 1 it appears the quantization assumes parameters in the range [ 0 , 1 ] .",fact
Hkcb6tG-M_8,Do n't negative values require another bit ?,fact
Hkcb6tG-M_9,What happens to values larger than 1 ?,request
Hkcb6tG-M_10,How are even bit depths and associated asymmetries w.r.t. 0 handled,request
Hkcb6tG-M_11,"( e.g. , three bits can represent -1 , 0 , and 1 , but 4 must choose to either not represent 0 or drop e.g. -1 ) ?",fact
Hkcb6tG-M_12,"None of these details are clearly discussed in the paper ,",evaluation
Hkcb6tG-M_13,and it 's not at all clear that the estimates of compression are correct if these bookkeeping matters are n't taken into account properly .,evaluation
Hkcb6tG-M_14,Additionally the paper implies that this style of quantization has benefits for compute in addition to memory savings .,fact
Hkcb6tG-M_15,"This is highly dubious ,",evaluation
Hkcb6tG-M_16,"since the method will require converting all parameters to a standard bit-depth on the fly ( probably back to floating point , since some parameters may have been quantized with bit depth up to 32 ) .",fact
Hkcb6tG-M_17,Alternatively custom GEMM/conv routines would be required which are impossible to make efficient for weights with varying bit depths .,fact
Hkcb6tG-M_18,So there are likely not runtime compute or memory savings from such an approach .,fact
Hkcb6tG-M_19,I have a few other specific questions : Are the gradients used to compute \ mu computed on the whole dataset or minibatches ?,request
Hkcb6tG-M_20,How would this scale to larger datasets ?,request
Hkcb6tG-M_21,I am confused by the equality in Equation 8 : What happens for values shared by many different quantization bit depths,request
Hkcb6tG-M_22,"( e.g. , representing 0 presumably requires 1 bit , but may be associated with a much finer tolerance ) ?",fact
Hkcb6tG-M_23,"Should "" minimization in equation 4 "" refer to equation 3 ?",request
Hkcb6tG-M_24,"In the end , while do like the general idea of utilizing the gradient to identify how sensitive the model might be to quantization of various parameters ,",evaluation
Hkcb6tG-M_25,"there are significant clarity issues in the paper ,",evaluation
Hkcb6tG-M_26,"I am a bit uneasy about some of the compression results claimed without clearer description of the bookkeeping ,",evaluation
Hkcb6tG-M_27,and I do n't believe an approach of this kind has any significant practical relevance for saving runtime memory or compute resources .,evaluation
SkDHZUXlG_0,"The authors train an RNN to perform deduced reckoning ( ded reckoning ) for spatial navigation ,",fact
SkDHZUXlG_1,and then study the responses of the model neurons in the RNN .,fact
SkDHZUXlG_2,"They find many properties reminiscent of neurons in the mammalian entorhinal cortex ( EC ) : grid cells , border cells , etc .",fact
SkDHZUXlG_3,"When regularization of the network is not used during training , the trained RNNs no longer resemble the EC .",fact
SkDHZUXlG_4,"This suggests that those constraints ( lower overall connectivity strengths , and lower metabolic costs ) might play a role in the EC 's navigation function .",evaluation
SkDHZUXlG_5,The paper is overall quite interesting and the study is pretty thorough :,evaluation
SkDHZUXlG_6,no major cons come to mind .,non-arg
SkDHZUXlG_7,Some suggestions / criticisms are given below .,non-arg
SkDHZUXlG_8,1 ) The findings seem conceptually similar to the older sparse coding ideas from the visual cortex .,evaluation
SkDHZUXlG_9,That connection might be worth discussing,request
SkDHZUXlG_10,"because removing the regularizing ( i.e. , metabolic cost ) constraint from your RNNS makes them learn representations that differ from the ones seen in EC .",fact
SkDHZUXlG_11,The sparse coding models see something similar :,evaluation
SkDHZUXlG_12,"without sparsity constraints , the image representations do not resemble those seen in V1 ,",fact
SkDHZUXlG_13,"but with sparsity , the learned representations match V1 quite well .",evaluation
SkDHZUXlG_14,"That the same observation is made in such disparate brain areas ( V1 , EC ) suggests that sparsity / efficiency might be quite universal constraints on the neural code .",evaluation
SkDHZUXlG_15,2 ) The finding that regularizing the RNN makes it more closely match the neural code is also foreshadowed somewhat by the 2015 Nature Neuro paper by Susillo et al .,evaluation
SkDHZUXlG_16,That could be worthy of some ( brief ) discussion .,request
SkDHZUXlG_17,"Sussillo , D. , Churchland , M. M. , Kaufman , M. T. , & Shenoy , K. V. ( 2015 ) . A neural network that finds a naturalistic solution for the production of muscle activity . Nature neuroscience , 18 ( 7 ) , 1025-1033 .",reference
SkDHZUXlG_18,3 ) Why the different initializations for the recurrent weights for the hexagonal vs other environments ?,non-arg
SkDHZUXlG_19,"I 'm guessing it 's because the RNNs do n't "" work "" in all environments with the same initialization ( i.e. , they either do n't look like EC , or they do n't obtain small errors in the navigation task ) .",evaluation
SkDHZUXlG_20,That seems important to explain more thoroughly than is done in the current text .,request
SkDHZUXlG_21,4 ) What happens with ongoing training ?,non-arg
SkDHZUXlG_22,Animals presumably continue to learn throughout their lives .,evaluation
SkDHZUXlG_23,"With on-going ( continous ) training , do the RNN neurons ' spatial tuning remain stable , or do they continue to "" drift "" ( so that border cells turn into grid cells turn into irregular cells , or some such ) ?",non-arg
SkDHZUXlG_24,"That result could make some predictions for experiment ,",evaluation
SkDHZUXlG_25,that would be testable with chronic methods ( like Ca2 + imaging ) that can record from the same neurons over multiple experimental sessions .,evaluation
SkDHZUXlG_26,"5 ) It would be nice to more quantitatively map out the relation between speed tuning , direction tuning , and spatial tuning ( illustrated in Fig. 3 ) .",request
SkDHZUXlG_27,"Specifically , I would quantify the cells ' direction tuning using the circular variance methods that people use for studying retinal direction selective neurons .",request
SkDHZUXlG_28,And I would quantify speed tuning via something like the slope of the firing rate vs speed curves .,request
SkDHZUXlG_29,And quantify spatial tuning somehow ( a natural method would be to use the sparsity measures sometimes applied to neural data to quantify how selective the spatial profile is to one or a few specific locations ) .,request
SkDHZUXlG_30,Then make scatter plots of these quantities against each other .,request
SkDHZUXlG_31,"Basically , I 'd love to see the trends for how these types of tuning relate to each other over the whole populations :",request
SkDHZUXlG_32,those trends could then be tested against experimental data ( possibly in a future study ) .,evaluation
BJBWMqqlf_0,This paper proposes a new theoretically-motivated method for combining reinforcement learning and imitation learning for acquiring policies that are as good as or superior to the expert .,fact
BJBWMqqlf_1,The method assumes access to an expert value function ( which could be trained using expert roll-outs ) and uses the value function to shape the reward function and allow for truncated-horizon policy search .,fact
BJBWMqqlf_2,"The algorithm can gracefully handle suboptimal demonstrations/value functions ,",evaluation
BJBWMqqlf_3,"since the demonstrations are only used for reward shaping ,",fact
BJBWMqqlf_4,and the experiments demonstrate faster convergence and better performance compared to RL and AggreVaTeD on a range of simulated control domains .,evaluation
BJBWMqqlf_5,The paper is well-written and easy to understand .,evaluation
BJBWMqqlf_6,My main feedback is with regard to the experiments : I appreciate that the experiments used 25 random seeds !,evaluation
BJBWMqqlf_7,This provides a convincing evaluation .,evaluation
BJBWMqqlf_8,"It would be nice to see experimental results on even higher dimensional domains such as the ant , humanoid , or vision-based tasks ,",request
BJBWMqqlf_9,since the experiments seem to suggest that the benefit of the proposed method is diminished in the swimmer and hopper domains compared to the simpler settings .,evaluation
BJBWMqqlf_10,"Since the method uses demonstrations ,",fact
BJBWMqqlf_11,"it would be nice to see three additional comparisons : ( a ) training with supervised learning on the expert roll-outs , ( b ) initializing THOR and AggreVaTeD ( k = 1 ) with a policy trained with supervised learning , and ( c ) initializing TRPO with a policy trained with supervised learning .",request
BJBWMqqlf_12,"There does n't seem to be any reason not to initialize in such a way , when expert demonstrations are available ,",evaluation
BJBWMqqlf_13,and such an initialization should likely provide a significant speed boost in training for all methods .,evaluation
BJBWMqqlf_14,How many demonstrations were used for training the value function in each domain ?,request
BJBWMqqlf_15,I did not see this information in the paper .,fact
BJBWMqqlf_16,"With regard to the method and discussion : The paper discusses the connection between the proposed method and short-horizon imitation and long-horizon RL , describing the method as a midway point .",fact
BJBWMqqlf_17,"It would also be interesting to see a discussion of the relation to inverse RL ,",request
BJBWMqqlf_18,which considers long-term outcomes from expert demonstrations .,fact
BJBWMqqlf_19,"For example , MacGlashn & Littman propose a midway point between imitation and inverse RL [ 1 ] .",fact
BJBWMqqlf_20,"Theoretically , would it make sense to anneal k from small to large ? ( to learn the most effectively from the smallest amount of experience )",non-arg
BJBWMqqlf_21,[ 1 ] <URL>,reference
BJBWMqqlf_22,"Minor feedback : - The RHS of the first inequality in the proof of Thm 3.3 seems to have an error in the indexing of i and exponent , which differs from the line before and line after",fact
rycZrCJef_0,"Authors of this paper derived an efficient quantum-inspired learning algorithm based on a hierarchical representation that is known as tree tensor network , which is inspired by the multipartite entanglement renormalization ansatz approach where the tensors in the TN are kept to be unitary during training .",fact
rycZrCJef_1,Some observations are : The limitation of learnability of TTN strongly depends on the physical indexes and the geometrical indexes determine how well the TTNs approximate the limit ;,fact
rycZrCJef_2,TTNs exhibit same increase level of abstractions as CNN or DBN ;,fact
rycZrCJef_3,Fidelity and entanglement entropy can be considered as some measurements of the network .,evaluation
rycZrCJef_4,"Authors introduced the two-dimensional hierarchical tensor networks for solving image recognition problems ,",fact
rycZrCJef_5,which suits more the 2-D nature of images .,evaluation
rycZrCJef_6,"In section 2 , authors stated that the choice of feature function is arbitrary ,",fact
rycZrCJef_7,and a specific feature map was introduced in Section 4 .,fact
rycZrCJef_8,"However , it is not straightforward to connect ( 10 ) to ( 1 ) or ( 2 ) .",evaluation
rycZrCJef_9,It is better to clarify this connection,request
rycZrCJef_10,because some important parameters such as the virtual bond and input bond are related to the complexity of the proposed algorithm as well as the limitation of learnability .,evaluation
rycZrCJef_11,"For example , the scaling of the complexity <VAR> is not easy to understand .",evaluation
rycZrCJef_12,Is it related to specific feature map ?,request
rycZrCJef_13,How about the complexity of eigen-decomposition for one tensor at each iterates .,request
rycZrCJef_14,"And also , whether the tricks used to accelerate the computations will affect the convergence of the algorithm ?",request
rycZrCJef_15,More details on these problems are required for readers ’ better understanding .,request
rycZrCJef_16,"From Fig 2 , it is difficult to see the relationship between learnability and parameters such input bond and virtual bond",evaluation
rycZrCJef_17,because it seems there are no clear trends in the Fig 2 ( a ) and ( b ) to make any conclusion .,evaluation
rycZrCJef_18,It is better to clarify these relationships with either clear explanation or better examples .,request
rycZrCJef_19,"From Fig 3 , authors claimed that TN obtained the same levels of abstractions as in deep learning .",fact
rycZrCJef_20,"However , from Fig 3 only , it is hard to make this conclusion .",evaluation
rycZrCJef_21,"First , there are not too many differences from Fig 3 ( a ) to Fig 3 ( e ) .",evaluation
rycZrCJef_22,"Second , there is no visualization result reported from deep learning on the same data for comparison .",fact
rycZrCJef_23,"Hence , it is not convincing to draw this conclusion only from Fig 3 .",evaluation
rycZrCJef_24,"In Section 4.2 , what strategy is used to obtain these parameters in Table 1 ?",non-arg
rycZrCJef_25,"In Section 5 , it is interesting to see more experiments in terms of fidelity and entanglement entropy .",request
SymYit2xf_0,The paper shows that several recently proposed interpretation techniques for neural network are performing similar processing and yield similar results .,fact
SymYit2xf_1,"The authors show that these techniques can all be seen as a product of input activations and a modified gradient , where the local derivative of the activation function at each neuron is replaced by some fixed function .",fact
SymYit2xf_2,A second part of the paper looks at whether explanations are global or local .,fact
SymYit2xf_3,"The authors propose a metric called sensitivity-n for that purpose ,",fact
SymYit2xf_4,and make some observations about the optimality of some interpretation techniques with respect to this metric in the linear case .,fact
SymYit2xf_5,The behavior of each explanation w.r.t. these properties is then tested on multiple DNN models tested on real-world datasets .,fact
SymYit2xf_6,Results further outline the resemblance between the compared methods .,fact
SymYit2xf_7,"In the appendix , the last step of the proof below Eq . 7 is unclear .",evaluation
SymYit2xf_8,"As far as I can see , the variable g_i ^ LRP was n’t defined ,",evaluation
SymYit2xf_9,and the use of Eq . 5 to achieve this last could be better explained .,request
SymYit2xf_10,"There also seems to be some issues with the ordering i , j , where these indices alternatively describe the lower/higher layers , or the higher/lower layers .",evaluation
rJ6Z7prxf_0,"This paper introdues NoisyNets , that are neural networks whose parameters are perturbed by a parametric noise function , and they apply them to 3 state-of-the-art deep reinforcement learning algorithms : DQN , Dueling networks and A3C .",fact
rJ6Z7prxf_1,"They obtain a substantial performance improvement over the baseline algorithms , without explaining clearly why .",evaluation
rJ6Z7prxf_2,"The general concept is nice ,",evaluation
rJ6Z7prxf_3,the paper is well written,evaluation
rJ6Z7prxf_4,"and the experiments are convincing ,",evaluation
rJ6Z7prxf_5,"so to me this paper should be accepted , despite a weak analysis .",evaluation
rJ6Z7prxf_6,Below are my comments for the authors . ---------------------------------,non-arg
rJ6Z7prxf_7,"General , conceptual comments : The second paragraph of the intro is rather nice ,",evaluation
rJ6Z7prxf_8,but it might be updated with recent work about exploration in RL .,request
rJ6Z7prxf_9,"Note that more than 30 papers are submitted to ICLR 2018 mentionning this topic ,",fact
rJ6Z7prxf_10,"and many things have happened since this paper was posted on arxiv ( see the "" official comments "" too ) .",fact
rJ6Z7prxf_11,"p2 : "" our NoisyNet approach requires only one extra parameter per weight """,quote
rJ6Z7prxf_12,"Parameters in a NN are mostly weights and biases ,",fact
rJ6Z7prxf_13,"so from this sentence one may understand that you close-to-double the number of parameters , which is not so few !",fact
rJ6Z7prxf_14,"If this is not what you mean , you should reformulate ...",request
rJ6Z7prxf_15,"p2 : "" Though these methods often rely on a non-trainable noise of vanishing size as opposed to NoisyNet which tunes the parameter of noise by gradient descent . """,quote
rJ6Z7prxf_16,Two ideas seem to be collapsed here :,fact
rJ6Z7prxf_17,"the idea of diminishing noise over an experiment , exploring first and exploiting later ,",fact
rJ6Z7prxf_18,and the idea of adapting the amount of noise to a specific problem .,fact
rJ6Z7prxf_19,It should be made clearer whether NoisyNet can address both issues and whether other algorithms do so too ...,request
rJ6Z7prxf_20,"In particular , an algorithm may adapt noise along an experiment or from an experiment to the next .",fact
rJ6Z7prxf_21,"From Fig. 3 , one can see that having the same initial noise in all environments is not a good idea ,",evaluation
rJ6Z7prxf_22,so the second mechanism may help much .,evaluation
rJ6Z7prxf_23,"BTW , the short section in Appendix B about initialization of noisy networks should be moved into the main text .",request
rJ6Z7prxf_24,p4 : the presentation of NoisyNets is not so easy to follow and could be clarified in several respects :,request
rJ6Z7prxf_25,"- a picture could be given to better explain the structure of parameters , particularly in the case of factorised ( factorized , factored ? ) Gaussian noise .",request
rJ6Z7prxf_26,"- I would start with the paragraph "" Considering a linear layer [ ... ] below ) "" and only after this I would introduce \ theta and \ xi as a more synthetic notation .",request
rJ6Z7prxf_27,"Later in the paper , you then have to state "" ... are now noted \ xi "" several times , which I found rather clumsy .",evaluation
rJ6Z7prxf_28,p5 : Why do you use option ( b ) for DQN and Dueling and option ( a ) for A3C ?,request
rJ6Z7prxf_29,The reason why ( if any ) should be made clear from the clearer presentation required above .,request
rJ6Z7prxf_30,"By the way , a wild question : if you wanted to use NoisyNets in an actor-critic architecture like DDPG , would you put noise both in the actor and the critic ?",request
rJ6Z7prxf_31,The paragraph above Fig3 raises important questions which do not get a satisfactory answer .,evaluation
rJ6Z7prxf_32,"Why is it that , in deterministic environments , the network does not converge to a deterministic policy , which should be able to perform better ?",request
rJ6Z7prxf_33,Why is it that the adequate level of noise changes depending on the environment ?,request
rJ6Z7prxf_34,"By the way , are we sure that the curves of Fig3 correspond to some progress in noise tuning ( that is , is the level of noise really "" better "" through time with these curves , or they they show something poorly correlated with the true reasons of success ? ) ?",request
rJ6Z7prxf_35,"Finally , I would be glad to see the effect of your technique on algorithms like TRPO and PPO which require a stochastic policy for exploration ,",request
rJ6Z7prxf_36,and where I believe that the role of the KL divergence bound is mostly to prevent the level of stochasticity from collasping too quickly .,evaluation
rJ6Z7prxf_37,----------------------------------- Local comments : The first sentence may make the reader think you only know about 4-5 old works about exploration .,evaluation
rJ6Z7prxf_38,"Pp. 1-2 : "" the approach differs ... from variational inference . [ ... ] It also differs variational inference ... """,quote
rJ6Z7prxf_39,"If you mean it differs from variational inference in two ways , the paragraph should be reorganized .",request
rJ6Z7prxf_40,"p2 : "" At a high level our algorithm induces a randomised network for exploration , with care exploration via randomised value functions can be provably-efficient with suitable linear basis ( Osband et al. , 2014 ) """,quote
rJ6Z7prxf_41,= > I do n't understand this sentence at all .,evaluation
rJ6Z7prxf_42,"At the top of p3 , you may update your list with PPO and ACKTR , which are now "" classical "" baselines too .",request
rJ6Z7prxf_43,"Appendices A1 and A2 are a lot redundant with the main text ( some sentences and equations are just copy-pasted ) ,",evaluation
rJ6Z7prxf_44,this should be improved .,request
rJ6Z7prxf_45,The best would be to need to reject nothing to the Appendix .,evaluation
rJ6Z7prxf_46,"--------------------------------------- Typos , language issues : p2 the idea ... the optimization process have been = > has",request
rJ6Z7prxf_47,p2 Though these methods often rely on a non-trainable noise of vanishing size as opposed to NoisyNet which tunes the parameter of noise by gradient descent .,quote
rJ6Z7prxf_48,= > you should make a sentence ...,request
rJ6Z7prxf_49,p3 the the double-DQN,quote
rJ6Z7prxf_50,"several times , an equation is cut over two lines , a line finishing with "" = "" ,",fact
rJ6Z7prxf_51,which is inelegant,evaluation
rJ6Z7prxf_52,You should deal better with appendices :,request
rJ6Z7prxf_53,"Every "" Sec . Ax/By/Cz "" should be replaced by "" Appendix Ax/By/Cz "" .",request
rJ6Z7prxf_54,"Besides , the big table and the list of performances figures should themselves be put in two additional appendices",request
rJ6Z7prxf_55,"and you should refer to them as Appendix D or E rather than "" the Appendix "" .",request
SkOj779lM_0,This paper proposes the concept of optimal representation space and suggests that a model should be evaluated in its optimal representation space to get good performance .,fact
SkOj779lM_1,"It could be a good idea if this paper could suggest some ways to find the optimal representation space in general , instead of just showing two cases .",request
SkOj779lM_2,"It is disappointing , because this paper is named as "" finding optimal representation spaces ... "" .",evaluation
SkOj779lM_3,"In addition , one of the contributions claimed in this paper is about introducing the "" formalism "" of an optimal representation space .",fact
SkOj779lM_4,"However , I did n't see any formal definition of this concept or theoretical justification .",fact
SkOj779lM_5,"About FastSent or any other log-linear model , the reason that dot product ( or cosine similarity ) is a good metric is because the model is trained to optimize the dot product , as shown in equation 5",fact
SkOj779lM_6,--- I think this simple fact is missed in this paper .,fact
SkOj779lM_7,"The experimental results are not convincing ,",evaluation
SkOj779lM_8,because I did n't find any consistent pattern that shows the performance is getting better once we evaluated the model in its optimal representation space .,fact
SkOj779lM_9,There are statements in this paper that I did n't agree with,evaluation
SkOj779lM_10,1 ) Distributional hypothesis from Harris ( 1954 ) is about words not sentences .,fact
SkOj779lM_11,2 ) Not sure the following line makes sense :,evaluation
SkOj779lM_12," However , these unsupervised tasks are more interesting from a general AI point of view , as they test whether the machine truly understands the human notion of similarity , without being explicitly told what is similar ",quote
B1ja8-9lf_0,This paper presents a novel approach to calibrate classifiers for out of distribution samples .,fact
B1ja8-9lf_1,"In additional to the original cross entropy loss , the “ confidence loss ” was proposed to guarantee the out of distribution points have low confidence in the classifier .",fact
B1ja8-9lf_2,"As out of distribution samples are hard to obtain ,",evaluation
B1ja8-9lf_3,authors also propose to use GAN generating “ boundary ” samples as out of distribution samples .,fact
B1ja8-9lf_4,The problem setting is new and objective ( 1 ) is interesting and reasonable .,evaluation
B1ja8-9lf_5,"However , I am not very convinced that objective ( 3 ) will generate boundary samples .",evaluation
B1ja8-9lf_6,Suppose that theta is set appropriately so that <VAR> gives a uniform distribution over labels for out of distribution samples .,fact
B1ja8-9lf_7,"Because of the construction of <VAR> , which uniformly assign labels to generated out of distribution samples ,",fact
B1ja8-9lf_8,the conditional probability <VAR> should always be uniform so <VAR> divided by <VAR> is almost always 1 .,fact
B1ja8-9lf_9,The KL divergence in ( a ) of ( 3 ) should always be approximately 0 no matter what samples are generated .,fact
B1ja8-9lf_10,I also have a few other concerns : 1 . There seems to be a related work :,fact
B1ja8-9lf_11,"[1] <CIT> ,",reference
B1ja8-9lf_12,"Where authors constructed a classifier , which output <VAR> labels and the <VAR> label is the “ background noise ” label for this classification problem .",fact
B1ja8-9lf_13,Is the method in [ 1 ] applicable to this paper ’s setting ?,request
B1ja8-9lf_14,"Moreover , [ 1 ] did not seem to generate any out of distribution samples .",evaluation
B1ja8-9lf_15,2 . I am not so sure that how the actual out of distribution detection was done,evaluation
B1ja8-9lf_16,( did I miss something here ? ) .,non-arg
B1ja8-9lf_17,"Authors repeatedly mentioned “ maximum prediction values ” ,",fact
B1ja8-9lf_18,but it was not defined throughout the paper .,fact
B1ja8-9lf_19,"Algorithm 1 . is called “ minimization for detection and generating out of distribution ( samples ) ” ,",fact
B1ja8-9lf_20,"but this is only gradient descent , right ?",fact
B1ja8-9lf_21,I do not see a detection procedure .,fact
B1ja8-9lf_22,"Given the title also contains “ detecting ” , I feel authors should write explicitly how the detection is done in the main body .",request
Hyu5lW5xf_0,"This paper proposes a method , Dual-AC , for optimizing the actor ( policy ) and critic ( value function ) simultaneously which takes the form of a zero-sum game resulting in a principled method for using the critic to optimize the actor .",fact
Hyu5lW5xf_1,"In order to achieve that , they take the linear programming approach of solving the bellman optimality equations , outline the deficiencies of this approach , and propose solutions to mitigate those problems .",fact
Hyu5lW5xf_2,The discussion on the deficiencies of the naive LP approach is mostly well done .,evaluation
Hyu5lW5xf_3,Their main contribution is extending the single step LP formulation to a multi-step dual form that reduces the bias and makes the connection between policy and value function optimization much clearer without loosing convexity by applying a regularization .,fact
Hyu5lW5xf_4,They perform an empirical study in the Inverted Double Pendulum domain to conclude that their extended algorithm outperforms the naive linear programming approach without the improvements .,fact
Hyu5lW5xf_5,"Lastly , there are empirical experiments done to conclude the superior performance of Dual-AC in contrast to other actor-critic algorithms .",fact
Hyu5lW5xf_6,"Overall , this paper could be a significant algorithmic contribution , with the caveat for some clarifications on the theory and experiments .",evaluation
Hyu5lW5xf_7,"Given these clarifications in an author response , I would be willing to increase the score .",evaluation
Hyu5lW5xf_8,"For the theory , there are a few steps that need clarification and further clarification on novelty .",request
Hyu5lW5xf_9,"For novelty , it is unclear if Theorem 2 and Theorem 3 are both being stated as novel results .",evaluation
Hyu5lW5xf_10,"It looks like Theorem 2 has already been shown in "" <CIT> ” .",evaluation
Hyu5lW5xf_11,"There is a statement that “ Chen & Wang ( 2016 ) ; Wang ( 2017 ) apply stochastic first-order algorithms ( Nemirovski et al. , 2009 ) for the one-step Lagrangian of the LP problem in reinforcement learning setting . However , as we discussed in Section 3 , their algorithm is restricted to tabular parametrization ” .",quote
Hyu5lW5xf_12,Is you Theorem 2 somehow an extension ?,request
Hyu5lW5xf_13,Is Theorem 3 completely new ?,request
Hyu5lW5xf_14,This is particularly called into question due to the lack of assumptions about the function class for value functions .,evaluation
Hyu5lW5xf_15,"It seems like the value function is required to be able to represent the true value function ,",evaluation
Hyu5lW5xf_16,which can be almost as restrictive as requiring tabular parameterizations ( which can represent the true value function ) .,evaluation
Hyu5lW5xf_17,"This assumption seems to be used right at the bottom of Page 17 , where <EQN> .",evaluation
Hyu5lW5xf_18,"Further , eta_v must be chosen to ensure that it does not affect ( constrain ) the optimal solution ,",fact
Hyu5lW5xf_19,which implies it might need to be very small .,evaluation
Hyu5lW5xf_20,More about conditions on eta_v would be illuminating .,request
Hyu5lW5xf_21,There is also one step in the theorem that I can not verify .,evaluation
Hyu5lW5xf_22,"On Page 18 , how is the squared removed for difference between U and Upi ?",request
Hyu5lW5xf_23,The transition from the second line of the proof to the third line is not clear .,evaluation
Hyu5lW5xf_24,"It would also be good to more clearly state on page 14 how you get the first inequality , for <VAR> .",request
Hyu5lW5xf_25,"For the experiments , the following should be addressed .",request
Hyu5lW5xf_26,1 . It would have been better to also show the performance graphs with and without the improvements for multiple domains .,request
Hyu5lW5xf_27,2 . The central contribution is extending the single step LP to a multi-step formulation .,evaluation
Hyu5lW5xf_28,It would be beneficial to empirically demonstrate how increasing k ( the multi-step parameter ) affects the performance gains .,request
Hyu5lW5xf_29,3 . Increasing k also comes at a computational cost .,fact
Hyu5lW5xf_30,I would like to see some discussions on this and how long dual-AC takes to converge in comparison to the other algorithms tested ( PPO and TRPO ) .,request
Hyu5lW5xf_31,4 . The authors concluded the presence of local convexity based on hessian inspection due to the use of path regularization .,fact
Hyu5lW5xf_32,It was also mentioned that increasing the regularization parameter size increases the convergence rate .,fact
Hyu5lW5xf_33,"Empirically , how does changing the regularization parameter affect the performance in terms of reward maximization ?",request
Hyu5lW5xf_34,"In the experimental section of the appendix , it is mentioned that multiple regularization settings were tried but their performance is not mentioned .",fact
Hyu5lW5xf_35,"Also , for the regularization parameters that were tried , based on hessian inspection , did they all result in local convexity ?",request
Hyu5lW5xf_36,A bit more discussion on these choices would be helpful .,request
Hyu5lW5xf_37,"Minor comments : 1 . Page 2 : In equation 5 , there should not be a ' ds ' in the dual variable constraint",request
SJzxBpKeM_0,SUMMARY : This work is about learning the validity of a sequences in specific application domains like SMILES strings for chemical compounds .,fact
SJzxBpKeM_1,"In particular , the main emphasis is on predicting if a prefix sequence could possibly be extended to a complete valid sequence .",fact
SJzxBpKeM_2,"In other words , one tries to predict if there exists a valid suffix sequence , and based on these predictions , the goal is to train a generative model that always produces valid sequences .",fact
SJzxBpKeM_3,"In the proposed reinforcement learning setting , a neural network models the probability that a certain action ( adding a symbol ) will result in a valid full sequence .",fact
SJzxBpKeM_4,"For training the network , a large set of ( validity - ) labelled sequences would be needed .",fact
SJzxBpKeM_5,"To overcome this problem , the authors introduce an active learning strategy , where the information gain is re-expressed as the conditional mutual information between the the label y and the network weights w , and this mutual information is maximized in a greedy sequential manner .",fact
SJzxBpKeM_6,"EVALUATION : CLARITY & NOVELTY : In principle , the paper is easy to read .",evaluation
SJzxBpKeM_7,"Unfortunately , however , for the reader is is not easy to find out what the authors consider their most relevant contribution .",evaluation
SJzxBpKeM_8,Every single part of the model seems to be quite standard ( basically a network that predicts the probability of a valid sequence and an information-gain based active learning strategy ),evaluation
SJzxBpKeM_9,- so is the specific application to SMILES strings what makes the difference here ?,non-arg
SJzxBpKeM_10,Or is is the specific greedy approximation to the mutual information criterion in the active learning part ?,non-arg
SJzxBpKeM_11,Or is it the way how you augment the dataset ?,non-arg
SJzxBpKeM_12,"All these aspects might be interesting ,",evaluation
SJzxBpKeM_13,but somehow I am missing a coherent picture .,evaluation
SJzxBpKeM_14,"SIGNIFICANCE : it is not entirely clear to me if the proposed "" pruning "" strategy for the completion of prefix sequences can indeed be generally applied to sequence modelling problems ,",evaluation
SJzxBpKeM_15,because in more general domains it might be very difficult to come up with reasonable validity estimates for prefixes that are significantly shorter than the whole sequence .,evaluation
SJzxBpKeM_16,I am not so familiar with SMILES strings,non-arg
SJzxBpKeM_17,-- but could it be that the experimental success reported here is mainly a result of the very specific structure of valid SMILES strings ?,request
SJzxBpKeM_18,"But then , what can be learned for general sequence validation problems ?",request
H1qiNa1HM_0,This paper examines the effects of RL in an augmented action space which includes sequences of actions ( e.g. meta actions ) as well as the primitive actions defined by the MDP .,fact
H1qiNa1HM_1,The authors extend a GPU-based A3C implementation to include meta actions,fact
H1qiNa1HM_2,and show that their algorithm can achieve better sample complexity / and higher performance in most cases .,fact
H1qiNa1HM_3,"The paper is well written ,",evaluation
H1qiNa1HM_4,but fails to mention the relationship between meta-actions and the Options framework ( Sutton et al 99 ) .,fact
H1qiNa1HM_5,"In particular , it seems that meta-actions can just be viewed as a set of predefined options given to the agent .",fact
H1qiNa1HM_6,Much prior work has studied how to combine options with Deep RL .,fact
H1qiNa1HM_7,"To name a few : Multi-Level Discovery of Deep Options ( Fox et al 17 ) ,",reference
H1qiNa1HM_8,"Classifying Options for Deep RL ( Arulkumaran 16 ) ,",reference
H1qiNa1HM_9,and Deep Exploration via Bootstrapped DQN ( Osband et al ) .,reference
H1qiNa1HM_10,The former even learns the options rather than pre-defining them .,fact
H1qiNa1HM_11,This connection needs to be made explicit .,request
H1qiNa1HM_12,I have concerns regarding the results in this paper :,evaluation
H1qiNa1HM_13,• Why on Qbert is the switching agent able to do so much better than both IU and DU ?,non-arg
H1qiNa1HM_14,"I suspect the curves may not be averaged over enough trials and results may be noisy ,",evaluation
H1qiNa1HM_15,as it seems this should n’t be possible .,evaluation
H1qiNa1HM_16,Results curves should show the standard deviation or variance of the 3 runs .,request
H1qiNa1HM_17,• I am concerned this approach will not scale to games that have more actions than the 4 games explored .,evaluation
H1qiNa1HM_18,The concern is that A4C exponentially increases the size of the action space as a function of k.,fact
H1qiNa1HM_19,"Cartpole as well as the explored Atari games all have relatively small action spaces ,",fact
H1qiNa1HM_20,so I think it is critical to show that it scales to games with larger spaces as well .,request
H1qiNa1HM_21,"My concern is that in larger actions spaces , the gains A4C gets from meta-actions will be outweighed by the difficulty of having to learn with so many different actions .",evaluation
H1qiNa1HM_22,• What is the value for k used in Atari experiments ?,non-arg
H1qiNa1HM_23,"Overall , I was very excited about this paper after reading the introduction .",evaluation
H1qiNa1HM_24,"I really like the idea of allowing the network to decide on sequences of actions ,",evaluation
H1qiNa1HM_25,and I think many games do have opportunity to identify and re-use combos of primitive actions ( e.g. to stay between lanes in Beam Rider ) .,evaluation
H1qiNa1HM_26,"However , I do n’t think the architecture , algorithm , and results live up to this motivation .",evaluation
H1qiNa1HM_27,Simply augmenting the action space with all possible sequences of actions begs for a better solution .,request
H1qiNa1HM_28,"Pros : • The authors show that in certain domains , exploration can be aided by pre-defined meta actions .",fact
H1qiNa1HM_29,• The authors introduce an algorithm to squeeze more gradient updates out of a meta-action ( DU-A4C ) .,fact
H1qiNa1HM_30,This is related to the insight that meta actions can be thought of as sequences of primitive actions .,evaluation
H1qiNa1HM_31,Cons : • Relationship to Options is not identified .,fact
H1qiNa1HM_32,• Results are only given for games with small action spaces .,fact
H1qiNa1HM_33,Unclear how the method scales to larger action spaces .,evaluation
H1qiNa1HM_34,• Method for augmenting action space is not particularly interesting .,evaluation
H1qiNa1HM_35,• Heuristic switching is somewhat undesirable .,evaluation
H1qiNa1HM_36,It would be nice to understand why DU stops working well and how to improve it .,request
ByvABDcxz_0,"The authors present an algorithm for training ensembles of policy networks that regularly mixes different policies in the ensemble together by distilling a mixture of two policies into a single policy network , adding it to the ensemble and selecting the strongest networks to remain ( under certain definitions of a "" strong "" network ) .",fact
ByvABDcxz_1,"The experiments compare favorably against PPO and A2C baselines on a variety of MuJoCo tasks ,",fact
ByvABDcxz_2,"although I would appreciate a wall-time comparison as well ,",request
ByvABDcxz_3,"as training the "" crossover "" network is presumably time-consuming .",evaluation
ByvABDcxz_4,"It seems that for much of the paper , the authors could dispense with the genetic terminology altogether - and I mean that as a compliment .",evaluation
ByvABDcxz_5,There are few if any valuable ideas in the field of evolutionary computing,evaluation
ByvABDcxz_6,"and I am glad to see the authors use sensible gradient-based learning for GPO , even if it makes it depart from what many in the field would consider "" evolutionary "" computing .",evaluation
ByvABDcxz_7,Another point on terminology that is important to emphasize - the method for training the crossover network by direct supervised learning from expert trajectories is technically not imitation learning but behavioral cloning .,fact
ByvABDcxz_8,I would perhaps even call this a distillation network rather than a crossover network .,evaluation
ByvABDcxz_9,"In many robotics tasks behavioral cloning is known for overfitting to expert trajectories ,",fact
ByvABDcxz_10,"but that may not be a problem in this setting as "" expert "" trajectories can be generated in unlimited quantities .",fact
BkuDb6tgf_0,This work attempts to improve the global consistency of samples generated by generative adversarial networks by replacing the discriminator with an autoregressive model in an encoded feature space .,fact
BkuDb6tgf_1,The log likelihood of the classification model is then replaced with the log likelihood of the feature space autoregressive model .,fact
BkuDb6tgf_2,"It 's not clear what can be said with respect to the convergence properties of this class of models ,",evaluation
BkuDb6tgf_3,and this is not discussed .,fact
BkuDb6tgf_4,"The method is quite similar in spirit to Denoising Feature Matching of Warde-Farley & Bengio ( 2017 ) ,",evaluation
BkuDb6tgf_5,"as both estimate a density model in feature space -- this method via a constrained autoregressive model and DFM via an estimator of the score function ,",fact
BkuDb6tgf_6,although DFM was used in conjunction with the standard criterion whereas this method replaces it .,fact
BkuDb6tgf_7,This is certainly worth mentioning and discussing .,request
BkuDb6tgf_8,In particular the section in Warde-Farley & Bengio regarding the feature space transformation of the data density seems quite relevant in this work .,evaluation
BkuDb6tgf_9,"Unfortunately the only quantitative measurements reporter are Inception scores ,",fact
BkuDb6tgf_10,which is known to be a poor measure,evaluation
BkuDb6tgf_11,"( and the scores presented are not particularly high , either ) ;",evaluation
BkuDb6tgf_12,Frechet Inception distance or log likelihood estimates via AIS on some dataset would be more convincing .,request
BkuDb6tgf_13,"On the plus side , the authors report an average over Inception scores for multiple runs .",fact
BkuDb6tgf_14,"On the other hand , it sounds as though the stopping criterion was still qualitative .",evaluation
SkKuc-Kef_0,Proposal is to restrict the feasible parameters to ones that have produce a function with small variance over pre-defined groups of images that should be classified the same .,fact
SkKuc-Kef_1,"As authors note , this constraint can be converted into a KKT style penalty with KKT multiplier lambda .",fact
SkKuc-Kef_2,"Thus this is very similar to other regularizers that increase smoothness of the function , such as total variation or a graph Laplacian defined with graph edges connecting the examples in each group , as well as manifold regularization ( see e.g. Belkin , Niyogi et al . JMLR ) .",evaluation
SkKuc-Kef_3,"Heck , in practie ridge regularization will also do something similar for many function classes .",evaluation
SkKuc-Kef_4,Experiments did n't compare to any similar smoothness regularization,fact
SkKuc-Kef_5,( and my preferred would have been a comparison to graph Laplacian or total variation on graphs formed by the same clustered examples ) .,request
SkKuc-Kef_6,It 's also not clear either how important it is that they hand-define the groups over which to minimize variance or if just generally adding smoothness regularization would have achieved the same results .,evaluation
SkKuc-Kef_7,That made it hard to get excited about the results in a vacuum .,evaluation
SkKuc-Kef_8,Would this proposed strategy have thwarted the Russian tank legend problem ?,request
SkKuc-Kef_9,Would it have fixed the Google gorilla problem ?,request
SkKuc-Kef_10,Why or why not ?,request
SkKuc-Kef_11,"Overall , I found the writing a bit bombastic for a strategy that seems to require the user to hand-define groups/clusters of examples .",evaluation
SkKuc-Kef_12,Page 2 : calling additional instances of the same person “ counterfactual observations ” did n’t seem consistent with the usual definition of that term …,evaluation
SkKuc-Kef_13,"maybe I am just missing the semantic link here ,",evaluation
SkKuc-Kef_14,but this is n't how we usually use the term counterfactual in my corner of the field .,evaluation
SkKuc-Kef_15,Re : “ one creates additional samples by modifying … ”,quote
SkKuc-Kef_16,"be nice to quote more of the early work doing this ,",request
SkKuc-Kef_17,"I believe the first work of this sort was Scholkopf ’s , he called it “ virtual examples ”",fact
SkKuc-Kef_18,"and I ’m pretty sure he specifically did it for rotation MNIST images ( and if not exactly that , it was implied ) .",evaluation
SkKuc-Kef_19,"I think the right citation is “ Incorporating invariances in support vector learning machines “ Scholkopf , Burges , Vapnik 1996 , but also see Decoste * Scholkopf 2002 “ Training invariant support vector machines . ”",request
ryVd3dFgf_0,This paper proposes a method for parameter space noise in exploration .,fact
ryVd3dFgf_1,"Rather than the "" baseline "" epsilon-greedy ( that sometimes takes a single action at random ) ... this paper presents an method for perturbations to the policy .",fact
ryVd3dFgf_2,In some domains this can be a much better approach and this is supported by experimentation .,evaluation
ryVd3dFgf_3,There are several things to like about the paper :,evaluation
ryVd3dFgf_4,- Efficient exploration is a big problem for deep reinforcement learning ( epsilon-greedy or Boltzmann is the de-facto baseline ),evaluation
ryVd3dFgf_5,and there are clearly some examples where this approach does much better .,evaluation
ryVd3dFgf_6,"- The noise-scaling approach is ( to my knowledge ) novel , good and in my view the most valuable part of the paper .",evaluation
ryVd3dFgf_7,- This is clearly a very practical and extensible idea ...,evaluation
ryVd3dFgf_8,the authors present good results on a whole suite of tasks .,evaluation
ryVd3dFgf_9,"- The paper is clear and well written ,",evaluation
ryVd3dFgf_10,it has a narrative and the plots/experiments tend to back this up .,evaluation
ryVd3dFgf_11,"- I like the algorithm , it 's pretty simple/clean and there 's something obviously * right * about it ( in SOME circumstances ) .",evaluation
ryVd3dFgf_12,"However , there are also a few things to be cautious of ... and some of them serious :",evaluation
ryVd3dFgf_13,- At many points in the paper the claims are quite overstated .,evaluation
ryVd3dFgf_14,Parameter noise on the policy wo n't necessarily get you efficient exploration ...,evaluation
ryVd3dFgf_15,and in some cases it can even be * worse * than epsilon-greedy ...,evaluation
ryVd3dFgf_16,"if you just read this paper you might think that this was a truly general "" statistically efficient "" method for exploration ( in the style of UCRL or even <VAR> etc ) .",evaluation
ryVd3dFgf_17,"- For instance , the example in 4.2 only works because the optimal solution is to go "" right "" in every timestep ...",evaluation
ryVd3dFgf_18,if you had the network parameterized in a different way ( or the actions left/right were relabelled ) then this parameter noise approach would * not * work ...,evaluation
ryVd3dFgf_19,"By contrast , methods such as UCRL/PSRL and RLSVI <URL> * are * able to learn polynomially in this type of environment .",fact
ryVd3dFgf_20,"I think the claim/motivation for this example in the bootstrapped DQN paper is more along the lines of "" deep exploration """,evaluation
ryVd3dFgf_21,and you should be clear that your parameter noise does * not * address this issue .,request
ryVd3dFgf_22,- That said I think that the example in 4.2 is * great * to include ...,evaluation
ryVd3dFgf_23,you just need to be more upfront about how/why it works and what you are banking on with the parameter-space exploration .,request
ryVd3dFgf_24,Essentially you perform a local exploration rule in parameter space ...,fact
ryVd3dFgf_25,and sometimes this is great,evaluation
ryVd3dFgf_26,- but you should be careful to distinguish this type of method from other approaches .,request
ryVd3dFgf_27,"This must be mentioned in section 4.2 "" does parameter space noise explore efficiently """,request
ryVd3dFgf_28,"because the answer you seem to imply is "" yes "" ... when the answer is clearly NOT IN GENERAL ... but it can still be good sometimes ; D",evaluation
ryVd3dFgf_29,"- The demarcation of "" RL "" and "" evolutionary strategies "" suggests a pretty poor understanding of the literature and associated concepts .",evaluation
ryVd3dFgf_30,"I ca n't really support the conclusion "" RL with parameter noise exploration learns more efficiently than both RL and evolutionary strategies individually "" .",evaluation
ryVd3dFgf_31,This sort of sentence is clearly wrong and for many separate reasons :,evaluation
ryVd3dFgf_32,- Parameter noise exploration is not a separate/new thing from RL ... it 's even been around for ages !,fact
ryVd3dFgf_33,"It feels like you are talking about DQN/A3C / ( whatever algorithm got good scores in Atari last year ) as "" RL "" and that 's just really not a good way to think about it .",evaluation
ryVd3dFgf_34,- Parameter noise exploration can be * extremely * bad relative to efficient exploration methods,evaluation
ryVd3dFgf_35,( see section 2.4.3 <URL> ),reference
ryVd3dFgf_36,"Overall , I like the paper , I like the algorithm",evaluation
ryVd3dFgf_37,and I think it is a valuable contribution .,evaluation
ryVd3dFgf_38,I think the value in this paper comes from a practical/simple way to do policy randomization in deep RL .,evaluation
ryVd3dFgf_39,"In some ( maybe even many of the ones you actually care about ) settings this can be a really great approach , especially when compared to epsilon-greedy .",evaluation
ryVd3dFgf_40,"However , I hope that you address some of the concerns I have raised in this review .",request
ryVd3dFgf_41,You should n't claim such a universal revolution to exploration / RL / evolution,request
ryVd3dFgf_42,because I do n't think that it 's correct .,fact
ryVd3dFgf_43,"Further , I do n't think that clarifying that this method is * not * universal/general really hurts the paper ...",evaluation
ryVd3dFgf_44,"you could just add a section in 4.2 pointing out that the "" chain "" example would n't work if you needed to do different actions at each timestep ( this algorithm does * not * perform "" deep exploration "" ) .",request
ryVd3dFgf_45,I vote accept .,evaluation
ByR8Gr5gf_0,"The paper proposed a copula-based modification to an existing deep variational information bottleneck model , such that the marginals of the variables of interest ( <VAR> , <VAR> ) are decoupled from the DVIB latent variable model , allowing the latent space to be more compact when compared to the non-modified version .",fact
ByR8Gr5gf_1,"The experiments verified the relative compactness of the latent space , and also qualitatively shows that the learned latent features are more ' disentangled ' .",fact
ByR8Gr5gf_2,"However , I wonder how sensitive are the learned latent features to the hyper-parameters and optimizations ?",non-arg
ByR8Gr5gf_3,Quality : Ok .,evaluation
ByR8Gr5gf_4,The claims appear to be sufficiently verified in the experiments .,evaluation
ByR8Gr5gf_5,"However , it would have been great to have an experiment that actually makes use of the learned features to make predictions .",request
ByR8Gr5gf_6,I struggle a little to see the relevance of the proposed method without a good motivating example .,evaluation
ByR8Gr5gf_7,Clarity : Below average .,evaluation
ByR8Gr5gf_8,Section 3 is a little hard to understand .,evaluation
ByR8Gr5gf_9,Is <VAR> in Fig 1 a typo ?,non-arg
ByR8Gr5gf_10,How about <VAR> in equation ( 5 ) ?,non-arg
ByR8Gr5gf_11,There is a reference that appeared twice in the bibliography ( 1st and 2nd ) .,fact
ByR8Gr5gf_12,Originality and Significance : Average .,evaluation
ByR8Gr5gf_13,The paper ( if I understood it correctly ) appears to be mainly about borrowing the key ideas from Rey et . al. 2014 and applying it to the existing DVIB model .,evaluation
H1g6bb9gG_0,The approach solves an important problem,evaluation
H1g6bb9gG_1,as getting labelled data is hard .,evaluation
H1g6bb9gG_2,"The focus is on the key aspect , which is generalisation across heteregeneous data .",fact
H1g6bb9gG_3,The novel idea is the dataset embedding,evaluation
H1g6bb9gG_4,so that their RL policy can be trained to work across diverse datasets .,fact
H1g6bb9gG_5,"Pros : 1 . The approach performs well against all the baselines , and also achieves good cross-task generalisation in the tasks they evaluated on .",evaluation
H1g6bb9gG_6,"2 . In particular , they alsoevaluated on test datasets with fairly different statistics from the training datasets , which isnt very common in most meta-learning papers today ,",evaluation
H1g6bb9gG_7,so it ’s encouraging that the method works in that regime .,evaluation
H1g6bb9gG_8,"Cons : 1 . The embedding strategy , especially the representative and discriminative histograms , is complicated .",evaluation
H1g6bb9gG_9,"It is unclear if the strategy is general enough to work on harder problems / larger datasets , or with higher dimensional data like images .",evaluation
H1g6bb9gG_10,More evidence in the paper for why it would work on harder problems would be great .,request
H1g6bb9gG_11,"2 . The policy network would have to output a probability for each datapoint in the dataset U ,",fact
H1g6bb9gG_12,"which could be fairly large ,",evaluation
H1g6bb9gG_13,thus the method is computationally much more expensive than random sampling .,evaluation
H1g6bb9gG_14,A section devoted to showing what practical problems could be potentially solved by this method would be useful .,request
H1g6bb9gG_15,"3 . It is unclear to me if the results in table 3 and 4 are achieved by retraining from scratch with an RBF SVM , or by freezing the policy network trained on a linear SVM and directly evaluating it with a RBF SVM base learner .",evaluation
H1g6bb9gG_16,Significance/Conclusion : The idea of meta-learning or learning to learn is fairly common now .,evaluation
H1g6bb9gG_17,"While they do show good performance ,",evaluation
H1g6bb9gG_18,it ’s unclear if the specific embedding strategy suggested in this paper will generalise to harder tasks .,evaluation
H1g6bb9gG_19,"Comments : There ’s lots of typos ,",evaluation
H1g6bb9gG_20,please proof read to improve the paper .,request
BkaINb9xz_0,The authors propose an extension to CNN using an autoregressive weighting for asynchronous time series applications .,fact
BkaINb9xz_1,"The method is applied to a proprietary dataset as well as a couple UCI problems and a synthetic dataset , showing improved performance over baselines in the asynchronous setting .",fact
BkaINb9xz_2,This paper is mostly an applications paper .,evaluation
BkaINb9xz_3,"The method itself seems like a fairly simple extension for a particular application ,",evaluation
BkaINb9xz_4,although perhaps the authors have not clearly highlighted details of methodological innovation .,evaluation
BkaINb9xz_5,"I liked that the method was motivated to solve a real problem , and that it does seem to do so well compared to reasonable baselines .",evaluation
BkaINb9xz_6,"However , as an an applications paper , the bread of experiments are a little bit lacking",evaluation
BkaINb9xz_7,"-- with only that one potentially interesting dataset , which happens to proprietary .",fact
BkaINb9xz_8,"Given the fairly empirical nature of the paper in general , it feels like a strong argument should be made , which includes experiments , that this work will be generally significant and impactful .",request
BkaINb9xz_9,The writing of the paper is a bit loose with comments like :,evaluation
BkaINb9xz_10,"“ Besides these and claims of secretive hedge funds ( it can be marketing surfing on the deep learning hype ) , no promising results or innovative architectures were publicly published so far , to the best of our knowledge . ”",quote
BkaINb9xz_11,"Parts of the also appear rush written , with some sentences half finished :",evaluation
BkaINb9xz_12,"“ "" ues of x might be heterogenous , hence On the other hand , significance network provides data-dependent weights for all regressors and sums them up in autoregressive manner . ””",quote
BkaINb9xz_13,"As a minor comment , the statement",non-arg
BkaINb9xz_14,"“ however , due to assumed Gaussianity they are inappropriate for financial datasets , which often follow fat-tailed distributions ( Cont , 2001 ) . ”",quote
BkaINb9xz_15,Is a bit too broad .,evaluation
BkaINb9xz_16,It depends where the Gaussianity appears .,fact
BkaINb9xz_17,"If the likelihood is non-Gaussian , then it often does n’t matter if there are latent Gaussian variables .",fact
BJFxOpcez_0,This paper explores learning dynamic filters for CNNs .,fact
BJFxOpcez_1,"The filters are generated by using the features of an autoencoder on the input image , and linearly combining a set of base filters for each layer .",fact
BJFxOpcez_2,"This addresses an interesting problem which has been looked at a lot before , but with some small new parts .",evaluation
BJFxOpcez_3,There is a lot of prior work in this area,evaluation
BJFxOpcez_4,that should be cited in the area of dynamic filters and steerable filters .,request
BJFxOpcez_5,There are also parallels to ladder networks that should be highlighted .,request
BJFxOpcez_6,"The results indicate improvement over baselines ,",fact
BJFxOpcez_7,however baselines are not strong baselines .,evaluation
BJFxOpcez_8,A key question is what happens when this method is combined with VGG11 which the authors train as a baseline ?,request
BJFxOpcez_9,What is the effect of the reconstruction loss ?,non-arg
BJFxOpcez_10,Can it be removed ?,non-arg
BJFxOpcez_11,There should be some ablation study here .,request
BJFxOpcez_12,"Figure 5 is unclear what is being displayed ,",evaluation
BJFxOpcez_13,there are no labels .,fact
BJFxOpcez_14,Overall I would advise the authors to address these questions and suggest this as a paper suitable for a workshop submission .,request
HyvB9RKez_0,This paper provides theoretical and empirical motivations for removing the top few principle components of commonly-used word embeddings .,fact
HyvB9RKez_1,The paper is well-written and I enjoyed reading it .,evaluation
HyvB9RKez_2,"However , it does not explain how significant this result is beyond that of ( Bullinaria and Levy , 2012 ) ,",fact
HyvB9RKez_3,who also removed the top N dimensions when benchmarking SVD-factorized word embeddings .,fact
HyvB9RKez_4,"From what I can see , this paper provides a more detailed explanation of the phenomenon ( "" why "" it works ) ,",evaluation
HyvB9RKez_5,"which is supported with both theoretical results and a series of empirical analyses , as well as "" updating "" the benchmarks and methods from the pre-neural era .",fact
HyvB9RKez_6,"Although this contribution is relatively incremental ,",evaluation
HyvB9RKez_7,"I find the depth of this work very interesting ,",evaluation
HyvB9RKez_8,and I think future work could perhaps rely on these insights to create better embedding algorithms that directly enforce isotropy .,evaluation
HyvB9RKez_9,"I have two concerns regarding the empirical section , which may be resolvable fairly quickly :",evaluation
HyvB9RKez_10,1 ) Are the embedding vectors L2 normalized before using them in each task ?,non-arg
HyvB9RKez_11,This is known to significantly affect performance .,evaluation
HyvB9RKez_12,I am curious whether removing the top PCs is redundant or not given L2 normalization .,evaluation
HyvB9RKez_13,"2 ) Most of the benchmarks used in this paper are "" toy "" tasks .",evaluation
HyvB9RKez_14,"As Schnabel et al ( 2015 ) and Tsvetkov et al ( 2015 ) showed , there is often little correlation between success on these benchmarks and improvement of downstream NLP tasks .",evaluation
HyvB9RKez_15,I would like to measure the change in performance on a major NLP task that heavily relies on pre-trained word embeddings such as SQuAD .,request
HyvB9RKez_16,"Minor Comments : * The last sentence in the first paragraph ( "" The success comes from the geometry of the representations ... "" ) is not true ;",evaluation
HyvB9RKez_17,the success stems from the ability to capture lexical similarity .,fact
HyvB9RKez_18,"Levy and Goldberg ( 2014 ) showed that searching for the closest word vector to ( king - man + woman ) is equivalent to optimizing a linear combination of 3 similarity terms [ + ( x , king ) , - ( x , man ) , + ( x , woman ) ] .",fact
HyvB9RKez_19,"This explanation was further demonstrated by Linzen ( 2016 ) who showed that even when removing the negative term ( x , man ) , many analogies can still be solved , i.e. by looking for a word that is similar both to "" king "" and to "" woman "" .",fact
HyvB9RKez_20,Add to that the fact that the analogy trick works best when the vectors are L2 normalized ;,fact
HyvB9RKez_21,"if they are all on the unit sphere , what is the geometric interpretation of ( king - man + woman ) , which is not on the unit sphere ?",non-arg
HyvB9RKez_22,"I suggest removing this sentence and other references to linguistic regularities from this paper ,",request
HyvB9RKez_23,"since they are controversial at best , and distract from the main findings .",evaluation
HyvB9RKez_24,* This is also related to Bullinaria and Levy 's ( 2012 ) finding that downweighting the eigenvalue matrix in SVD-based methods improves their performance .,fact
HyvB9RKez_25,Levy et al ( 2015 ) showed that keeping the original eigenvalues can actually degenerate SVD-based embeddings .,fact
HyvB9RKez_26,Perhaps there is a connection to the findings in this paper ?,evaluation
HJ75M8ogM_0,The authors have addressed the problem of translating natural language queries to SQL queries .,fact
HJ75M8ogM_1,They proposed a deep neural network based solution which combines the attention based neural semantic parser and pointer networks .,fact
HJ75M8ogM_2,They also released a new dataset WikiSQL for the problem .,fact
HJ75M8ogM_3,The proposed method outperforms the existing semantic parsing baselines on WikiSQL dataset .,fact
HJ75M8ogM_4,Pros : 1 . The idea of using pointer networks for reducing search space of generated queries is interesting .,evaluation
HJ75M8ogM_5,"Also , using extrinsic evaluation of generated queries handles the possibility of paraphrasing SQL queries .",fact
HJ75M8ogM_6,2 . A new dataset for the problem .,fact
HJ75M8ogM_7,3 . The experiments report a significant boost in the performance compared to the baseline .,fact
HJ75M8ogM_8,The ablation study is helpful for understanding the contribution of different component of the proposed method .,evaluation
HJ75M8ogM_9,Cons : 1 . It would have been better to see performance of the proposed method in other datasets ( wherever possible ) .,request
HJ75M8ogM_10,This is my main concern about the paper .,evaluation
HJ75M8ogM_11,2 . Extrinsic evaluation can slow down the overall training .,evaluation
HJ75M8ogM_12,Comparison of running times would have been helpful .,request
HJ75M8ogM_13,3 . More details about training procedure ( specifically for the RL part ) would have been better .,request
BJu6VdYlf_0,The paper suggests an importance sampling based Coreset construction for Support Vector Machines ( SVM ) .,fact
BJu6VdYlf_1,"To understand the results , we need to understand Coreset and importance sampling :",evaluation
BJu6VdYlf_2,"Coreset : In the context of SVMs , a Coreset is a ( weighted ) subset of given dataset such that for any linear separator , the cost of the separator with respect to the given dataset X is approximately ( there is an error parameter \ eps ) the same as the cost with respect to the weighted subset .",fact
BJu6VdYlf_3,"The main idea is that if one can find a small coreset , then finding the optimal separator ( maximum margin etc. ) over the coreset might be sufficient .",fact
BJu6VdYlf_4,"Since the computation is done over a small subset of points , one hopes to gain in terms of the running time .",fact
BJu6VdYlf_5,"Importance sampling : This is based on the theory developed in Feldman and Langberg , 2011 ( and some of the previous works such as Langberg and Schulman 2010 , the reference of which is missing ) .",fact
BJu6VdYlf_6,The idea is to define a quantity called sensitivity of a data-point that captures how important this datapoint is with respect to contributing to the cost function .,fact
BJu6VdYlf_7,Then a subset of datapoint are sampled based on the sensitivity and the sampled data point is given weight proportional to inverse of the sampling probability .,fact
BJu6VdYlf_8,"As per the theory developed in these past works , sampling a subset of size proportional to the sum of sensitivities gives a coreset for the given problem .",fact
BJu6VdYlf_9,"So , the main contribution of the paper is to do all the sensitivity calculations with respect to SVM problem and then use the importance sampling theory to obtain bounds on the coreset size .",fact
BJu6VdYlf_10,One interesting point of this construction is that Coreset construction involves solving the SVM problem on the given dataset which may seem like beating the purpose .,evaluation
BJu6VdYlf_11,"However , the authors note that one only needs to compute the Coreset of small batches of the given dataset and then use standard procedures ( available in streaming literature ) to combine the Coresets into a single Coreset .",fact
BJu6VdYlf_12,This should give significant running time benefits .,fact
BJu6VdYlf_13,The paper also compares the results against the simple procedure where a small uniform sample from the dataset is used for computation .,fact
BJu6VdYlf_14,Evaluation : Significance : Coresets give significant running time benefits when working with very big datasets .,fact
BJu6VdYlf_15,Coreset construction in the context of SVMs is a relevant problem and should be considered significant .,evaluation
BJu6VdYlf_16,Clarity : The paper is reasonably well-written .,evaluation
BJu6VdYlf_17,The problem has been well motivated and all the relevant issues point out for the reader .,evaluation
BJu6VdYlf_18,The theoretical results are clearly stated as lemmas a theorems that one can follow without looking at proofs .,evaluation
BJu6VdYlf_19,Originality : The paper uses previously developed theory of importance sampling .,fact
BJu6VdYlf_20,"However , the sensitivity calculations in the SVM context is new as per my knowledge .",fact
BJu6VdYlf_21,It is nice to know the bounds given in the paper and to understand the theoretical conditions under which we can obtain running time benefits using corsets .,evaluation
BJu6VdYlf_22,Quality : The paper gives nice theoretical bounds in the context of SVMs .,evaluation
BJu6VdYlf_23,One aspect in which the paper is lacking is the empirical analysis .,evaluation
BJu6VdYlf_24,The paper compares the Coreset construction with simple uniform sampling .,fact
BJu6VdYlf_25,"Since Coreset construction is being sold as a fast alternative to previous methods for training SVMs ,",evaluation
BJu6VdYlf_26,it would have been nice to see the running time and cost comparison with other training methods that have been discussed in section 2 .,request
rJ74wm5xM_0,The paper describes a neural network-based approach to active localization based upon RGB images .,fact
rJ74wm5xM_1,The framework employs Bayesian filtering to maintain an estimate of the agent 's pose using a convolutional network model for the measurement ( perception ) function .,fact
rJ74wm5xM_2,A convolutional network models the policy that governs the action of the agent .,fact
rJ74wm5xM_3,The architecture is trained in an end-to-end manner via reinforcement learning .,fact
rJ74wm5xM_4,The architecture is evaluated in 2D and 3D simulated environments of varying complexity and compared favorably to traditional ( structured ) approaches to passive and active localization .,fact
rJ74wm5xM_5,"As the paper correctly points out , there is large body of work on map-based localization ,",fact
rJ74wm5xM_6,"but relatively little attention has been paid to decision theoretic formulations to localization , whereby the agent 's actions are chosen in order to improve localization accuracy .",fact
rJ74wm5xM_7,"More recent work instead focuses on the higher level objective of navigation , whereby any effort act in an effort to improve localization are secondary to the navigation objective .",fact
rJ74wm5xM_8,"The idea of incorporating learned representations with a structured Bayesian filtering approach is interesting ,",evaluation
rJ74wm5xM_9,but it 's utility could be better motivated .,evaluation
rJ74wm5xM_10,What are the practical benefits to learning the measurement and policy model beyond ( i ) the temptation to apply neural networks to this problem and ( ii ) the ability to learn these in an end-to-end fashion ?,request
rJ74wm5xM_11,"That 's not to say that there are n't benefits , but rather that they are n't clearly demonstrated here .",evaluation
rJ74wm5xM_12,"Further , the paper seems to assume ( as noted below ) that there is no measurement uncertainty and , with the exception of the 3D evaluations , no process noise .",fact
rJ74wm5xM_13,"The evaluation demonstrates that the proposed method yields estimates that are more accurate according to the proposed metric than the baseline methods , with a significant reduction in computational cost .",fact
rJ74wm5xM_14,"However , the environments considered are rather small by today 's standards",evaluation
rJ74wm5xM_15,and the baseline methods almost 20 years old .,fact
rJ74wm5xM_16,"Further , the evaluation makes a number of simplifying assumptions , the largest being that the measurements are not subject to noise",fact
rJ74wm5xM_17,( the only noise that is present is in the motion for the 3D experiments ) .,fact
rJ74wm5xM_18,This assumption is clearly not valid in practice .,fact
rJ74wm5xM_19,"Further , it is not clear from the evaluation whether the resulting distribution that is maintained is consistent ( e.g. , are the estimates over - / under-confident ? ) .",evaluation
rJ74wm5xM_20,This has important implications if the system were to actually be used on a physical system .,evaluation
rJ74wm5xM_21,"Further , while the computational requirements at test time are significantly lower than the baselines ,",fact
rJ74wm5xM_22,the time required for training is likely very large .,evaluation
rJ74wm5xM_23,"While this is less of an issue in simulation , it is important for physical deployments .",evaluation
rJ74wm5xM_24,"Ideally , the paper would demonstrate performance when transferring a policy trained in simulation to a physical environment ( e.g. , using diversification , which has proven effective at simulation-to-real transfer ) .",request
rJ74wm5xM_25,Comments/Questions : * The nature of the observation space is not clear .,evaluation
rJ74wm5xM_26,"* Recent related work has focused on learning neural policies for navigation , and any localization-specific actions are secondary to the objective of reaching the goal .",fact
rJ74wm5xM_27,It would be interesting to discuss how one would balance the advantages of choosing actions that improve localization with those in the context of a higher-level task ( or at least including a cost on actions as with the baseline method of Fox et al. ) .,request
rJ74wm5xM_28,* The evaluation that assigns different textures to each wall is unrealistic .,evaluation
rJ74wm5xM_29,* It is not clear why the space over which the belief is maintained flips as the robot turns and shifts as it moves .,evaluation
rJ74wm5xM_30,* The 3D evaluation states that a 360 deg view is available .,fact
rJ74wm5xM_31,What happens when the agent can only see in one ( forward ) direction ?,request
rJ74wm5xM_32,* AML includes a cost term in the objective .,fact
rJ74wm5xM_33,Did the author ( s ) experiment with setting this cost to zero ?,non-arg
rJ74wm5xM_34,* The 3D environments rely upon a particular belief size ( 70 x 70 ) being suitable for all environments .,fact
rJ74wm5xM_35,What would happen if the test environment was larger than those encountered in training ?,request
rJ74wm5xM_36,"* The comment that the PoseNet and VidLoc methods "" lack a strainghtforward method to utilize past map data to do localization in a new environment "" is unclear .",evaluation
rJ74wm5xM_37,* The environments that are considered are quite small compared to the domains currently considered for,evaluation
rJ74wm5xM_38,* Minor : It might be better to move Section 3 into Section 4 after introducing notation ( to avoid redundancy ) .,request
rJ74wm5xM_39,"* The paper should be proofread for grammatical errors ( e.g. , "" bayesian "" -- > "" Bayesian "" , "" gaussian "" -- > "" Gaussian "" )",request
H1JzYwcxM_0,"=== SUMMARY === The paper considers a combination of Reinforcement Learning ( RL ) and Imitation Learning ( IL ) , in the infinite horizon discounted MDP setting .",fact
H1JzYwcxM_1,"The IL part is in the form of an oracle that returns a value function <VAR> , which is an approximation of the optimal value function .",fact
H1JzYwcxM_2,"The paper defines a new cost ( or reward ) function based on <VAR> , through shaping ( Eq . 1 ) .",fact
H1JzYwcxM_3,It is known that shaping does not change the optimal policy .,fact
H1JzYwcxM_4,"A key aspect of this paper is to consider a truncated horizon problem ( say horizon k ) with the reshaped cost function , instead of an infinite horizon MDP .",fact
H1JzYwcxM_5,"For this truncated problem , one can write the ( dis ) advantage function as a k-step sum of reward plus the value returned by the oracle at the k-th step ( cf. Eq . 5 ) .",fact
H1JzYwcxM_6,Theorem 3.3 shows that the value of the optimal policy of the truncated MDP w.r.t. the original MDP is only <VAR> worse than the optimal policy of the original problem ( gamma is the discount factor and eps is the error between <VAR> and <VAR> ) .,fact
H1JzYwcxM_7,This suggests two things : 1 ) Having an oracle that is accurate ( small eps ) leads to good performance .,fact
H1JzYwcxM_8,"If oracle is the same as the optimal value function , we do not need to plan more than a single step ahead .",fact
H1JzYwcxM_9,"2 ) By planning for k steps ahead , one can decrease the error in the oracle geometrically fast .",fact
H1JzYwcxM_10,"In the limit of <EQN> , the error in the oracle does not matter .",fact
H1JzYwcxM_11,"Based on this insight , the paper suggests an actor-critic-like algorithm called THOR ( Truncated HORizon policy search ) that minimizes the total cost over a truncated horizon with a modified cost function .",fact
H1JzYwcxM_12,"Through a series of experiments on several benchmark problems ( inverted pendulum , swimmer , etc. ) , the paper shows the effect of planning horizon k.",fact
H1JzYwcxM_13,=== EVALUATION & COMMENTS === I like the main idea of this paper .,evaluation
H1JzYwcxM_14,The paper is also well-written .,evaluation
H1JzYwcxM_15,"But one of the main ideas of this paper ( truncating the planning horizon and replacing it with approximation of the optimal value function ) is not new and has been studied before ,",fact
H1JzYwcxM_16,but has not been properly cited and discussed .,fact
H1JzYwcxM_17,There are a few papers that discuss truncated planning .,fact
H1JzYwcxM_18,Most closely is the following paper :,evaluation
H1JzYwcxM_19,<CIT> .,reference
H1JzYwcxM_20,The motivation of AAAI 2016 paper is different from this work .,fact
H1JzYwcxM_21,"The goal there is to speedup the computation of finite , but large , horizon problem with a truncated horizon planning .",fact
H1JzYwcxM_22,"The setting there is not the combination of RL and IL , but multi-task RL .",fact
H1JzYwcxM_23,An approximation of optimal value function for each task is learned off-line and then used as the terminal cost .,fact
H1JzYwcxM_24,The important point is that the learned function there plays the same role as the value provided by the oracle <VAR> in this work .,fact
H1JzYwcxM_25,They both are used to shorten the planning horizon .,fact
H1JzYwcxM_26,"That paper theoretically shows the effect of various error terms , including terms related to the approximation in the planning process ( this paper does not do that ) .",fact
H1JzYwcxM_27,"Nonetheless , the resulting algorithms are quite different .",evaluation
H1JzYwcxM_28,The result of this work is an actor-critic type of algorithm .,fact
H1JzYwcxM_29,AAAI 2016 paper is an approximate dynamic programming type of algorithm .,fact
H1JzYwcxM_30,There are some other papers that have ideas similar to this work in relation to truncating the horizon .,fact
H1JzYwcxM_31,"For example , the multi-step lookahead policies and the use of approximate value function as the terminal cost in the following paper :",fact
H1JzYwcxM_32,<CIT> .,reference
H1JzYwcxM_33,"The use of learned value function to truncate the rollout trajectory in a classification-based approximate policy iteration method has been studied by Gabillon , Lazaric , Ghavamzadeh , and Scherrer , “ Classification-based Policy Iteration with a Critic , ” ICML , 2011 .",fact
H1JzYwcxM_34,"Or in the context of Monte Carlo Tree Search planning , the following paper is relevant :",fact
H1JzYwcxM_35,<CIT> .,reference
H1JzYwcxM_36,Their “ value network ” has a similar role to <VAR> .,evaluation
H1JzYwcxM_37,It provides an estimate of the states at the truncated horizon to shorten the planning depth .,fact
H1JzYwcxM_38,"Note that even though these aforementioned papers are not about IL ,",fact
H1JzYwcxM_39,this paper ’s stringent requirement of having access to <VAR> essentially make it similar to those papers .,evaluation
H1JzYwcxM_40,"In short , a significant part of this work ’s novelty has been explored before .",evaluation
H1JzYwcxM_41,"Even though not being completely novel is totally acceptable ,",evaluation
H1JzYwcxM_42,it is important that the paper better position itself compared to the prior art .,request
H1JzYwcxM_43,"Aside this main issue , there are some other comments : - Theorem 3.1 is not stated clearly and may suggest more than what is actually shown in the proof .",evaluation
H1JzYwcxM_44,The problem is that it is not clear about the fact the choice of eps is not arbitrary .,evaluation
H1JzYwcxM_45,The proof works only for eps that is larger than 0.5 .,fact
H1JzYwcxM_46,"With the construction of the proof , if eps is smaller than 0.5 , there would not be any error , i.e. , <EQN> .",fact
H1JzYwcxM_47,"The theorem basically states that if the error is very large ( half of the range of value function ) , the agent does not not perform well .",fact
H1JzYwcxM_48,Is this an interesting case ?,evaluation
H1JzYwcxM_49,"- In addition to the papers I mentioned earlier , there are some results suggesting that shorter horizons might be beneficial and/or sufficient under certain conditions .",fact
H1JzYwcxM_50,A related work is a theorem in the PhD dissertation of Ng :,fact
H1JzYwcxM_51,<CIT> .,reference
H1JzYwcxM_52,( Theorem 5 in Appendix 3 . B : Learning with a smaller horizon ) .,quote
H1JzYwcxM_53,"It is shown that if the error between Phi ( equivalent to <VAR> here ) and <VAR> is small , one may choose a discount factor gamma ’ that is smaller than gamma of the original MDP , and still have some guarantees .",fact
H1JzYwcxM_54,"As the discount factor has an interpretation of the effective planning horizon ,",fact
H1JzYwcxM_55,this result is relevant .,evaluation
H1JzYwcxM_56,"The result , however , is not directly comparable to this work",evaluation
H1JzYwcxM_57,"as the planning horizon appears implicitly in the form of <VAR> instead of k ,",fact
H1JzYwcxM_58,but I believe it is worth to mention and possibly compare .,request
H1JzYwcxM_59,"- The IL setting in this work is that an oracle provides <VAR> , which is the same as ( Ross & Bagnell , 2014 ) .",fact
H1JzYwcxM_60,I believe this setting is relatively restrictive,evaluation
H1JzYwcxM_61,"as in many problems we only have access to ( state , action ) pairs , or sequence thereof , and not the associated value function .",fact
H1JzYwcxM_62,"For example , if a human is showing how a robot or a car should move , we do not easily have access to <VAR> ( unless the reward function is known and we estimate the value with rollouts ; which requires us having a long trajectory ) .",fact
H1JzYwcxM_63,"This is not a deal breaker ,",evaluation
H1JzYwcxM_64,"and I would not consider this as a weakness of the work ,",evaluation
H1JzYwcxM_65,but the paper should be more clear and upfront about this .,request
H1JzYwcxM_66,"- The use of differential operator nabla instead of gradient of a function ( a vector field ) in Equations ( 10 ) , ( 14 ) , ( 15 ) is non-standard .",fact
H1JzYwcxM_67,"- Figures are difficult to read ,",evaluation
H1JzYwcxM_68,as the colors corresponding to confidence regions of different curves are all mixed up .,fact
H1JzYwcxM_69,Maybe it is better to use standard error instead of standard deviation .,request
S1kxi6OlM_0,In general I find this to be a good paper and vote for acceptance .,evaluation
S1kxi6OlM_1,The paper is well-written and easy to follow .,evaluation
S1kxi6OlM_2,The proposed approach is a useful addition to existing literature .,evaluation
S1kxi6OlM_3,Besides that I have not much to say except one point I would like to discuss :,evaluation
S1kxi6OlM_4,In 4.2 I am not fully convinced of using an adversial model for goal generation .,evaluation
S1kxi6OlM_5,RL algorithms generally suffer from poor stability,evaluation
S1kxi6OlM_6,and GANs themselves can have convergence issues .,evaluation
S1kxi6OlM_7,This imposes another layer of possible instability .,fact
S1kxi6OlM_8,"Besides , generating useful reward function , while not trivial , can be seen as easier than solving the full RL problem .",evaluation
S1kxi6OlM_9,"Can the authors argue why this model class was chosen over other , more simple , generative models ?",request
S1kxi6OlM_10,"Furthermore , did the authors do experiments with simpler models ?",request
S1kxi6OlM_11,"Related : "" We found that the LSGAN works better than other forms of GAN for our problem . """,quote
S1kxi6OlM_12,"Was this improvement minor , or major , or did n't even work with other GAN types ?",request
S1kxi6OlM_13,"This question is important ,",evaluation
S1kxi6OlM_14,because for me the big question is if this model is universal and stable in a lot of applications or requires careful fine-tuning and monitoring .,evaluation
S1ufxZqlG_0,The authors propose an objective whose Lagrangian dual admits a variety of modern objectives from variational auto-encoders and generative adversarial networks .,fact
S1ufxZqlG_1,They describe tradeoffs between flexibility and computation in this objective leading to different approaches .,fact
S1ufxZqlG_2,"Unfortunately , I 'm not sure what specific contributions come out ,",evaluation
S1ufxZqlG_3,and the paper seems to meander in derivations and remarks that I did n't understand what the point was .,evaluation
S1ufxZqlG_4,"First , it 's not clear what this proposed generalization offers .",evaluation
S1ufxZqlG_5,"It 's a very nuanced and not insightful construction ( eq . 3 ) and with a specific choice of a weighted sum of mutual informations subject to a combinatorial number of divergence measure constraints , each possibly held in expectation ( eq . 5 ) to satisfy the chosen subclass of VAEs and GANs ; and with or without likelihoods ( eq . 7 ) .",evaluation
S1ufxZqlG_6,What specific insights come from this that is n't possible without the proposed generalization ?,fact
S1ufxZqlG_7,It 's also not clear with many GAN algorithms that reasoning with their divergence measure in the limit of infinite capacity discriminators is even meaningful,evaluation
S1ufxZqlG_8,"( e.g. , Arora et al. , 2017 ;",reference
S1ufxZqlG_9,"Fedus et al. , 2017 ) .",reference
S1ufxZqlG_10,It 's only true for consistent objectives such as MMD-GANs .,fact
S1ufxZqlG_11,Section 4 seems most pointed in explaining potential insights .,evaluation
S1ufxZqlG_12,"However , it only introduces hyperparameters and possible combinatorial choices with no particular guidance in mind .",fact
S1ufxZqlG_13,"For example , there are no experiments demonstrating the usefulness of this approach except for a toy mixture of Gaussians and binarized MNIST , explaining what is already known with the beta-VAE and infoGAN .",fact
S1ufxZqlG_14,It would be useful if the authors could make the paper overall more coherent and targeted to answer specific problems in the literature rather than try to encompass all of them .,request
S1ufxZqlG_15,"Misc + The "" feature marginal "" is also known as the aggregate posterior ( Makhzani et al. , 2015 ) and average encoding distribution ( Hoffman and Johnson , 2016 ) ; also see Tomczak and Welling ( 2017 ) .",fact
S1UrbZQ-f_0,( Score before author revision : 4 ) ( Score after author revision : 7 ),non-arg
S1UrbZQ-f_1,"I think the authors have taken both the feedback of reviewers as well as anonymous commenters thoroughly into account , running several ablations as well as reporting nice results on an entirely new dataset ( MultiNLI )",evaluation
S1UrbZQ-f_2,where they show how their multi level fusion mechanism improves a baseline significantly .,fact
S1UrbZQ-f_3,I think this is nice,evaluation
S1UrbZQ-f_4,since it shows how their mechanism helps on two different tasks ( question answering and natural language inference ) .,fact
S1UrbZQ-f_5,Therefore I would now support accepting this paper .,evaluation
S1UrbZQ-f_6,------------ ( Original review below ) -----------------------,non-arg
S1UrbZQ-f_7,"The authors present an enhancement to the attention mechanism called "" multi-level fusion """,fact
S1UrbZQ-f_8,that they then incorporate into a reading comprehension system .,fact
S1UrbZQ-f_9,It basically takes into account a richer context of the word at different levels in the neural net to compute various attention scores .,fact
S1UrbZQ-f_10,"i.e. the authors form a vector "" HoW "" ( called history of the word ) ,",fact
S1UrbZQ-f_11,"that is defined as a concatenation of several vectors : <EQN> where <EQN> , <EQN> there is no predicate put with 15",fact
S1UrbZQ-f_12,"( <CIT> ) ,",reference
S1UrbZQ-f_13,and <VAR> and <VAR> are different LSTM states for that word .,fact
S1UrbZQ-f_14,The attention score is then a function of these concatenated vectors i.e. <EQN> it can not stay by itself merge with 21,fact
S1UrbZQ-f_15,Results on SQuAD show a small gain in accuracy ( 75.7 -> 76.0 Exact Match ) .,fact
S1UrbZQ-f_16,The gains on the adversarial set are larger,fact
S1UrbZQ-f_17,"but that is because some of the higher performing , more recent baselines do n't seem to have adversarial numbers .",evaluation
S1UrbZQ-f_18,The authors also compare various attention functions ( Table 5 ) showing a particular one ( Symmetric + ReLU ) works the best .,fact
S1UrbZQ-f_19,Comments : - I feel overall the contribution is not very novel .,evaluation
S1UrbZQ-f_20,The general neural architecture that the authors propose in Section 3 is generally quite similar to the large number of neural architectures developed for this dataset,evaluation
S1UrbZQ-f_21,( e.g. some combination of attention between question/context and LSTMs over question/context ) .,non-arg
S1UrbZQ-f_22,"The only novelty is these "" HoW "" inputs to the extra attention mechanism",evaluation
S1UrbZQ-f_23,that takes a richer word representation into account .,fact
S1UrbZQ-f_24,"- I feel the model is seems overly complicated for the small gain ( i.e. 75.7 -> 76.0 Exact Match ) ,",evaluation
S1UrbZQ-f_25,especially on a relatively exhausted dataset ( SQuAD ) that is known to have lots of pecularities ( see anonymous comment below ) .,evaluation
S1UrbZQ-f_26,It is possible the gains just come from having more parameters .,evaluation
S1UrbZQ-f_27,"- The authors ( on page 6 ) claim that that by running attention multiple times with different parameters but different inputs ( i.e. <EQN> , <EQN> , <EQN> ) it will learn to attend to "" different regions for different level "" .",fact
S1UrbZQ-f_28,"However , there is nothing enforcing this",fact
S1UrbZQ-f_29,and the gains just probably come from having more parameters/complexity .,evaluation
Hy4cMGVlf_0,"The authors build on the work of Tang et al. ( 2017 ) ,",evaluation
Hy4cMGVlf_1,"who made a minor change to the skip-thought model by decoding only the next sentence , rather than the previous one also .",fact
Hy4cMGVlf_2,"The additional minor change in this paper is to use a CNN , rather than RNN , decoder .",evaluation
Hy4cMGVlf_3,"I am sympathetic to the goals of the work , and believe this sort of work should be carried out ,",evaluation
Hy4cMGVlf_4,but I see the contribution as too minor to constitute a paper at the conference track of a leading international conference such as ICLR .,evaluation
Hy4cMGVlf_5,"Given the incremental nature of the work , I think this would be a good fit for something like a short paper at * ACL .",evaluation
Hy4cMGVlf_6,"I found the more theoretical motivation of the CNN decoder not terribly convincing , and somewhat post-hoc .",evaluation
Hy4cMGVlf_7,I feel as though analogous arguments could just as easily be made for an RNN decoder .,evaluation
Hy4cMGVlf_8,Ultimately I see these questions - such as CNN vs. RNN for the decoder - as empirical ones .,evaluation
Hy4cMGVlf_9,"Finally , the authors have admirably attempted a thorough comparison with existing work , in the related work section ,",evaluation
Hy4cMGVlf_10,"but this section takes up a large chunk of the paper at the end ,",fact
Hy4cMGVlf_11,and again I would have preferred this section to be much shorter and more concise .,request
Hy4cMGVlf_12,"Summary : worthwhile empirical goal ,",evaluation
Hy4cMGVlf_13,but the paper could have been easily written using half as much space .,evaluation
r1rOlgOlz_0,"Authors describe a procedure of building their production recommender system from scratch , begining with formulating the recommendation problem , label data formation , model construction and learning .",fact
r1rOlgOlz_1,"They use several different evaluation techniques to show how successful their model is ( offline metrics , A/B test results , etc . )",fact
r1rOlgOlz_2,Most of the originality comes from integrating time decay of purchases into the learning framework .,evaluation
r1rOlgOlz_3,Rest of presented work is more or less standard .,evaluation
r1rOlgOlz_4,Paper may be useful to practitioners who are looking to implement something like this in production .,evaluation
BJiW7IkZM_0,"In this work , the objective is to analyze the robustness of a neural network to any sort of attack .",fact
BJiW7IkZM_1,This is measured by naturally linking the robustness of the network to the local Lipschitz properties of the network function .,fact
BJiW7IkZM_2,"This approach is quite standard in learning theory ,",evaluation
BJiW7IkZM_3,I am not aware of how original this point of view is within the deep learning community .,evaluation
BJiW7IkZM_4,This is estimated by obtaining values of the norm of the gradient ( also naturally linked to the Lipschitz properties of the function ) by backpropagation .,fact
BJiW7IkZM_5,This is again a natural idea .,evaluation
HkKWeUCef_0,Adversarial example is studied on one synthetic data .,fact
HkKWeUCef_1,A neural networks classifier is trained on this synthetic data .,fact
HkKWeUCef_2,Average distances and norms of errorneous perturbations are computed .,fact
HkKWeUCef_3,It is observed that small perturbation ( chosen in a right direction ) is sufficient to cause misclassification .,fact
HkKWeUCef_4,"CONS : The writing is bad and hard to follow , with typos :",evaluation
HkKWeUCef_5,for example what is a period just before section 3.1 for ?,request
HkKWeUCef_6,"Another example is "" Red lines indicate the range of needed for perfect classification "" , which does not make sense .",request
HkKWeUCef_7,Yet another example is the period at the end of Proposition 4.1 .,request
HkKWeUCef_8,"Another example is "" One counter-intuitive property of adversarial examples is it that nearly "" .",request
HkKWeUCef_9,"It looks as if the paper was written in a hurry , and it shows in the writing .",evaluation
HkKWeUCef_10,"At the beginning of Section 3 , Figure 1 is discussed .",fact
HkKWeUCef_11,It points out that there exists adversarial directions that are very bad .,fact
HkKWeUCef_12,But I do n't see how it is relevant to adversarial examples .,evaluation
HkKWeUCef_13,"If one was interested in studying adversarial examples , then one would have done the following .",evaluation
HkKWeUCef_14,"Under the setting of Figure 1 , pick a test data randomly from the distribution ( and one of the classes ) , and find an adversarial direction",request
HkKWeUCef_15,I do not see how Section 3.1 fits in with other parts of the paper .,evaluation
HkKWeUCef_16,Is it related to any experiment ?,request
HkKWeUCef_17,Why it defining a manifold attack ?,request
HkKWeUCef_18,"Putting a "" conjecture "" on a paper has to be accompanied by the depth of the insight that brought the conjecture .",evaluation
HkKWeUCef_19,"Having an unjustified conjecture 5.1 would poison the field of adversarial examples ,",evaluation
HkKWeUCef_20,and it must be removed .,request
HkKWeUCef_21,"This paper is a list of experiments and observations , that are not coherent and does not give much insight into the topics of "" adversarial examples "" .",evaluation
HkKWeUCef_22,"The only main messages are that on ONE synthetic dataset , random perturbation does not cause misclassification and targeted classification can cause misclassification .",fact
HkKWeUCef_23,"And , expected loss is good while worst-case loss is bad .",evaluation
HkKWeUCef_24,"This , in my opinion , is not enough to be published at a conference .",evaluation
BkzesZcxG_0,The authors propose an extension to the Neural Statistician which can model contexts with multiple partially overlapping features .,fact
BkzesZcxG_1,This model can explain datasets by taking into account covariate structure needed to explain away factors of variation and it can also share this structure partially between datasets .,fact
BkzesZcxG_2,"A particularly interesting aspect of this model is the fact that it can learn these context c as features conditioned on meta-context a , which leads to a disentangled representation .",evaluation
BkzesZcxG_3,This is also not dissimilar to ideas used in ' Bayesian Representation Learning With Oracle Constraints ' Karaletsos et al 2016,evaluation
BkzesZcxG_4,where similar contextual features c are learned to disentangle representations over observations and implicit supervision .,evaluation
BkzesZcxG_5,The authors provide a clean variational inference algorithm to learn their model .,evaluation
BkzesZcxG_6,"However , a key problem is the following : the nature of the discrete variables being used makes them hard to be inferred with variational inference .",evaluation
BkzesZcxG_7,"The authors mention categorical reparametrization as their trick of choice , but do not go into empirical details int heir experiments regarding the success of this approach .",fact
BkzesZcxG_8,"In fact , it would be interesting to study which level of these variables could be analytically collapsed ( such as done in the Semi-Supervised learning work by Kingma et al 2014 ) and which ones can be sampled effectively using a form of reparametrization .",request
BkzesZcxG_9,"This also touches on the main criticism of the paper : While the model technically makes sense and is cleanly described and derived , the empirical evaluation is on the weak side and the rich properties of the model are not really shown off .",evaluation
BkzesZcxG_10,It would be interesting if the authors could consider adding a more illustrative experiment and some more empirical results regarding inference in this model and the marginal structures that can be learned with this model in controlled toy settings .,request
BkzesZcxG_11,Can the model recover richer structure that was imposed during data generation ?,request
BkzesZcxG_12,How limiting is the learning of a ?,request
BkzesZcxG_13,How does the likelihood of the model behave under the circumstances ?,request
BkzesZcxG_14,The experiments do not really convey how well this all will work in practice .,fact
B1m1clFlM_0,"This paper presents MAd-RL , a method for decomposition of a single-agent RL problem into a simple sub-problems , and aggregating them back together .",fact
B1m1clFlM_1,"Specifically , the authors propose a novel local planner - emphatic , and analyze the newly proposed local planner along of two existing ones - egocentric and agnostic .",fact
B1m1clFlM_2,"The MAd-RL , and theoretical analysis , is evaluated on the Pac-Boy task , and compared to DQN and Q-learning with function approximation .",fact
B1m1clFlM_3,"Pros : 1 . The paper is well written , and well-motivated .",evaluation
B1m1clFlM_4,"2 . The authors did an extraordinary job in building the intuition for the theoretical work , and giving appropriate examples where needed .",evaluation
B1m1clFlM_5,3 . The theoretical analysis of the paper is extremely interesting .,evaluation
B1m1clFlM_6,"The observation that a linearly weighted reward , implies linearly weighted Q function , analysis of different policies , and local minima that result is the strongest and the most interesting points of this paper .",evaluation
B1m1clFlM_7,Cons : 1 . The paper is too long .,evaluation
B1m1clFlM_8,"14 pages total - 4 extra pages ( in appendix ) over the 8 page limit ,",fact
B1m1clFlM_9,and 1 extra page of references .,fact
B1m1clFlM_10,"That is 50 % overrun in the context ,",fact
B1m1clFlM_11,and 100 % overrun in the references .,fact
B1m1clFlM_12,"The most interesting parts and the most of the contributions are in the Appendix ,",evaluation
B1m1clFlM_13,which makes it hard to assess the contributions of the paper .,evaluation
B1m1clFlM_14,"There are two options : 1.1 If the paper is to be considered as a whole , the excessive overrun gives this paper unfair advantage over other ICLR papers .",evaluation
B1m1clFlM_15,The flavor and scope and quality of the problems that can be tackled with 50 % more space is substantially different from what can be addressed within the set limit .,evaluation
B1m1clFlM_16,"If the extra space is necessary , perhaps this paper is better suited for another publication ?",request
B1m1clFlM_17,"1.2 If the paper is assessed only based on the main part without Appendix , then the only novelty is emphatic planner , and the theoretical claims with no proofs .",evaluation
B1m1clFlM_18,"The results are interesting ,",evaluation
B1m1clFlM_19,but are lacking implementation details .,evaluation
B1m1clFlM_20,"Overall , a substandard paper .",evaluation
B1m1clFlM_21,2 . Experiments are disjoint from the method ’s section .,evaluation
B1m1clFlM_22,For example : 2.1 Section 5.1 is completely unrelated with the material presented in Section 4 .,evaluation
B1m1clFlM_23,"2.2 The noise evaluation in Section 5.3 is nice ,",evaluation
B1m1clFlM_24,but not related with the Section 4 .,evaluation
B1m1clFlM_25,"This is problematic because , it is not clear if the focus of the paper is on evaluating MAd-RL and performance on the Ms.PacMan task , or experimentally demonstrating claims in Section 4 .",evaluation
B1m1clFlM_26,Recommendations : 1 . Shorten the paper to be within ( or close to the recommended length ) including Appendix .,request
B1m1clFlM_27,"2 . Focus paper on the analysis of the advisors ,",request
B1m1clFlM_28,and Section 5 . on demonstrating the claims .,request
B1m1clFlM_29,3 . Be more explicit about the contributions .,request
B1m1clFlM_30,4 . How does the negative reward influence the behavior the agent ?,non-arg
B1m1clFlM_31,The agent receives negative reward when near ghosts .,fact
B1m1clFlM_32,5 . Move the short ( or all ) proofs from Appendix into the main text .,request
B1m1clFlM_33,6 . Move implementation details of the experiments ( in particular the short ones ) into the main text .,request
B1m1clFlM_34,7 . Use the standard terminology ( greedy and random policies vs. egoistic and agnostic ) where possible .,request
B1m1clFlM_35,The new terms for well-established make the paper needlessly more complex .,evaluation
B1m1clFlM_36,"8 . Focus the literature review on the most relevant work , and contrast the proposed work with existing peer reviewed methods .",request
B1m1clFlM_37,9 . Revise the literature to emphasize more recent peer reviewed references .,request
B1m1clFlM_38,"Only three references are recent ( less than 5 years ) , peer reviewed references ,",evaluation
B1m1clFlM_39,while there are 12 historic references .,fact
B1m1clFlM_40,Try to reduce dependencies on non-peer reviewed references ( ~ 10 of them ) .,request
B1m1clFlM_41,"10 . Make a pass through the paper , and decouple it from the van Seijen et al. , 2017a",request
B1m1clFlM_42,11 . Minor : Some claims need references :,request
B1m1clFlM_43,"11.1 Page 5 : “ egocentric sub-optimality does not come from the actions that are equally good , nor from the determinism of the policy , since adding randomness … ” -",quote
B1m1clFlM_44,Would n’t adding epsilon-greediness get the agent unstuck ?,non-arg
B1m1clFlM_45,11.2 Page 1 . “ It is shown on the navigation task … . ” -,quote
B1m1clFlM_46,"This seems to be shown later in the results ,",fact
B1m1clFlM_47,"but in the intro it is not clear if some other work , or this one shows it .",evaluation
B1m1clFlM_48,12 . Minor : 12.1 Mix genders when talking about people .,fact
B1m1clFlM_49,"Do n’t assume all people that make “ complex and important problems ” , or who are “ consulted for advice ” , are male .",request
B1m1clFlM_50,12.2 Typo : Page 5 : a_0 sine die,fact
B1m1clFlM_51,12.3 Page 7 - omit results that are not shown,fact
B1m1clFlM_52,"12.4 Make Figures larger - it is difficult , if not impossible to see",request
B1m1clFlM_53,12.5 What is the difference between Pac-Boy and Ms. Pacman task ? And why not use Ms. Packman ?,non-arg
S1omqSUSM_0,This paper proposes to use a hybrid of convolutional and recurrent networks to predict the DSL specification of a GUI given a screenshot of the GUI .,fact
S1omqSUSM_1,Pros : The paper is clear,evaluation
S1omqSUSM_2,and the proposed problem is novel and well-defined .,evaluation
S1omqSUSM_3,"The training data is synthetic , allowing for arbitrarily large training sets to be generated .",fact
S1omqSUSM_4,The authors have made their synthetic dataset publicly available .,fact
S1omqSUSM_5,The method seems to work well based on the samples and ROC curves presented .,evaluation
S1omqSUSM_6,"Cons : This is mostly an application of an existing method to a new domain -- as stated in the related work section , effectively the same convnet + RNN architecture has been in common use for image captioning and other vision applications .",fact
S1omqSUSM_7,The UIs that are represented in the dataset seem quite simple ;,evaluation
S1omqSUSM_8,it ’s not clear that this will transfer to arbitrarily complex and multi-page UIs .,evaluation
S1omqSUSM_9,The main motivation for the proposed system seems to be for non-technical designers to be able to implement UIs just by drawing a mockup screenshot .,fact
S1omqSUSM_10,"However , the paper has n’t shown that this is necessarily possible assuming the hand-designed mockups are n’t pixel-for-pixel matches with a screenshot that could be generated by the “ DSL code - > screenshot ” mapping that this system learns to invert .",fact
S1omqSUSM_11,There exist a number of “ drag and drop ” style UI design products ( at least for HTML ) that would seem to accomplish the same basic goal as the proposed system in a more reliable way .,fact
S1omqSUSM_12,"( Though the proposed system does have the advantage of only requiring a screenshot created using any software , rather than being restricted to a particular piece of software . )",fact
S1omqSUSM_13,"Overall , the paper is well-written but the novelty and applicability seems a bit limited .",evaluation
SJyXoTtlG_0,This paper introduces a generative approach for 3D point clouds .,fact
SJyXoTtlG_1,"More specifically , two Generative Adversarial approaches are introduced : Raw point cloud GAN , and Latent-space GAN ( r-GAN and l-GAN as referred to in the paper ) .",fact
SJyXoTtlG_2,"In addition , a GMM sampling + GAN decoder approach to generation is also among the experimented variations .",fact
SJyXoTtlG_3,"The results look convincing for the generation experiments in the paper , both from class-specific ( Figure 1 ) and multi-class generators ( Figure 6 ) .",evaluation
SJyXoTtlG_4,The quantitative results also support the visuals .,fact
SJyXoTtlG_5,One question that arises is whether the point cloud approaches to generation is any more valuable compared to voxel-grid based approaches .,request
SJyXoTtlG_6,"Especially Octree based approaches [ 1-below ] show very convincing and high-resolution shape generation results ,",evaluation
SJyXoTtlG_7,whereas the details seem to be washed out for the point cloud results presented in this paper .,fact
SJyXoTtlG_8,I would like to see comparison experiments with voxel based approaches in the next update for the paper .,request
SJyXoTtlG_9,[1] <CIT>,reference
HJyXsRtef_0,"This paper presents a new approach to determining what to measure and when to measure it , using a novel deep learning architecture .",fact
HJyXsRtef_1,The problem addressed is important and timely,evaluation
HJyXsRtef_2,and advances here may have an impact on many application areas outside medicine .,evaluation
HJyXsRtef_3,The approach is evaluated on real-world medical datasets and has increased accuracy over the other methods compared against .,fact
HJyXsRtef_4,"+ A key advantage of the approach is that it continually learns from the collected data , using new measurements to update the model , and that it runs efficiently even on large real-world datasets .",fact
HJyXsRtef_5,"- However , the related work section is significantly underdeveloped , making it difficult to really compare the approach to the state of the art .",evaluation
HJyXsRtef_6,"The paper is ambitious and claims to address a variety of problems ,",evaluation
HJyXsRtef_7,but as a result each segment of related work seems to have been shortchanged .,evaluation
HJyXsRtef_8,"In particular , the section on missing data is missing a large amount of recent and related work .",fact
HJyXsRtef_9,"Normally , methods for handling missing data are categorized based on the missingness model ( MAR/MCAR/MNAR ) .",fact
HJyXsRtef_10,"The paper seems to assume all data are missing at random , which is also a significant limitation of the methods .",evaluation
HJyXsRtef_11,"- The paper is organized in a nonstandard way , with the methods split across two sections , separated by the related work .",fact
HJyXsRtef_12,It would be easier to follow with a more common intro/related work/methods structure .,request
HJyXsRtef_13,Questions : - One of the key motivations for the approach is sensing in medicine .,fact
HJyXsRtef_14,"However , many tests come as a group ( e.g. the chem-7 or other panels ) .",fact
HJyXsRtef_15,"In this case , even if the only desired measurement is glucose , others will be included as well .",fact
HJyXsRtef_16,Is it possible to incorporate this ?,request
HJyXsRtef_17,"It may change the threshold for the decision , as a combination of measures can be obtained for the same cost .",evaluation
H1Pyl4sxM_0,"Summary of paper : The paper proposes an RNN-based neural network architecture for embedding programs , focusing on the semantics of the program rather than the syntax .",fact
H1Pyl4sxM_1,The application is to predict errors made by students on programming tasks .,fact
H1Pyl4sxM_2,This is achieved by creating training data based on program traces obtained by instrumenting the program by adding print statements .,fact
H1Pyl4sxM_3,"The neural network is trained using this program traces with an objective for classifying the student error pattern ( e.g. list indexing , branching conditions , looping bounds ) .",fact
H1Pyl4sxM_4,--- Quality : The experiments compare the three proposed neural network architectures with two syntax-based architectures .,fact
H1Pyl4sxM_5,It would be good to see a comparison with some techniques from Reed & De Freitas ( 2015 ),request
H1Pyl4sxM_6,as this work also focuses on semantics-based embeddings .,fact
H1Pyl4sxM_7,Clarity : The paper is clearly written .,evaluation
H1Pyl4sxM_8,Originality : This work does n't seem that original from an algorithmic point of view,evaluation
H1Pyl4sxM_9,since Reed & De Freitas ( 2015 ) and Cai et . al ( 2017 ) among others have considered using execution traces .,fact
H1Pyl4sxM_10,However the application to program repair is novel ( as far as I know ) .,evaluation
H1Pyl4sxM_11,Significance : This work can be very useful for an educational platform,evaluation
H1Pyl4sxM_12,though a limitation is the need for adding instrumentation print statements by hand .,request
H1Pyl4sxM_13,--- Some questions/comments : - Do we need to add the print statements for any new programs that the students submit ?,request
H1Pyl4sxM_14,What if the structure of the submitted program does n't match the structure of the intended solution and hence adding print statements can not be automated ?,request
H1Pyl4sxM_15,"--- References Cai , J. , Shin , R. , & Song , D. ( 2017 ) . Making Neural Programming Architectures Generalize via Recursion . In International Conference on Learning Representations ( ICLR ) .",reference
HkeBFwYgf_0,This paper introduces a new toolbox for deep neural networks learning and evaluation .,fact
HkeBFwYgf_1,The central idea is to include time in the processing of all the units in the network .,fact
HkeBFwYgf_2,"For this , the authors propose a paradigm switch : form layerwise-sequential networks , where at every time frame the network is evaluated by updating each layer – from bottom to top – sequentially ; to layerwise-parallel networks , where all the neurons are updated in parallel .",fact
HkeBFwYgf_3,The new paradigm implies that the layer update is achieved by using the stored previous state and the corresponding previous state of the previous layer .,evaluation
HkeBFwYgf_4,This has three consequences .,fact
HkeBFwYgf_5,"First , every layer now use memory ,",fact
HkeBFwYgf_6,a condition that already applies for RNNs in layerwise-sequential networks .,fact
HkeBFwYgf_7,"Second , in order to have a consistent output , the information has to flow in the network for a number of time frames equal to the number of layers .",fact
HkeBFwYgf_8,"In Neuroscience , this concept is known as reaction time .",fact
HkeBFwYgf_9,"Third , since the network is not synchronized in terms of the information that is processed in a specific time frame , there are discrepancies w.r.t. the layerwise-sequential networks computation : all the techniques used to train deep NNs have to be reconsidered .",fact
HkeBFwYgf_10,"Overall , the concept is interesting and timely especially for the rising field of spiking neural networks or for large and distributed architectures .",evaluation
HkeBFwYgf_11,"The paper , however , should probably provide more examples and results in terms of architectures that can been implemented with the toolbox in comparison with other toolboxes .",request
HkeBFwYgf_12,The paper presents a single example in which either the accuracy and the training time are not reported .,fact
HkeBFwYgf_13,"While I understand that the main result of this work is the toolbox itself , more examples and results would improve the clarity and the implications for such paradigm switch .",request
HkeBFwYgf_14,"Another concern comes from the choice to use Theano as back-end ,",fact
HkeBFwYgf_15,since it 's known that it is going to be discontinued .,fact
HkeBFwYgf_16,"Finally I suggest to improve the clarity and description of Figure 2 ,",request
HkeBFwYgf_17,which is messy and confusing especially if printed in B&W .,evaluation
rk_xMk8ef_0,Summary This paper presents a dataset of mathematical equations and applies TreeLSTMs to two tasks : verifying and completing mathematical equations .,fact
rk_xMk8ef_1,"For these tasks , TreeLSTMs outperform TreeNNs and RNNs .",fact
rk_xMk8ef_2,"In my opinion , the main contribution of this paper is this potentially useful dataset , as well as an interesting way of representing fixed-precision floats .",fact
rk_xMk8ef_3,"However , the application of TreeNNs and TreeLSTMs is rather straight-forward ,",evaluation
rk_xMk8ef_4,so in my ( subjective ) view there are only a few insights salvageable for the ICLR community,evaluation
rk_xMk8ef_5,and compared to Allamanis et al. ( 2017 ) this paper is a rather incremental extension .,evaluation
rk_xMk8ef_6,Strengths The authors present a new datasets for mathematical identities .,fact
rk_xMk8ef_7,The method for generating additional correct identities could be useful for future research in this area .,evaluation
rk_xMk8ef_8,I find the representation of fixed-precision floats presented in this paper intriguing .,evaluation
rk_xMk8ef_9,I believe this contribution should be emphasized more,request
rk_xMk8ef_10,as it allows the model to generalize to unseen numbers,fact
rk_xMk8ef_11,and I am wondering whether the authors see some wider application of this representation for neural programming models .,request
rk_xMk8ef_12,I liked the categorization of the related work .,evaluation
rk_xMk8ef_13,"Weaknesses p2 : It is mentioned that the framework is the first to combine symbolic expressions with black-box function evaluations ,",fact
rk_xMk8ef_14,but I would argue that Neural Programmer-Interpreters ( NPI ; Reed & De Freitas ) are already doing that,fact
rk_xMk8ef_15,"( see Fig 1 in that paper where the execution trace is a symbolic expression and some expressions "" Act ( LEFT ) "" are black-box function applications directly changing the image ) .",fact
rk_xMk8ef_16,The differences to Allamanis et al. ( 2017 ) are not worked out well .,evaluation
rk_xMk8ef_17,"For instance , the authors use the TreeNN model from that paper as a baseline",fact
rk_xMk8ef_18,but the EqNet model is not mentioned at all .,fact
rk_xMk8ef_19,The obvious question is whether EqNets can be applied to the two tasks ( verifying and completing mathematical equations ) and if so why this has not been done .,request
rk_xMk8ef_20,The contribution regarding black box function application is unclear to me .,evaluation
rk_xMk8ef_21,"On page 6 , it is unclear to me what "" handles [ … ] function evaluation expressions "" .",evaluation
rk_xMk8ef_22,"As far as I understand , the TreeLSTM learns to the return value of function evaluation expressions in order to predict equality of equations ,",fact
rk_xMk8ef_23,but this should be clarified .,request
rk_xMk8ef_24,"I find the connection of the proposed model and task to "" neural programming "" weak .",evaluation
rk_xMk8ef_25,"For instance , as far as I understand there is no support for stateful programs .",fact
rk_xMk8ef_26,"Furthermore , it would be interesting to hear how this work can be applied to existing programming languages such as Haskell .",request
rk_xMk8ef_27,What are the limitations of the architecture ?,request
rk_xMk8ef_28,Could it learn to identify equality of two lists in Haskell ?,request
rk_xMk8ef_29,p6 : The paragraph on baseline models is rather uninformative .,evaluation
rk_xMk8ef_30,TreeLSTMs have been shown to outperform Tree NN 's in various prior work .,fact
rk_xMk8ef_31,"The statement that "" LSTM cell [ … ] helps the model to have a better understanding of the underlying functions in the domain "" is vague .",evaluation
rk_xMk8ef_32,LSTM cells compared to fully-connected layers in Tree NNs ameliorate vanishing and exploding gradients along paths in the tree .,fact
rk_xMk8ef_33,"Furthermore , I would like to see a qualitative analysis of the reasoning capabilities that are mentioned here .",request
rk_xMk8ef_34,Did you observe any systematic differences in the ~ 4 % of equations where the TreeLSTM fails to generalize ( Table 3 ; first column ) .,request
rk_xMk8ef_35,"Minor Comments Abstract : "" Our framework generalizes significantly better "" I think it would be good to already mention in comparison to what this statement is .",request
rk_xMk8ef_36,"p1 : "" aim to solve tasks such as learn mathematical "" - > "" aim to solve tasks such as learning mathematical """,request
rk_xMk8ef_37,"p2 : You could add a citation for Theano , Tensorflow and Mxnet .",request
rk_xMk8ef_38,p2 : Could you elaborate how equation completion is used in Mathematical Q&A ?,request
rk_xMk8ef_39,"p3 : Could you expand on "" mathematical equation verification and completion [ … ] has broader applicability "" by maybe giving some concrete examples .",request
rk_xMk8ef_40,p3 Eq . 5 : What precision do you consider ?,request
rk_xMk8ef_41,Two digits ?,non-arg
rk_xMk8ef_42,"p3 : "" division because that they can "" - > "" division because they can """,non-arg
rk_xMk8ef_43,p4 Fig. 1 : Is there a reason 1 is represented as 10 ^ 0 here ?,request
rk_xMk8ef_44,Do you need the distinction between 1 ( the integer ) and 1.0 ( the float ) ?,request
rk_xMk8ef_45,"p5 : "" we include set of changes "" - > "" we include the set of changes """,request
rk_xMk8ef_46,p5 : In my view there is enough space to move appendix A to section 2 .,request
rk_xMk8ef_47,"In addition , it would be great to see more examples of generated identities at this stage ( including negative ones ) .",request
rk_xMk8ef_48,"p5 : "" We generate all possible equations ( with high probability ) """,quote
rk_xMk8ef_49,– what is probabilistic about this ?,request
rk_xMk8ef_50,p5 : I do n't understand why function evaluation results in identities of depth 2 and 3 .,request
rk_xMk8ef_51,Is it both or one of them ?,request
rk_xMk8ef_52,"p6 : The modules "" symbol "" and "" number "" are not shown in the figure .",fact
rk_xMk8ef_53,I assume they refer to projections using Wsymb and Wnum ?,request
rk_xMk8ef_54,"p6 : "" tree structures neural networks "" - > "" tree structured neural networks """,request
rk_xMk8ef_55,p6 : A reference for the ADAM optimizer should be added .,request
rk_xMk8ef_56,p6 : Which method was used for optimizing these hyperparameters ?,request
rk_xMk8ef_57,"If a grid search was used , what intervals were used ?",request
rk_xMk8ef_58,"p7 : "" the superiority of Tree LSTM to Tree NN shows that is important to incorporate cells that have memory "" is not a novel insight .",evaluation
rk_xMk8ef_59,"p8 : When you mention "" you give this set of equations to the models look at the top k predictions "" I assume you ranked the substituted equations by the probability that the respective model assigns to it ?",non-arg
rk_xMk8ef_60,"p8 : Do you have an intuition why prediction function evaluations for "" cos "" seem to plateau certain points ?",non-arg
rk_xMk8ef_61,"Furthermore , it would be interesting to see what effect the choice of non-linearity on the output of the TreeLSTM has on how accurately it can learn to evaluate functions .",request
rk_xMk8ef_62,"For instance , one could replace the tanh with cos and might expect that the model has now an easy time to learn to evaluate <VAR> .",evaluation
rk_xMk8ef_63,"p8 Fig 4b ; p9 : Relating to the question regarding plateaus in the function evaluation : "" in Figure 4b [ … ] the top prediction ( 0.28 ) is the correct value for tan with precision 2 , but even other predictions are quite close "" – they are all the same and this bad , right ?",request
rk_xMk8ef_64,"p9 : "" of the state-of-the-art neural reasoning systems "" is very broad and in my opinion misleading too .",evaluation
rk_xMk8ef_65,"First , there are other reasoning tasks ( machine reading/Q & A , Visual Q&A , knowledge base inference etc. ) too",fact
rk_xMk8ef_66,and it is not obvious how ideas from this paper translate to these domains .,evaluation
rk_xMk8ef_67,"Second , for other tasks TreeLSTMs are likely not state-of-the-art",evaluation
rk_xMk8ef_68,( see for example models on the SQuAD leaderboard : <URL> ) .,reference
rk_xMk8ef_69,"p9 : "" exploring recent neural models that explicitly use memory cells """,quote
rk_xMk8ef_70,– I think what you mean is models with addressable differentiable memory .,fact
r1Q8qCdgf_0,The authors investigate different message passing schedules for GNN learning .,fact
r1Q8qCdgf_1,"Their proposed approach is to partition the graph into disjoint subregions , pass many messages on the sub regions and pass fewer messages between regions ( an approach that is already considered in related literature , e.g. , the BP literature ) , with the goal of minimizing the number of messages that need to be passed to convey information between all pairs of nodes in the network .",fact
r1Q8qCdgf_2,"Experimentally , the proposed approach seems to perform comparably to existing methods ( or slightly worse on average in some settings ) .",fact
r1Q8qCdgf_3,The paper is well-written and easy to read .,evaluation
r1Q8qCdgf_4,My primary concern is with novelty .,evaluation
r1Q8qCdgf_5,Many similar ideas have been floating around in a variety of different message-passing communities .,evaluation
r1Q8qCdgf_6,"With no theoretical reason to prefer the proposed approach , it seems like it may be of limited interest to the community if speed is its only benefit ( see detailed comments below ) .",evaluation
r1Q8qCdgf_7,"Specific comments : 1 ) "" When information from any one node has reached all other nodes in the graph for the first time , this problem is considered as solved . """,quote
r1Q8qCdgf_8,"Perhaps it is my misunderstanding of the way in which GNNs work , but is n't the objective actually to reach a set of fixed point equations .",fact
r1Q8qCdgf_9,"If so , then simply propagating information from one side of the graph may not be sufficient .",fact
r1Q8qCdgf_10,2 ) The experimental results in Section 4.4 are almost impossible to interpret .,evaluation
r1Q8qCdgf_11,Perhaps it is better to plot number of edges updated versus accuracy ?,request
r1Q8qCdgf_12,This at least would put them on equal footing .,fact
r1Q8qCdgf_13,"In addition , the experiments that use randomness should be repeated and plotted on average ( just in case you happened to pick a bad schedule ) .",request
r1Q8qCdgf_14,"3 ) More generally , why not consider random schedules ( i.e. , just pick a random edge , update , repeat ) or random partitions ?",request
r1Q8qCdgf_15,"I 'm not certain that a fixed set will perform best independent of the types of updates being considered , and random schedules , like the fully synchronous case for an important baseline ( especially if update speed is all you care about ) .",evaluation
r1Q8qCdgf_16,"Typos : - pg . 6 , "" Thm . 2 "" - > "" Table 2 """,request
HkBIjt2xz_0,Summary : This paper presents a derivation which links a DNN to recursive application of maximum entropy model fitting .,fact
HkBIjt2xz_1,"The mathematical notation is unclear ,",evaluation
HkBIjt2xz_2,and in one cases the lemmas are circular ( i.e. two lemmas each assume the other is correct for their proof ) .,fact
HkBIjt2xz_3,"Additionally the main theorem requires complete independence ,",fact
HkBIjt2xz_4,"but the second theorem provides pairwise independence ,",fact
HkBIjt2xz_5,and the two are not the same .,fact
HkBIjt2xz_6,Major comments : - The second condition of the maximum entropy equivalence theorem requires that all T are conditionally independent of Y.,fact
HkBIjt2xz_7,"This statement is unclear ,",evaluation
HkBIjt2xz_8,"as it could mean pairwise independence , or it could mean jointly independent ( i.e. for all pairs of non-overlapping subsets A & B of <EQN> ) .",fact
HkBIjt2xz_9,"This is the same as saying the mapping <EQN> is making each dimension of T orthogonal , as otherwise it would introduce correlations .",fact
HkBIjt2xz_10,The proof of the theorem assumes that pairwise independence induces joint independence,fact
HkBIjt2xz_11,and this is not correct .,fact
HkBIjt2xz_12,"- Section 4.1 makes an analogy to EM ,",fact
HkBIjt2xz_13,"but gradient descent is not like this process as all the parameters are updated at once , and only optimised by a single ( noisy ) step .",fact
HkBIjt2xz_14,"The optimisation with respect to a single layer is conditional on all the other layers remaining fixed ,",fact
HkBIjt2xz_15,but the gradient information is stale,fact
HkBIjt2xz_16,( as it knows about the previous step of the parameters in the layer above ) .,fact
HkBIjt2xz_17,"This means that gradient descent does all 1 . . L steps in parallel ,",fact
HkBIjt2xz_18,and this is different to the definition given .,fact
HkBIjt2xz_19,"- The proofs in Appendix C which are used for the statement <EQN> are incomplete ,",evaluation
HkBIjt2xz_20,"and in generate this statement is not true ,",fact
HkBIjt2xz_21,so requires proof .,request
HkBIjt2xz_22,"- Lemma 1 appears to assume Lemma 2 , and Lemma 2 appears to assume Lemma 1 .",fact
HkBIjt2xz_23,Either these lemmas are circular or the derivations of both of them are unclear .,evaluation
HkBIjt2xz_24,- In Lemma 3 what is the minimum taken over for the left hand side ?,request
HkBIjt2xz_25,"Elsewhere the minimum is taken over T , but T does not appear on the left hand side .",fact
HkBIjt2xz_26,"Explicit minimums help the reader to follow the logic ,",evaluation
HkBIjt2xz_27,and implicit ones should only be used when it is obvious what the minimum is over .,evaluation
HkBIjt2xz_28,"- In Lemma 5 , what does "" T is only related to X "" mean ?",request
HkBIjt2xz_29,"The proof states that <EQN> forms a Markov chain ,",fact
HkBIjt2xz_30,"but this implies that T is a function of Y , not X.",fact
HkBIjt2xz_31,"Minor comments : - I assume that the <VAR> notation is the expectation of that probability distribution ,",fact
HkBIjt2xz_32,"but this notation is uncommon ,",evaluation
HkBIjt2xz_33,and should be replaced with a more explicit one .,request
HkBIjt2xz_34,"- Markov is usually romanized with a "" k "" not a "" c "" .",evaluation
HkBIjt2xz_35,"- The paper is missing numerous prepositions and articles ,",evaluation
HkBIjt2xz_36,and contains multiple spelling mistakes & typos .,fact
BynUQBQZM_0,"This paper proposes a regularization to the softmax layer , which try to make the distribution of feature representation ( inputs fed to the softmax layer ) more meaningful according to the Euclidean distance .",fact
BynUQBQZM_1,"The proposed isotropic loss in equation 3 tries to equalize the squared distances from each point to the mean ,",fact
BynUQBQZM_2,so the features are encouraged to lie close to a sphere .,fact
BynUQBQZM_3,"Overall , the proposed method is a relatively simple tweak to softmax .",evaluation
BynUQBQZM_4,"The authors show that empirically , features learned under softmax loss + isotropic regularization outperforms other features in Euclidean metric-based tasks .",fact
BynUQBQZM_5,My main concern with this paper is the motivation :,evaluation
BynUQBQZM_6,what are the practical scenarios in which one would want to used proposed method ?,non-arg
BynUQBQZM_7,"1 . It is true that features learned with the pure softmax loss may not presents the ideal similarity under the Euclidean metric ( e.g. the problem depicted in Figure 1 ) ,",fact
BynUQBQZM_8,because they are not trained to do so :,fact
BynUQBQZM_9,their purpose is just to predict the correct label .,fact
BynUQBQZM_10,"While the proposed regularization does lead to a nicer Euclidean geometry ,",evaluation
BynUQBQZM_11,there is not sufficient motivation and evidence showing this regularization improves classification accuracy .,evaluation
BynUQBQZM_12,"2 . In table 2 , the authors seem to indicate that not using the label information in the definition of Isotropic loss is an advantage .",fact
BynUQBQZM_13,But this does not matter,fact
BynUQBQZM_14,since you already use the labels in the softmax loss .,fact
BynUQBQZM_15,"3 . I can not easily think of scenarios in which , we would like to perform KNN in the feature space ( Table 3 ) after training a softmax layer .",evaluation
BynUQBQZM_16,"In fact , Table 3 shows KNN is almost always worse than softmax in terms of classification accuracy .",fact
BynUQBQZM_17,"4 . Running kmeans or agglomerative clustering in the feature space ( Table 5 ) * using the Euclidean metric * is again ill-posed ,",evaluation
BynUQBQZM_18,because the softmax layer is not trained to do this .,fact
BynUQBQZM_19,"If one really wants good clustering performance , one shall always try to learn a good metric , or ,",fact
BynUQBQZM_20,why do not you perform clustering on the softmax output ( a probability vector ? ),evaluation
BynUQBQZM_21,"5 . The experiments on adversarial robustness and face verification seems more interesting to me ,",evaluation
BynUQBQZM_22,but the tasks were not carefully explained for someone not familiar with that literature .,evaluation
BynUQBQZM_23,"Perhaps for these tasks , multi-class classification is not the most correct objective , and maybe the proposed regularization can help ,",evaluation
BynUQBQZM_24,but the motivations are not given .,fact
rySfFbFgz_0,"In this paper , the authors propose a novel tracking loss to convert the RPN to a tracker .",fact
rySfFbFgz_1,The internal structure of top layer features of RPN is exploited to treat feature points discriminatively .,fact
rySfFbFgz_2,"In addition , the proposed compression network speeds up the tracking algorithm .",fact
rySfFbFgz_3,The experimental results on the VOT2016 dataset demonstrate its efficiency in tracking .,fact
rySfFbFgz_4,This work is the combination of Faster R-CNN ( Ren et al . PAMI 2015 ) and tracking-by-detection framework .,fact
rySfFbFgz_5,"The main contributions proposed in this paper are new tracking loss , network compression and results .",fact
rySfFbFgz_6,There are numerous concerns with this work :,evaluation
rySfFbFgz_7,1 . The new tracking loss shown in equation 2 is similar with the original Faster R-CNN loss shown in equation 1 .,evaluation
rySfFbFgz_8,"The only difference is to replace the regression loss with a predefined mask selection loss ,",fact
rySfFbFgz_9,which is of little sense that the feature processing can be further fulfilled through one-layer CNN .,evaluation
rySfFbFgz_10,The empirical operation shown in figure 2 seems arbitrary and lack of theoretical explanation .,evaluation
rySfFbFgz_11,There is no insight of why doing so .,fact
rySfFbFgz_12,"Simply showing the numbers in table 1 does not imply the necessity ,",fact
rySfFbFgz_13,which ought to be put in the experiment sections .,request
rySfFbFgz_14,2 . The network compression is engineering and lack insight as well .,evaluation
rySfFbFgz_15,To remove part of the CNN and retrain is a common strategy in the CNN compression methods [ a ] [ b ] .,evaluation
rySfFbFgz_16,There is a lack of discussion with the relationship with prior arts .,fact
rySfFbFgz_17,3 . The organization is not clear .,evaluation
rySfFbFgz_18,Section 3.4 should be set in the experiments,request
rySfFbFgz_19,and Section 3.5 should be set at the beginning of the algorithm .,request
rySfFbFgz_20,"The description of the network compression is not clear enough , especially the training details .",evaluation
rySfFbFgz_21,"Meanwhile , the presentation is hard to follow .",evaluation
rySfFbFgz_22,There is no clear expression of how the tracker performs in practice .,evaluation
rySfFbFgz_23,"4 . In addition , VOT 2016 , the method should evaluate on the OTB dataset with the following trackers [ c ] [ d ] .",request
rySfFbFgz_24,5 . The evaluation is not fair .,evaluation
rySfFbFgz_25,"In Sec 6 , the authors indicate that MDNet runs at 1 FPS while the proposed tracker runs at 1.6 FPS .",fact
rySfFbFgz_26,"However , MDNet is based on Matlab",fact
rySfFbFgz_27,"and the proposed tracker is based on C++ ( i.e. , Caffe ) .",fact
rySfFbFgz_28,Reference : [a] <CIT>,reference
rySfFbFgz_29,[b] <CIT>,reference
rySfFbFgz_30,[c] <CIT>,reference
rySfFbFgz_31,[d] <CIT>,reference
BkOfh_eWM_0,The paper discusses the problem of optimizing neural networks with hard threshold and proposes a novel solution to it .,fact
BkOfh_eWM_1,The problem is of significance because in many applications one requires deep networks which uses reduced computation and limited energy .,evaluation
BkOfh_eWM_2,The authors frame the problem of optimizing such networks to fit the training data as a convex combinatorial problems .,fact
BkOfh_eWM_3,"However since the complexity of such a problem is exponential , the authors propose a collection of heuristics/approximations to solve the problem .",fact
BkOfh_eWM_4,"These include , a heuristic for setting the targets at each layer , using a soft hinge loss , mini-batch training and such .",fact
BkOfh_eWM_5,Using these modifications the authors propose an algorithm ( Algorithm 2 in appendix ) to train such models efficiently .,fact
BkOfh_eWM_6,"They compare the performance of a bunch of models trained by their algorithm against the ones trained using straight-through-estimator ( SSTE ) on a couple of datasets , namely , CIFAR-10 and ImageNet .",fact
BkOfh_eWM_7,They show superiority of their algorithm over SSTE .,fact
BkOfh_eWM_8,I thought the paper is very well written and provides a really nice exposition of the problem of training deep networks with hard thresholds .,evaluation
BkOfh_eWM_9,The authors formulation of the problem as one of combinatorial optimization,fact
BkOfh_eWM_10,and proposing Algorithm 1 is also quite interesting .,evaluation
BkOfh_eWM_11,The results are moderately convincing in favor of the proposed approach .,evaluation
BkOfh_eWM_12,Though a disclaimer here is that I 'm not 100 % sure that SSTE is the state of the art for this problem .,evaluation
BkOfh_eWM_13,Overall i like the originality of the paper and feel that it has a potential of reasonable impact within the research community .,evaluation
BkOfh_eWM_14,"There are a few flaws/weaknesses in the paper though , making it somewhat lose .",evaluation
BkOfh_eWM_15,- The authors start of by posing the problem as a clean combinatorial optimization problem and propose Algorithm 1 .,fact
BkOfh_eWM_16,"Realizing the limitations of the proposed algorithm , given the assumptions under which it was conceived in ,",evaluation
BkOfh_eWM_17,the authors relax those assumptions in the couple of paragraphs before section 3.1,fact
BkOfh_eWM_18,"and pretty much throw away all the nice guarantees , such as checks for feasibility , discussed earlier .",fact
BkOfh_eWM_19,"- The result of this is another algorithm ( I guess the main result of the paper ) , which is strangely presented in the appendix as opposed to the main text , which has no such guarantees .",fact
BkOfh_eWM_20,"- There is no theoretical proof that the heuristic for setting the target is a good one , other than a rough intuition",fact
BkOfh_eWM_21,- The authors do not discuss at all the impact on generalization ability of the model trained using the proposed approach .,fact
BkOfh_eWM_22,The entire discussion revolves around fitting the training set and somehow magically everything seem to generalize and not overfit .,evaluation
r1ajkbceM_0,The authors present a new RL algorithm for sparse reward tasks .,fact
r1ajkbceM_1,"The work is fairly novel in its approach ,",evaluation
r1ajkbceM_2,combining a learned reward estimator with a contextual bandit algorithm for exploration/exploitation .,fact
r1ajkbceM_3,"The paper was mostly clear in its exposition ,",evaluation
r1ajkbceM_4,however some additional information of the motivation for why the said reduction is better than simpler alternatives would help .,request
r1ajkbceM_5,Pros 1 . The results on bandit structured prediction problems are pretty good,evaluation
r1ajkbceM_6,"2 . The idea of a learnt credit assignment function , and using that to separate credit assignment from the exploration/exploitation tradeoff is good .",evaluation
r1ajkbceM_7,"Cons : 1 . The method seems fairly more complicated than PPO / A2C ,",evaluation
r1ajkbceM_8,yet those methods seem to perform equally well on the RL problems ( Figure 2 . ) .,fact
r1ajkbceM_9,It also seems to be designed only for discrete action spaces .,fact
r1ajkbceM_10,"2 . Reslope Boltzmann performs much worse than Reslope Bootstrap ,",fact
r1ajkbceM_11,thus having a bag of policies helps .,fact
r1ajkbceM_12,"However , in the comparison in Figures 2 and 3 , the policy gradient methods dont have the advantage of using a bag of policies .",fact
r1ajkbceM_13,A fairer comparison would be to compare with methods that use ensembles of Q-functions .,request
r1ajkbceM_14,( like this <URL> by Chen et al. ) .,reference
r1ajkbceM_15,The Q learning methods in general would also have better sample efficiency than the policy gradient methods .,fact
r1ajkbceM_16,"3 . The method claims to learn an internal representation of a denser reward function for the sparse reward problem ,",fact
r1ajkbceM_17,however the experimental analysis of this is pretty limited ( Section 5.3 ) .,evaluation
r1ajkbceM_18,It would be useful to do a more thorough investigation of whether it learnt a good credit assignment function in the games .,request
r1ajkbceM_19,"One way to do this would be to check the qualitative aspects of the function in a well understood game , like Blackjack .",request
r1ajkbceM_20,"Suggestions : 1 . What is the advantage of the method over a simple RL method that predicts a reward at every step ( such that the dense rewards add up to match the sparse reward for the episode ) , and uses this predicted dense reward to perform RL ?",non-arg
r1ajkbceM_21,"This , and also a bigger discussion on prior bandit learning methods like LOLS will help under the context for why we \ u2019re performing the reduction stated in the paper .",request
r1ajkbceM_22,"Significance : While the method is novel and interesting , the experimental analysis and the explanations in the paper leave it unclear as to whether its significant compared to prior work .",evaluation
ryD53e9xG_0,This work addresses the scenario of fine-tuning a pre-trained network for new data/tasks and empirically studies various regularization techniques .,fact
ryD53e9xG_1,"Overall , the evaluation concludes with recommending that all layers of a network whose weights are directly transferred during fine-tuning should be regularized against the initial net with an L2 penalty during further training .",fact
ryD53e9xG_2,Relationship to prior work : Regularizing a target model against a source model is not a new idea .,fact
ryD53e9xG_3,"The authors miss key connections to A-SVM [ 1 ] and PMT-SVM [ 2 ] -- two proposed transfer learning models applied to SVM weights , but otherwise very much the same as the proposed solution in this paper .",fact
ryD53e9xG_4,"Though the study here may offer new insights for deep nets ,",fact
ryD53e9xG_5,it is critical to mention prior work which also does analysis of these regularization techniques .,request
ryD53e9xG_6,"Significance : As the majority of visual recognition problems are currently solved using variants of fine-tuning ,",fact
ryD53e9xG_7,"if the findings reported in this paper generalize , then it could present a simple new regularization which improves the training of new models .",evaluation
ryD53e9xG_8,The change is both conceptually simple and easy to implement so could be quickly integrated by many people .,evaluation
ryD53e9xG_9,"Clarity and Questions : The purpose of the paper is clear ,",evaluation
ryD53e9xG_10,"however , some questions remain unanswered .",evaluation
ryD53e9xG_11,1 ) How is the regularization weight of 0.01 chosen ?,request
ryD53e9xG_12,This is likely a critical parameter .,evaluation
ryD53e9xG_13,"In an experimental paper , I would expect to see a plot of performance for at least one experiment as this regularization weighting parameter is varied .",request
ryD53e9xG_14,2 ) How does the use of L2 regularization on the last layer effect the regularization choice of other layers ?,request
ryD53e9xG_15,What happens if you use no regularization on the last layer ?,request
ryD53e9xG_16,L1 regularization ?,request
ryD53e9xG_17,3 ) Figure 1 is difficult to read .,evaluation
ryD53e9xG_18,Please at least label the test sets on each sub-graph .,request
ryD53e9xG_19,4 ) There seems to be some issue with the freezing experiment in Figure 2 .,evaluation
ryD53e9xG_20,"Why does performance of L2 regularization improve as you freeze more and more layers , but is outperformed by un-freezing all .",request
ryD53e9xG_21,5 ) Figure 3 and the discussion of linear dependence with the original model in general seems does not add much to the paper .,evaluation
ryD53e9xG_22,It is clear that regularizing against the source model weights instead of 0 should result in final weights that are more similar to the initial source weights .,evaluation
ryD53e9xG_23,I would rather the authors use this space to provide a deeper analysis of why this property should help performance .,request
ryD53e9xG_24,6 ) Initializing with a source model offers a strong starting point so full from scratch learning is n’t necessary -- meaning fewer examples are needed for the continued learning ( fine-tuning ) phase .,fact
ryD53e9xG_25,"In a similar line of reasoning , does regularizing against the source further reduce the number of labeled points needed for fine-tuning ?",request
ryD53e9xG_26,Can you recover L2 fine-tuning performance with fewer examples when you use L2-SP ?,request
ryD53e9xG_27,[1] <CIT>,reference
ryD53e9xG_28,[2] <CIT>,reference
BynVEQJGM_0,This paper considers the problem of autonomous lane changing for self-driving cars in multi-lane multi-agent slot car setting .,fact
BynVEQJGM_1,The authors propose a new learning strategy called Q-masking which couples well a defined low level controller with a high level tactical decision making policy .,fact
BynVEQJGM_2,"The authors rightly say that one of the skills an autonomous car must have is the ability to change lanes ,",evaluation
BynVEQJGM_3,however this task is not one of the most difficult for autonomous vehicles to achieve and this ability has already been implemented in real vehicles .,evaluation
BynVEQJGM_4,"Real vehicles also decouple wayfinding with local vehicle control , similar to the strategy employed here .",fact
BynVEQJGM_5,"To make a stronger case for this research being relevant to the real autonomous driving problem , the authors would need to compare their algorithm to a real algorithm and prove that it is more “ data efficient . ”",request
BynVEQJGM_6,This is a difficult comparison,evaluation
BynVEQJGM_7,"since the sensing strategies employed by real vehicles – LIDAR , computer vision , recorded , labeled real maps are vastly different from the slot car model proposed by the authors .",evaluation
BynVEQJGM_8,"In term of impact , this is a theoretical paper looking at optimizing a sandbox problem where the results may be one day applicable to the real autonomous driving case .",evaluation
BynVEQJGM_9,In this paper the authors investigate “ the use and place ” of deep reinforcement learning in solving the autonomous lane change problem they propose a framework that uses Q-learning to learn “ high level tactical decisions ” and introduce “ Q-masking ” a way of limiting the problem that the agent has to learn to force it to learn in a subspace of the Q-values .,evaluation
BynVEQJGM_10,"The authors claim that “ By relying on a controller for low-level decisions we are also able to completely eliminate collisions during training or testing , which makes it a possibility to perform training directly on real systems . ”",quote
BynVEQJGM_11,I am not sure what is meant by this since in this paper the authors never test their algorithm on real systems,evaluation
BynVEQJGM_12,and in real systems it is not possible to completely eliminate collisions .,fact
BynVEQJGM_13,"If it were , this would be a much sought breakthrough .",evaluation
BynVEQJGM_14,Additionally for their experiment authors use the SUMO top view driving simulator .,fact
BynVEQJGM_15,This choice makes their algorithm not currently relevant to most autonomous vehicles that use ego-centric sensing .,evaluation
BynVEQJGM_16,This paper presents a learning algorithm that can “ outperform a greedy baseline in terms of efficiency ” and “ humans driving the simulator in terms of safety and success ” within their top view driving game .,fact
BynVEQJGM_17,"The game can be programmed to have an “ n ” lane highway , where n could reasonable go up to five to represent larger highways .",fact
BynVEQJGM_18,The authors limit the problem by specifying that all simulated cars must operate between a preset minimum and maximum and follow a target ( random ) speed within these limits .,fact
BynVEQJGM_19,"Cars follow a fixed model of behavior , do not collide with each other and can not switch lanes .",fact
BynVEQJGM_20,"It is unclear if the simulator extends beyond a single straight section of highway , as shown in Figure 1 .",evaluation
BynVEQJGM_21,The agent is tasked with driving the ego-car down the n-lane highway and stopping at “ the exit ” in the right hand lane D km from the start position .,fact
BynVEQJGM_22,The authors use deep Q learning from Mnih et al 2015 to learn their optimal policy .,fact
BynVEQJGM_23,They use a sparse reward function of +10 for reaching the goal and -10 x ( lane difference from desired lane ) as a penalty for failure .,fact
BynVEQJGM_24,This simple reward function is possible because the authors do not require the ego car to obey speed limits or avoid collisions .,evaluation
BynVEQJGM_25,The authors limit what the car is able to do,fact
BynVEQJGM_26,– for example it is not allowed to take actions that would get it off the highway .,fact
BynVEQJGM_27,This makes the high level learning strategy more efficient,evaluation
BynVEQJGM_28,because it does not have to explore these possibilities ( Q-masking ) .,fact
BynVEQJGM_29,The authors claim that this limitation of the simulation is made valid by the ability of the low level controller to incorporate prior knowledge and perfectly limit these actions .,fact
BynVEQJGM_30,"In the real world , however , it is unlikely that any low level controller would be able to do this perfectly .",evaluation
BynVEQJGM_31,"In terms of evaluation , the authors do not compare their result against any other method .",fact
BynVEQJGM_32,"Instead , using only one set of test parameters , the authors compare their algorithm to a “ greedy baseline ” policy that is specified a “ always try to change lanes to the right until the lane is correct ” then it tries to go as fast as possible while obeying the speed limit and not colliding with any car in front .",fact
BynVEQJGM_33,It seems that baseline is additionally constrained vs the ego car due to the speed limit and the collision avoidance criteria and is not a fair comparison .,evaluation
BynVEQJGM_34,So given a fixed policy and these constraints it is not surprising that it underperforms the Q-masked Q-learning algorithm .,evaluation
BynVEQJGM_35,"With respect to the comparison vs. human operators of the car simulation , the human operators were not experts .",fact
BynVEQJGM_36,They were only given “ a few trials ” to learn how to operate the controls before the test .,fact
BynVEQJGM_37,"It was reported that the human participants “ did not feel comfortable ” with the low level controller on ,",fact
BynVEQJGM_38,possibly indicating that the user experience of controlling the car was less than ideal .,evaluation
BynVEQJGM_39,"With the low level controller off , collisions became possible .",evaluation
BynVEQJGM_40,It is possibly not a fair claim to say that human drivers were “ less safe ” but rather that it was difficult to play the game or control the car with the safety module on .,evaluation
BynVEQJGM_41,This could be seen as a game design issue .,evaluation
BynVEQJGM_42,It was not clear from this presentation how the human participants were rewarded for their performance .,evaluation
BynVEQJGM_43,"In more typical HCI experiments the gender distribution and ages ranges of participants are specified as well as how participants were recruited and how the game was motivated , including compensation ( reward ) are specified .",request
BynVEQJGM_44,"Overall , this paper presents an overly simplified game simulation with a weak experimental result .",evaluation
BJQD_I_eM_0,The paper proposes an analysis on different adaptive regularization techniques for deep transfer learning .,fact
BJQD_I_eM_1,Specifically it focuses on the use of an L2-SP condition that constraints the new parameters to be close to the ones previously learned when solving a source task .,fact
BJQD_I_eM_2,+ The paper is easy to read and well organized,evaluation
BJQD_I_eM_3,+ The advantage of the proposed regularization against the more standard L2 regularization is clearly visible from the experiments,evaluation
BJQD_I_eM_4,- The idea per se is not new :,evaluation
BJQD_I_eM_5,there is a list of shallow learning methods for transfer learning based on the same L2 regularization choice,fact
BJQD_I_eM_6,[ <CIT> ],reference
BJQD_I_eM_7,[ <CIT> ],reference
BJQD_I_eM_8,[ <CIT> ],reference
BJQD_I_eM_9,I believe this literature should be discussed in the related work section,request
BJQD_I_eM_10,"- It is true that the L2-SP-Fisher regularization was designed for life-long learning cases with a fixed task ,",fact
BJQD_I_eM_11,"however , this solution seems to work quite well in the proposed experimental settings .",evaluation
BJQD_I_eM_12,From my understanding L2-SP-Fisher can be considered the best competitor of L2-SP,evaluation
BJQD_I_eM_13,so I think the paper should dedicate more space to the analysis of their difference and similarities both from the theoretical and experimental point of view .,request
BJQD_I_eM_14,For instance : -- adding the L2-SP-Fisher results in table 2,request
BJQD_I_eM_15,-- repeating the experiments of figure 2 and figure 3 with L2-SP-Fisher,request
H1q18tjxM_0,This paper presents a nearest-neighbor based continuous control policy .,fact
H1q18tjxM_1,"Two algorithms are presented : NN-1 runs open-loop trajectories from the beginning state ,",fact
H1q18tjxM_2,and NN-2 runs a state-condition policy that retrieves nearest state-action tuples for each state .,fact
H1q18tjxM_3,"The overall algorithm is very simple to implement and can do reasonably well on some simple control tasks ,",evaluation
H1q18tjxM_4,but quickly gets overwhelmed by higher-dimensional and stochastic environments .,evaluation
H1q18tjxM_5,"It is very similar to "" Learning to Steer on Winding Tracks Using Semi-Parametric Control Policies "" and is effectively an indirect form of tile coding ( each could be seen as a fixed voronoi cell ) .",evaluation
H1q18tjxM_6,I am sure this idea has been tried before in the 90s,fact
H1q18tjxM_7,but I am not familiar enough with all the literature to find it,non-arg
H1q18tjxM_8,"( A quick google search brings this up : Reinforcement Learning of Active Recognition Behaviors , with a chapter on nearest-neighbor lookup for policies : <URL> ) .",reference
H1q18tjxM_9,"Although I believe there is work to be done in the current round of RL research using nearest neighbor policies ,",evaluation
H1q18tjxM_10,I do n't believe this paper delves very far into pushing new ideas,evaluation
H1q18tjxM_11,"( even a simple adaptive distance metric could have provided some interesting results , nevermind doing a learned metric in a latent space to allow for rapid retrainig of a policy on new domains .... ) ,",request
H1q18tjxM_12,and for that reason I do n't think it has a place as a conference paper at ICLR .,evaluation
H1q18tjxM_13,I would suggest its submission to a workshop where it might have more use triggering discussion of further work in this area .,request
Hk2dO8ngz_0,This very well written paper covers the span between W-GAN and VAE .,fact
Hk2dO8ngz_1,"For a reviewer who is not an expert in the domain , it reads very well ,",evaluation
Hk2dO8ngz_2,and would have been of tutorial quality if space had allowed for more detailed explanations .,evaluation
Hk2dO8ngz_3,"The appendix are very useful , and tutorial paper material ( especially A ) .",evaluation
Hk2dO8ngz_4,"While I am not sure description would be enough to reproduce and no code is provided , every aspect of the architecture , if not described , if referred as similar to some previous work .",evaluation
Hk2dO8ngz_5,"There are also some notation shortcuts ( not explained ) in the proof of theorems that can lead to initial confusion , but they turn out to be non-ambiguous .",evaluation
Hk2dO8ngz_6,One that could be improved is <VAR> where one loses the fact that the second random variable is Y.,request
Hk2dO8ngz_7,"This work contains plenty of novel material , which is clearly compared to previous work :",evaluation
Hk2dO8ngz_8,- The main consequence of the use of Wasserstein distance is the surprisingly simple and useful Theorem 1 .,evaluation
Hk2dO8ngz_9,"I could not verify its novelty , but this seems to be a great contribution .",evaluation
Hk2dO8ngz_10,"- Blending GAN and auto-encoders has been tried in the past ,",fact
Hk2dO8ngz_11,but the authors claim better theoretical foundations that lead to solutions that do not rquire min-max,fact
Hk2dO8ngz_12,- The use of MMD in the context of GANs has also been tried .,fact
Hk2dO8ngz_13,The authors claim that their use in the latent space makes it more practival,fact
Hk2dO8ngz_14,"The experiments are very convincing , both numerically and visually .",evaluation
Hk2dO8ngz_15,"Source of confusion : in algorithm 1 and 2 , <VAR> is "" sampled "" from <VAR> ,",evaluation
Hk2dO8ngz_16,"some one is lead to believe that this is the sampling process as in VAEs , while in reality <VAR> is deterministic in the experiments .",evaluation
HJUMdjteM_0,The authors propose a model for learning physical interaction skills through trial and error .,fact
HJUMdjteM_1,"They use end-to-end deep reinforcement learning - the DQN model - including the task goal as an input in order to to improve generalization over several tasks , and shaping the reward depending on the visual differences between the goal state and the current state .",fact
HJUMdjteM_2,They show that the task performance of their model is better than the DQN on two simulated tasks .,fact
HJUMdjteM_3,"The paper is well-written , clarity is good ,",evaluation
HJUMdjteM_4,"it could be slightly improved by updating the title "" Toy example with Goal integration "" to make it consistent with the naming "" navigation task "" used elsewhere .",request
HJUMdjteM_5,"If the proposed model is new given the reviewer 's knowledge , the contribution is small .",evaluation
HJUMdjteM_6,The biggest change compared to the DQN model is the addition of information in the input .,evaluation
HJUMdjteM_7,"The authors initially claim that "" In this paper , [ they ] study how an artificial agent can autonomously acquire this intuition through interaction with the environment "" ,",quote
HJUMdjteM_8,however the proposed tasks present little to no realistic physical interaction :,fact
HJUMdjteM_9,the navigation task is a toy problem where no physics is simulated .,fact
HJUMdjteM_10,"In the stacking task , only part of the simulation actually use the physical simulation result .",fact
HJUMdjteM_11,"Given that machine learning methods are in general good at finding optimal policies that exploit simulation limitations ,",fact
HJUMdjteM_12,this problem seems a threat to the significance of this work .,evaluation
HJUMdjteM_13,The proposed GDQN model shows better performance than the DQN model .,fact
HJUMdjteM_14,"However , as the authors do not provide in-depth analysis of what the network learns ( e.g. by testing policies in the absence of an explicit goal ) ,",fact
HJUMdjteM_15,it is difficult to judge if the network learnt a meaningful representation of the world 's physics .,evaluation
HJUMdjteM_16,This limitation along with potential other are not discussed in the paper .,evaluation
HJUMdjteM_17,"Finally , more than a third ( 10/26 ) references point to Arxiv papers .",fact
HJUMdjteM_18,"Despite Arxiv definitely being an important tool for paper availability , it is not peer-reviewed and there are also work that are non-finished or erroneous .",fact
HJUMdjteM_19,"It is thus a necessary condition that all Arxiv references are replaced by the peer-reviewed material when it exist ( e.g. Lerer 2016 in ICML or Denil 2016 in ICLR 2017 ) , once again to strengthen the author 's claim .",request
HJI6Rf1eG_0,This writeup describes an application of recurrent autoencoder to analysis of multidimensional time series .,fact
HJI6Rf1eG_1,"The quality of writing , experimentation and scholarship is clearly below than what is expected from a scientific article .",evaluation
HJI6Rf1eG_2,"The method is explained in a very unclear way ,",evaluation
HJI6Rf1eG_3,there is no mention of any related work .,fact
HJI6Rf1eG_4,"I would encourage the authors to take a look at other ICLR submissions and see how rigorously written they are , how they position the reported research among comparable works .",request
B1LfYs_gf_0,This paper proposes to use 3D conditional GAN models to generate fMRI scans .,fact
B1LfYs_gf_1,"Using the generated images , paper reports improvement in classification accuracy on various tasks .",fact
B1LfYs_gf_2,One claim of the paper is that a generative model of fMRI data can help to caracterize and understand variability of scans across subjects .,fact
B1LfYs_gf_3,"Article is based on recent works such as Wasserstein GANs and AC-GANs by ( Odena et al. , 2016 ) .",fact
B1LfYs_gf_4,Despite the rich literature of this recent topic,evaluation
B1LfYs_gf_5,the related work section is rather convincing .,evaluation
B1LfYs_gf_6,Model presented extends IW-GAN by using 3D convolution and also by supervising the generator using sample labels .,fact
B1LfYs_gf_7,Major : - The size of the generated images is up to 26x31x22,fact
B1LfYs_gf_8,which is limited ( about half the size of the actual resolution of fMRI data ) .,evaluation
B1LfYs_gf_9,As a consequence results on decoding learning task using low resolution images can end up worse than with the actual data ( as pointed out ) .,fact
B1LfYs_gf_10,What it means is that the actual impact of the work is probably limited .,evaluation
B1LfYs_gf_11,- Generating high resolution images with GANs even on faces for which there is almost infinite data is still a challenge .,evaluation
B1LfYs_gf_12,Here a few thousand data points are used .,fact
B1LfYs_gf_13,So it raises too concerns : First is it enough ?,evaluation
B1LfYs_gf_14,Using so-called learning curves is a good way to answer this .,evaluation
B1LfYs_gf_15,Second is what are the contributions to the state-of-the-art of the 2 methods introduced ?,evaluation
B1LfYs_gf_16,"Said differently , as there is no classification results using images produced by an another GAN architecture",fact
B1LfYs_gf_17,it is hard to say that the extra complexity proposed here ( which is a bit contribution of the work ) is actually necessary .,evaluation
B1LfYs_gf_18,Minor : - Fonts in figure 4 are too small .,request
SJ22faFez_0,There is no scientific consensus on whether quantum annealers such as the D-Wave 2000Q that use the transverse-field Ising models yield any gains over classical methods,fact
SJ22faFez_1,( c.f. <URL> ) .,reference
SJ22faFez_2,"However , it is an exciting research area",evaluation
SJ22faFez_3,and this paper is an interesting demonstration of the feasibility of using quantum annealers for reinforcement learning .,evaluation
SJ22faFez_4,"This paper builds on Crawford et al. ( 2016 ) , an unpublished preprint , who develop a quantum Boltzmann machine reinforcement learning algorithm ( QBM-RL ) .",fact
SJ22faFez_5,"A QBM consists of adding a transverse field term to the RBM Hamiltonian ( negative log likelihood ) ,",fact
SJ22faFez_6,but the benefits of this for unsupervised tasks are unclear,evaluation
SJ22faFez_7,"( c.f. <URL> , another unpublished preprint ) .",reference
SJ22faFez_8,QBM-RL consists of using a QBM to model the state-action variables :,fact
SJ22faFez_9,it is an undirected graphical model whose visible nodes are clamped to observed state-action pairs .,fact
SJ22faFez_10,"The hidden nodes model dependencies between states and actions , and the weights of the model are updated to maximize the free energy or Q function ( value of the state-action pair ) .",fact
SJ22faFez_11,"The authors extend QBM-RL to work with quantum annealers such as the D-Wave 2000Q , which has a specific bipartite graph structure and requires special consideration",fact
SJ22faFez_12,because it can only yield samples of hidden variables in a fixed basis .,fact
SJ22faFez_13,"To overcome this , the authors develop a Suzuki-Trotter expansion and call it ' replica stacking ' , where a classical Hamiltonian in one dimension higher is used to approximate the quantum Hamiltonian .",fact
SJ22faFez_14,This enables the use of quantum annealers .,fact
SJ22faFez_15,The authors compare their method to standard baselines in a grid world environment .,fact
SJ22faFez_16,"Overall , I do not want to criticize the work .",evaluation
SJ22faFez_17,It is an interesting proof of concept .,evaluation
SJ22faFez_18,"But given the high price of quantum annealers , limited applicability of the technique , and unclear benefits of the authors ' method , I do not think it is relevant to this specific conference .",evaluation
SJ22faFez_19,It may be better suited to a workshop specific to quantum machine learning methods .,evaluation
SJ22faFez_20,======================================= + please add an algorithm box for your method .,request
SJ22faFez_21,It deviates significantly from QBM-RL .,evaluation
SJ22faFez_22,"For example , something like : ( 1 ) init weights of boltzmann machine randomly ( 2 ) sample c_eff ~ C from the pool of configurations sampled from the transverse-field Ising model using a quantum annealer with chimera graph ( 3 ) using the samples , calculate effective classical hamiltonian used to approximate the quantum system ( 4 ) use the weight update rules derived from Bellman equations ( spell out the rules ) .",request
SJ22faFez_23,+ moving the details of sampling into the appendix would help ;,request
SJ22faFez_24,they are not important for understanding the main ingredients of your method,evaluation
SJ22faFez_25,"There are so many moving parts in your system ,",evaluation
SJ22faFez_26,and someone without a physics background will struggle to understand it .,evaluation
SJ22faFez_27,Clarifying the algorithm in terms familiar to machine learning researchers will go a long way toward helping people understand your method .,request
SJ22faFez_28,+ the benefits of your method is unclear -,evaluation
SJ22faFez_29,"it looks like the method works , but does n't outperform the others .",evaluation
SJ22faFez_30,"this is fine ,",evaluation
SJ22faFez_31,but it is better to be straightforward about this and bill it as a ' proof of concept ',evaluation
SJ22faFez_32,+ perhaps consider rebranding the paper as something like ' RL using replica stacking for sampling from quantum boltzmann machines with quantum annealers ' .,request
SJ22faFez_33,"Elucidating why replica stacking is a crucial contribution of your work would be helpful , and could be of broad interest in the machine learning community .",evaluation
SJ22faFez_34,Right now it is too dense to be useful for the average person without a physics background :,evaluation
SJ22faFez_35,what difficulties are intrinsic to a quantum Hamiltonian ?,request
SJ22faFez_36,What is the intuition behind the Suzuki-Trotter decomposition you develop ?,request
SJ22faFez_37,What is the ' quantum ' Boltzmann machine in machine learning terms ( hidden-hidden connections in an undirected graphical model ! ) ?,request
SJ22faFez_38,What is replica-stacking in graphical model terms,request
SJ22faFez_39,( this would be a great ML contribution in its own right ! ) ?,evaluation
SJ22faFez_40,Really spelling these things out in detail ( or in the appendix ) would help,request
SJ22faFez_41,========================================== 1 ) eq 14 is malformed,evaluation
SJ22faFez_42,2 ) references are not well-formatted,evaluation
SJ22faFez_43,3 ) need factor of 1/2 to avoid double counting in sums over nearest neighbors ( please be precise ),request
HJmKXVcgz_0,This paper proposes a ranking-based similarity metric for distributional semantic models .,fact
HJmKXVcgz_1,"The main idea is to learn "" baseline "" word embeddings , retrofitting those and applying localized centering , to then calculate similarity using a measure called "" Ranking-based Exponential Similarity Measure "" ( RESM ) , which is based on the recently proposed APSyn measure .",fact
HJmKXVcgz_2,I think the work has several important issues :,evaluation
HJmKXVcgz_3,1 . The work is very light on references .,evaluation
HJmKXVcgz_4,"There is a lot of previous work on evaluating similarity in word embeddings ( e.g. Hill et al , a lot of the papers in RepEval workshops , etc. ) ; specialization for similarity of word embeddings ( e.g. Kiela et al. , Mrksic et al. , and many others ) ; multi-sense embeddings ( e.g. from Navigli 's group ) ; and the hubness problem ( e.g. Dinu et al. ) .",fact
HJmKXVcgz_5,"For the localized centering approach , Hara et al. 's introduced that method .",fact
HJmKXVcgz_6,"None of this work is cited , which I find inexcusable .",evaluation
HJmKXVcgz_7,"2 . The evaluation is limited , in that the standard evaluations ( e.g. SimLex would be a good one to add , as well as many others , please refer to the literature ) are not used and there is no comparison to previous work .",evaluation
HJmKXVcgz_8,The results are also presented in a confusing way,evaluation
HJmKXVcgz_9,", with the current state of the art results separate from the main results of the paper .",fact
HJmKXVcgz_10,"It is unclear what exactly helps , in which case , and why .",evaluation
HJmKXVcgz_11,"3 . There are technical issues with what is presented , with some seemingly factual errors .",evaluation
HJmKXVcgz_12,"For example , "" In this case we could apply the inversion , however it is much more convinient [ sic ] to take the negative of distance . Number 1 in the equation stands for the normalizing , hence the similarity is defined as follows """,quote
HJmKXVcgz_13,"- the 1 does not stand for normalizing , that is the way to invert the cosine distance",fact
HJmKXVcgz_14,"( put differently , cosine distance is 1-cosine similarity , which is a metric in Euclidean space due to the properties of the dot product ) .",fact
HJmKXVcgz_15,"Another example , "" are obtained using the GloVe vector , not using PPMI """,quote
HJmKXVcgz_16,"- there are close relationships between what GloVe learns and PPMI ,",evaluation
HJmKXVcgz_17,which the authors seem unaware of ( see e.g. the GloVe paper and Omer Levy 's work ) .,evaluation
HJmKXVcgz_18,"4 . Then there is the additional question , why should we care ?",evaluation
HJmKXVcgz_19,The paper does not really motivate why it is important to score well on these tests :,evaluation
HJmKXVcgz_20,"these kinds of tests are often used as ways to measure the quality of word embeddings ,",fact
HJmKXVcgz_21,but in this case the main contribution is the similarity metric used * on top * of the word embeddings .,evaluation
HJmKXVcgz_22,"In other words , what is supposed to be the take-away , and why should we care ?",request
HJmKXVcgz_23,"As such , I do not recommend it for acceptance -",evaluation
HJmKXVcgz_24,it needs significant work before it can be accepted at a conference .,evaluation
HJmKXVcgz_25,Minor points : - Typo in Eq 10,fact
HJmKXVcgz_26,- Typo on page 6 ( / cite instead of \ cite ),fact
H1k_ZpFlf_0,Summary : The paper proposes to learn new priors for latent codes z for GAN training .,fact
H1k_ZpFlf_1,for this the paper shows that there is a mismatch between the gaussian prior and an estimated of the latent codes of real data by reversal of the generator .,fact
H1k_ZpFlf_2,"To fix this the paper proposes to learn a second GAN to learn the prior distributions of "" real latent code "" of the first GAN .",fact
H1k_ZpFlf_3,The first GAN then uses the second GAN as prior to generate the z codes .,fact
H1k_ZpFlf_4,Quality/clarity : The paper is well written and easy to follow .,evaluation
H1k_ZpFlf_5,Originality : pros : - The paper while simple sheds some light on important problem with the prior distribution used in GAN .,evaluation
H1k_ZpFlf_6,- the second GAN solution trained on reverse codes from real data is interesting,evaluation
H1k_ZpFlf_7,"- In general the topic is interesting , the solution presented is simple but needs more study",evaluation
H1k_ZpFlf_8,"cons : - It related to adversarial learned inference and BiGAN , in term of learning the mapping <EQN> , <EQN> and seeking the agreement .",fact
H1k_ZpFlf_9,- The solution presented is not end to end,fact
H1k_ZpFlf_10,( learning a prior generator on learned models have been done in many previous works on encoder/decoder ),fact
H1k_ZpFlf_11,General Review : More experimentation with the latent codes will be interesting :,request
H1k_ZpFlf_12,- Have you looked at the decay of the singular values of the latent codes obtained from reversing the generator ?,request
H1k_ZpFlf_13,Is this data low rank ?,request
H1k_ZpFlf_14,how does this change depending on the dimensionality of the latent codes ?,request
H1k_ZpFlf_15,Maybe adding plots to the paper can help .,request
H1k_ZpFlf_16,- the prior agreement score is interesting,evaluation
H1k_ZpFlf_17,but assuming gaussian prior also for the learned latent codes from real data is maybe not adequate .,evaluation
H1k_ZpFlf_18,Maybe computing the entropy of the codes using a nearest neighbor estimate of the entropy can help understanding the entropy difference wrt to the isotropic gaussian prior ?,request
H1k_ZpFlf_19,- Have you tried to multiply the isotropic normal noise with the learned singular values and generate images from this new prior and compute inceptions scores etc ?,request
H1k_ZpFlf_20,Maybe also rotating the codes with the singular vector matrix V or <EQN> ?,request
H1k_ZpFlf_21,- What architecture did you use for the prior generator GAN ?,request
H1k_ZpFlf_22,- Have you thought of an end to end way to learn the prior generator GAN ?,request
HyKlaaFxf_0,"Overall , the paper is well-written",evaluation
HyKlaaFxf_1,and the proposed model is quite intuitive .,evaluation
HyKlaaFxf_2,"Specifically , the idea is to represent entailment as a product of continuous functions over possible worlds .",fact
HyKlaaFxf_3,"Specifically , the idea is to generate possible worlds , and compute the functions that encode entailment in those worlds .",fact
HyKlaaFxf_4,The functions themselves are designed as tree neural networks to take advantage of logical structure .,fact
HyKlaaFxf_5,"Several different encoding benchmarks of the entailment task are designed to compare against the performance of the proposed model , using a newly created dataset .",fact
HyKlaaFxf_6,The results seem very impressive with > 99 % accuracy on tests sets .,evaluation
HyKlaaFxf_7,One weakness with the paper was that it was only tested on 1 dataset .,evaluation
HyKlaaFxf_8,"Also , should some form of cross-validation be applied to smooth out variance in the evaluation results .",request
HyKlaaFxf_9,"I am not sure if there are standard "" shared "" datasets for this task ,",non-arg
HyKlaaFxf_10,which would make the results much stronger .,request
HyKlaaFxf_11,"Also how about the tradeoff , i.e. , does training time significantly increase when we "" imagine "" more worlds .",request
HyKlaaFxf_12,"Also , in general , a discussion on the efficiency of training the proposed model as compared to TreeNN would be helpful .",request
HyKlaaFxf_13,"The size of the world vectors , I would believe is quite important ,",evaluation
HyKlaaFxf_14,so maybe a more detailed analysis on how this was chosen is important to replicate the results .,request
HyKlaaFxf_15,"This problem , I think , is quite related to model counting .",evaluation
HyKlaaFxf_16,There has been a lot of work on model counting .,evaluation
HyKlaaFxf_17,a discussion on how this relates to those lines of work would be interesting .,request
ryIbx22yz_0,"The authors perform a set of experiments in which they inspect the Hessian matrix of the loss of a neural network , and observe that most of the eigenvalues are very close to zero .",fact
ryIbx22yz_1,"This is a potentially important observation ,",evaluation
ryIbx22yz_2,"and the experiments were well worth performing ,",evaluation
ryIbx22yz_3,but I do n't find them fully convincing,evaluation
ryIbx22yz_4,( partly because I was confused by the presentation ) .,evaluation
ryIbx22yz_5,They perform four sets of experiments :,fact
ryIbx22yz_6,"1 ) In section 3.1 , they show on simulated data that for data drawn from k clusters , there are roughly k significant eigenvalues in the Hessian of the solution .",fact
ryIbx22yz_7,"2 ) In section 3.2 , they show on MNIST that the solution contains few large eigenvalues , and also that there are negative eigenvalues .",fact
ryIbx22yz_8,"3 ) In section 3.3 , they show ( again on MNIST ) that at their respective solutions , large batch and small batch methods find solutions with similar numbers of large eigenvalues , but that for the large batch method the magnitudes are larger .",fact
ryIbx22yz_9,"4 ) In section 4.1 , they train ( on CIFAR10 ) using a large batch method , and then transition to a small batch method , and argue that the second solution appears to be better than the first , but that they are a part of the same basin",fact
ryIbx22yz_10,( since linearly while interpolating between them they do n't run into any barriers ) .,fact
ryIbx22yz_11,"I 'm not fully convinced by the second and third experiments ,",evaluation
ryIbx22yz_12,"partly because I did n't fully understand the plots ( more on this below ) ,",evaluation
ryIbx22yz_13,"but also because it is n't clear to me what we should expect from the spectrum of a Hessian ,",evaluation
ryIbx22yz_14,"so I do n't know whether the observed specra have fewer large eigenvalues , or more large eigenvalues , then would be "" natural "" .",evaluation
ryIbx22yz_15,"In other words , there is n't a * baseline * .",fact
ryIbx22yz_16,"For the fourth experiment , it 's unsurprising that the small batch method winds up in a different location in the same basin as the large batch method ,",evaluation
ryIbx22yz_17,since it was initialized to the large batch method 's solution,fact
ryIbx22yz_18,"( and it does n't appear to me , in figure 9 , that the small batch solution is significantly different ) .",evaluation
ryIbx22yz_19,"Section 2.1 is said to contain an argument that the second term of equation 5 can be ignored , but only says that if \ ell ' and \ nabla ^ 2 of f are uncorrelated , then it can be ignored .",fact
ryIbx22yz_20,"I do n't see any reason that these two quantities should be correlated ,",fact
ryIbx22yz_21,but this is not an argument that they are uncorrelated .,non-arg
ryIbx22yz_22,"Also , it is n't clear to me where this approximation was used -- everywhere ?",evaluation
ryIbx22yz_23,"In section 3.2 , it sounds as if the exact Hessian is used ,",evaluation
ryIbx22yz_24,"and at the end of this section the authors say that figure 6 demonstrates that the effect of this second term is small ,",fact
ryIbx22yz_25,"but I do n't see why this is ,",evaluation
ryIbx22yz_26,and it is n't explained .,fact
ryIbx22yz_27,My main complaint is that I had a great deal of difficulty interpreting the plots :,evaluation
ryIbx22yz_28,"it often was n't clear to me what exactly was being plotted ,",evaluation
ryIbx22yz_29,and most of the language describing them was frustratingly vague .,evaluation
ryIbx22yz_30,"For example , figure 6 is captioned "" left edge of the spectrum , eigenvalues are scaled by their ratio "" .",fact
ryIbx22yz_31,"The text explains that "" left edge of the spectrum "" means "" small but negative eigenvalues """,fact
ryIbx22yz_32,"( this would be better in the caption ) ,",evaluation
ryIbx22yz_33,but what are the ratios ?,evaluation
ryIbx22yz_34,Ratio of what to what ?,evaluation
ryIbx22yz_35,"I think it would greatly enhance clarity if every plot caption described exactly , and unambiguously , what quantities were plotted on the horizontal and vertical axes .",request
ryIbx22yz_36,"Some minor notes : There are a number of places where "" it 's "" is used , where it should be "" its "" .",request
ryIbx22yz_37,"In the introduction , the definition of \ mathcal { L } ' is slightly confusing ,",evaluation
ryIbx22yz_38,"since it 's an expectation ,",fact
ryIbx22yz_39,"but the use of "" ' "" makes one expect a derivative .",evaluation
ryIbx22yz_40,"Perhaps use \ hat { \ mathcal { L } } for the empirical loss , and \ mathcal { L } for the expected one ?",request
ryIbx22yz_41,"On the bottom of page 4 , "" if \ ell ' and \ nabla f are not correlated "" : I think the \ nabla should be \ nabla ^ 2 .",request
ryIbx22yz_42,"It 's "" principal components "" , not "" principle components "" .",request
rJAS034ez_0,This paper presents a framework to recover a set of independent mechanisms .,fact
rJAS034ez_1,In order to do so it uses a set of experts each one made out of a GAN .,fact
rJAS034ez_2,My main concern with this work is that I do n't see any mechanism in the framework that prevents an expert ( or few of them ) to win all examples except its own learning capacities .,evaluation
rJAS034ez_3,p7 authors have also noticed that several experts fail to specialize,fact
rJAS034ez_4,and I bet that is the reason why .,evaluation
rJAS034ez_5,"Thus , authors should analyze how well we can have all/most experts specialize in a pool vs expert capacity/architecture .",request
rJAS034ez_6,It would also be great to integrate a direct regularization mechanism in the cost in order to do so .,request
rJAS034ez_7,Like for example a penalty in how many examples a expert has catched .,request
rJAS034ez_8,"Moreover , the discrimator D ( which is trained to discriminate between real or fake examples ) seems to be directly used to tell if an example is throw from the targeted distribution .",evaluation
rJAS034ez_9,It is not the same task .,evaluation
rJAS034ez_10,How D will handle an example far from fake or real ones ?,non-arg
rJAS034ez_11,Why will D answer negatively ( or positively ) on this example ?,non-arg
H1kAEtYlz_0,"The paper studies different methods for defining hypergraph embeddings , i.e. defining vectorial representations of the set of hyperedges of a given hypergraph .",fact
H1kAEtYlz_1,It should be noted that the framework does not allow to compute a vectorial representation of a set of nodes not already given as an hyperedge .,fact
H1kAEtYlz_2,A set of methods is presented : the first one is based on an auto-encoder technique ;,fact
H1kAEtYlz_3,the second one is based on tensor decomposition ;,fact
H1kAEtYlz_4,the third one derives from sentence embedding methods .,fact
H1kAEtYlz_5,The fourth one extends over node embedding techniques,fact
H1kAEtYlz_6,and the last one use spectral methods .,fact
H1kAEtYlz_7,The two first methods use plainly the set structure of hyperedges .,fact
H1kAEtYlz_8,Experimental results are provided on semi-supervised regression tasks .,fact
H1kAEtYlz_9,They show very similar performance for all methods and variants .,evaluation
H1kAEtYlz_10,Also run-times are compared,fact
H1kAEtYlz_11,and the results are expected .,evaluation
H1kAEtYlz_12,"In conclusion , the paper gives an overview of methods for computing hypernode embeddings .",fact
H1kAEtYlz_13,This is interesting in its own .,evaluation
H1kAEtYlz_14,"Nevertheless , as the target problem on hypergraphs is left unspecified ,",fact
H1kAEtYlz_15,it is difficult to infer conclusions from the study .,evaluation
H1kAEtYlz_16,"Therefore , I am not convinced that the paper should be published in ICLR ' 18 .",evaluation
H1kAEtYlz_17,"* typos * Recent surveys on graph embeddings have been published in 2017 and should be cited as "" A comprehensive survey of graph embedding ... "" by Cai et al",request
H1kAEtYlz_18,* Preliminaries . The occurrence number <VAR> are not modeled in the hypergraphs .,fact
H1kAEtYlz_19,A graph N_a is defined but not used in the paper .,fact
H1kAEtYlz_20,* Section 3.1 . the procedure for sampling hyperedges in the lattice shoud be given .,request
H1kAEtYlz_21,"At least , you should explain how it is made efficient when the number of nodes is large .",request
H1kAEtYlz_22,* Section 3.2 . The method seems to be restricted to cases where the cardinality of hyperedges can take a small number of values .,fact
H1kAEtYlz_23,This is discussed in Section 3.6,fact
H1kAEtYlz_24,but the discussion is not convincing enough .,evaluation
H1kAEtYlz_25,* Section 3.3 The term Sen2vec is not common knowledge,evaluation
H1kAEtYlz_26,* Section 3.3 The length of the sentences depends on the number of permutations of <VAR> elements .,fact
H1kAEtYlz_27,How can you deal with large k ?,request
H1kAEtYlz_28,* Section 3.4 and Section 3.5 . The methods proposed in these two sections should be related with previous works on hypergraph kernels .,request
H1kAEtYlz_29,I.e. there should be mentions on the clique expansion and star expansion of hypergraphs .,request
H1kAEtYlz_30,This leads to the question why graph embeddings methods on these expansions have not be considered in the paper .,evaluation
H1kAEtYlz_31,"* Section 4.1 . Only hyperedeges of cardinality in [ 2,6 ] are considered .",fact
H1kAEtYlz_32,This seems a rather strong limitation,evaluation
H1kAEtYlz_33,and this hypothesis does not seem pertinent in many applications .,evaluation
H1kAEtYlz_34,"* Section 4 . For online multi-player games , hypernode embeddings only allow to evaluate existing teams , i.e. already existing as hyperedges in the input hypergraph .",fact
H1kAEtYlz_35,One of the most important problem for multi-player games is team making where team evaluation should be made for all possible teams .,evaluation
H1kAEtYlz_36,* Section 5 . Seems redundant with the Introduction .,evaluation
B1qhp-qeG_0,The paper investigates the iterative estimation view on gated recurrent networks ( GNN ) .,fact
B1qhp-qeG_1,Authors observe that the average estimation error between a given hidden state and the last hidden state gradually decreases toward zeros .,fact
B1qhp-qeG_2,This suggest that GNN are bias toward an identity mapping and learn to preserve the activation through time .,fact
B1qhp-qeG_3,"Given this observation , authors then propose RIN , a new RNN parametrization where the hidden to hidden matrix is decomposed as a learnable weight matrix plus the identity matrix .",fact
B1qhp-qeG_4,"Authors evaluate their RIN on the adding , sequential MNIST and the baby tasks and show that their IRNN outperforms the IRNN and LSTM models .",fact
B1qhp-qeG_5,Questions : - Section 2 suggests that use of the gate in GNNs encourages to learn an identity mapping .,fact
B1qhp-qeG_6,Does the average iteration error behaves differently in case of a tanh-RNN ?,non-arg
B1qhp-qeG_7,- It seems from Figure 4 ( a ) that the average estimation error is higher for RIN than IRNN and LSTM and only decrease toward zero at the very end .,fact
B1qhp-qeG_8,What could explain this phenomenon ?,non-arg
B1qhp-qeG_9,"- While the LSTM baseline matches the results of Le et al. ,",fact
B1qhp-qeG_10,later work such as Recurrent Batch Normalization or Unitary Evolution RNN have demonstrated much better performance with a vanilla LSTM on those tasks ( outperforming both IRNN and RIN ) .,fact
B1qhp-qeG_11,What could explain this difference in the performances ?,non-arg
B1qhp-qeG_12,"- Unless I am mistaken , Gated Orthogonal Recurrent Units : On Learning to Forget from Jing et al. also reports better performances for the LSTM ( and GRU ) baselines that outperform RIN on the baby tasks with mean performances of 58.2 and 56.0 for GRU and LSTM respectively ?",fact
B1qhp-qeG_13,- Quality/Clarity : The paper is well written and pleasant to read,evaluation
B1qhp-qeG_14,- Originality : Looking at RNN from an iterative refinement point of view seems novel .,evaluation
B1qhp-qeG_15,"- Significance : While looking at RNN from an iterative estimation is interesting ,",evaluation
B1qhp-qeG_16,the experimental part does not really show what are the advantages of the propose RIN .,fact
B1qhp-qeG_17,"In particular , the LSTM baseline seems to weak compared to other works .",fact
HkJ6DWtgf_0,This paper studies a new architecture DualAC .,fact
HkJ6DWtgf_1,The author give strong and convincing justifications based on the Lagrangian dual of the Bellman equation,evaluation
HkJ6DWtgf_2,"( although not new , introducing this as the justification for the architecture design is plausible ) .",evaluation
HkJ6DWtgf_3,There are several drawbacks of the current format of the paper :,evaluation
HkJ6DWtgf_4,1 . The algorithm is vague .,evaluation
HkJ6DWtgf_5,Alg 1 line 5 : ' closed form ' :,quote
HkJ6DWtgf_6,there is no closed form in Eq ( 14 ) .,fact
HkJ6DWtgf_7,It is just an MC approximation .,fact
HkJ6DWtgf_8,line 6 : Decay O ( 1/t ^ \ beta ) .,quote
HkJ6DWtgf_9,This is indeed vague albeit easy to understand .,evaluation
HkJ6DWtgf_10,The algorithm requires that every step is crystal clear .,evaluation
HkJ6DWtgf_11,"2 . Also , there are several format error which may be due to compiling , e.g. , line 2 of Abstract , ' Dual-AC ' ( an extra space ) .",evaluation
HkJ6DWtgf_12,There are many format errors like this throughout the paper .,evaluation
HkJ6DWtgf_13,The author is suggested to do a careful format check .,request
HkJ6DWtgf_14,3 . The author is suggested to explain more about the necessity of introducing path regularization and SDA .,request
HkJ6DWtgf_15,The current justification is reasonable but too brief .,evaluation
HkJ6DWtgf_16,"4 . The experimental part is ok to me ,",evaluation
HkJ6DWtgf_17,but not very impressive .,evaluation
HkJ6DWtgf_18,"Overall , this seems to be a nice paper to me .",evaluation
H1GhmwqgG_0,Summary : - This paper proposes a hand-designed network architecture on a graph of object proposals to perform soft non-maximum suppression to get object count .,fact
H1GhmwqgG_1,Contribution : - This paper proposes a new object counting module which operates on a graph of object proposals .,fact
H1GhmwqgG_2,Clarity : - The paper is well written and clarity is good .,evaluation
H1GhmwqgG_3,Figure 2 & 3 helps the readers understand the core algorithm .,evaluation
H1GhmwqgG_4,Pros : - De-duplication modules of inter and intra object edges are interesting .,evaluation
H1GhmwqgG_5,- The proposed method improves the baseline by 5 % on counting questions .,fact
H1GhmwqgG_6,Cons : - The proposed model is pretty hand-crafted .,evaluation
H1GhmwqgG_7,"I would recommend the authors to use something more general , like graph convolutional neural networks ( Kipf & Welling , 2017 ) or graph gated neural networks ( Li et al. , 2016 ) .",request
H1GhmwqgG_8,- One major bottleneck of the model is that the proposals are not jointly finetuned .,evaluation
H1GhmwqgG_9,"So if the proposals are missing a single object , this can not really be counted .",fact
H1GhmwqgG_10,"In short , if the proposals do n’t have 100 % recall , then the model is then trained with a biased loss function which asks it to count all the objects even if some are already missing from the proposals .",fact
H1GhmwqgG_11,The paper did n’t study what is the recall of the proposals and how sensitive the threshold is .,fact
H1GhmwqgG_12,- The paper does n’t study a simple baseline that just does NMS on the proposal domain .,fact
H1GhmwqgG_13,"- The paper does n’t compare experiment numbers with ( Chattopadhyay et al. , 2017 ) .",fact
H1GhmwqgG_14,- The proposed algorithm does n’t handle symmetry breaking when two edges are equally confident ( in 4.2.2 it basically scales down both edges ) .,fact
H1GhmwqgG_15,This is similar to a density map approach and the problem is that the model does n’t develop a notion of instance .,evaluation
H1GhmwqgG_16,"- Compared to ( Zhou et al. , 2017 ) , the proposed model does not improve much on the counting questions .",evaluation
H1GhmwqgG_17,"- Since the authors have mentioned in the related work , it would also be more convincing if they show experimental results on CL",request
H1GhmwqgG_18,"Conclusion : - I feel that the motivation is good , but the proposed model is too hand-crafted .",evaluation
H1GhmwqgG_19,"Also , key experiments are missing : 1 ) NMS baseline 2 ) Comparison with VQA counting work ( Chattopadhyay et al. , 2017 ) .",request
H1GhmwqgG_20,Therefore I recommend reject .,evaluation
H1GhmwqgG_21,"References : - Kipf , T.N. , Welling , M. , Semi-Supervised Classification with Graph Convolutional Networks . ICLR 2017 .",reference
H1GhmwqgG_22,"- Li , Y. , Tarlow , D. , Brockschmidt , M. , Zemel , R. Gated Graph Sequence Neural Networks . ICLR 2016 .",reference
S1jAR0Klf_0,The authors present a model for unsupervised NMT which requires no parallel corpora between the two languages of interest .,fact
S1jAR0Klf_1,While the results are interesting I find very few original ideas in this paper .,evaluation
S1jAR0Klf_2,Please find my comments/questions/suggestions below : 1 ) The authors mention that there are 3 important aspects in which their model differs from a standard NMT architecture .,fact
S1jAR0Klf_3,All the 3 differences have been adapted from existing works .,fact
S1jAR0Klf_4,The authors clearly acknowledge and cite the sources .,fact
S1jAR0Klf_5,Even sharing the encoder using cross lingual embeddings has been explored in the context of multilingual NER,fact
S1jAR0Klf_6,( please see <URL> ) .,reference
S1jAR0Klf_7,Because of this I find the paper to be a bit lacking on the novelty quotient .,evaluation
S1jAR0Klf_8,Even backtranslation has been used successfully in the past ( as acknowledged by the authors ) .,fact
S1jAR0Klf_9,Unsupervised MT in itself is not a new idea ( again clearly acknowledged by the authors ) .,evaluation
S1jAR0Klf_10,2 ) I am not very convinced about the idea of denoising .,evaluation
S1jAR0Klf_11,"Specifically , I am not sure if it will work for arbitrary language pairs .",evaluation
S1jAR0Klf_12,"In fact , I think there is a contradiction even in the way the authors write this .",evaluation
S1jAR0Klf_13,"On one hand , they want to "" learn the internal structure of the languages involved """,fact
S1jAR0Klf_14,and on the other hand they deliberately corrupt this structure by adding noise .,fact
S1jAR0Klf_15,This seems very counter-intuitive and in fact the results in Table 1 suggest that it leads to a drop in performance .,evaluation
S1jAR0Klf_16,I am not very sure that the analogy with autoencoders holds in this case .,evaluation
S1jAR0Klf_17,"3 ) Following up on the above question , the authors mention that "" We emphasize , however , that it is not possible to use backtranslation alone without denoising "" .",quote
S1jAR0Klf_18,"Again , if denoising itself leads to a drop in the performance as compared to the nearest neighbor baseline then why use backtranslation in conjunction with denoising and not in conjunction with the baseline itself .",non-arg
S1jAR0Klf_19,4 ) This point is more of a clarification and perhaps due to my lack of understanding .,non-arg
S1jAR0Klf_20,Backtranslation to generate a pseudo corpus makes sense only after the model has achieved a certain ( good ) performance .,evaluation
S1jAR0Klf_21,Can you please provide details of how long did you train the model ( with denoising ? ) before producing the backtranslations ?,request
S1jAR0Klf_22,5 ) The authors mention that 100K parallel sentences may be insufficient for training a NMT system .,fact
S1jAR0Klf_23,"However , this size may be decent enough for a PBSMT system .",evaluation
S1jAR0Klf_24,It would be interesting to see the performance of a PBSMT system trained on 100K parallel sentences .,request
S1jAR0Klf_25,6 ) How did you arrive at the beam size of 12 ?,request
S1jAR0Klf_26,Was this a hyperparameter ?,request
S1jAR0Klf_27,Just curious .,request
S1jAR0Klf_28,7 ) The comparable NMT set up is not very clear .,evaluation
S1jAR0Klf_29,Can you please explain it in detail ?,request
S1jAR0Klf_30,"In the same paragraph , what exactly do you mean by "" the supervised system in this paper is relatively small ? """,request
SkmXSLZEM_0,This paper proposes a novel image/album geolocation algorithm .,fact
SkmXSLZEM_1,"This is based on the recent PlaNet approach , with several extensions , including a better mesh representation for discretization and the time-aware prediction .",fact
SkmXSLZEM_2,"However , contributions are limited to me .",evaluation
SkmXSLZEM_3,Pros : - Clearly written,evaluation
SkmXSLZEM_4,- Good results,evaluation
SkmXSLZEM_5,- Interesting empirical analysis between UTC time and geo-location,evaluation
SkmXSLZEM_6,Contribution : - Triangulation based meshify vs quad-tree meshification was claimed to be one of the major contributions .,fact
SkmXSLZEM_7,It is an incremental improvement towards Wayand et al .,evaluation
SkmXSLZEM_8,However I do n’t think this is significant enough for an ICLR paper .,evaluation
SkmXSLZEM_9,- Incorporating the time-ordering into album based geo-localization is considered a contribution on the application side .,fact
SkmXSLZEM_10,But in computer vision this is not the first one to exploit this information ( see [ a ] ),fact
SkmXSLZEM_11,while the technical approach to encode this information is quite standard ( LSTM ) .,fact
SkmXSLZEM_12,Improvement : - Compared against Wayand et al. and Vo et al. the quantitative results are not very impressive .,evaluation
SkmXSLZEM_13,Minor : Some choices in feature engineering are not explained well :,evaluation
SkmXSLZEM_14,"e.g . In section 3.2 , why you only choose top 10 maximum entries ,",request
SkmXSLZEM_15,why l1-norm is appended as a feature ?,request
SkmXSLZEM_16,Are those choice made because of validation performance ?,request
SkmXSLZEM_17,Minor writing : Fig. 1 “ 01:00 than ”,request
SkmXSLZEM_18,Fig. 1 “ to make a prediction ”,request
SkmXSLZEM_19,“ Fig. 3.1.1 ” Table 5 . Why not bold the best performance ?,request
SkmXSLZEM_20,Acknowledgements should n’t be put in double-blind reviewing process,request
SkmXSLZEM_21,[A] <CIT>,reference
Skjfeg5gG_0,"In the context of multitask reinforcement learning , this paper considers the problem of learning behaviours when given specifications of subtasks and the relationship between them , in the form of a task graph .",fact
Skjfeg5gG_1,"The paper presents a neural task graph solver ( NTS ) , which encodes this as a recursive-reverse-recursive neural network .",fact
Skjfeg5gG_2,"A method for learning this is presented , and fine tuned with an actor-critic method .",fact
Skjfeg5gG_3,The approach is evaluated in a multitask grid world domain .,fact
Skjfeg5gG_4,This paper addresses an important issue in scaling up reinforcement learning to large domains with complex interdependencies in subtasks .,evaluation
Skjfeg5gG_5,"The method is novel ,",evaluation
Skjfeg5gG_6,and the paper is generally well written .,evaluation
Skjfeg5gG_7,"I unfortunately have several issues with the paper in its current form , most importantly around the experimental comparisons .",evaluation
Skjfeg5gG_8,"The paper is severely weakened by not comparing experimentally to other learning ( hierarchical ) schemes , such as options or HAMs .",request
Skjfeg5gG_9,None of the comparisons in the paper feature any learning .,fact
Skjfeg5gG_10,"Ideally , one should see the effect of learning with options ( and not primitive actions ) to fairly compare against the proposed framework .",evaluation
Skjfeg5gG_11,"At some level , I question whether the proposed framework is doing any more than just value function propagation at a task level ,",evaluation
Skjfeg5gG_12,and these experiments would help resolve this .,evaluation
Skjfeg5gG_13,"Additionally , the example domain makes no sense .",evaluation
Skjfeg5gG_14,"Rather use something more standard , with well-known baselines , such as the taxi domain .",request
Skjfeg5gG_15,"I would have liked to see a discussion in the related work comparing the proposed approach to the long history of reasoning with subtasks from the classical planning literature , notably HTNs .",request
Skjfeg5gG_16,"I found the description of the training of the method to be rather superficial ,",evaluation
Skjfeg5gG_17,and I do n't think it could be replicated from the paper in its current level of detail .,evaluation
Skjfeg5gG_18,The approach raises the natural questions of where the tasks and the task graphs come from .,request
Skjfeg5gG_19,Some acknowledgement and discussion of this would be useful .,request
Skjfeg5gG_20,The legend in the middle of Fig 4 obscures the plot ( admittedly not substantially ) .,evaluation
Skjfeg5gG_21,"There are also a number of grammatical errors in the paper , including the following non-exhaustive list :",fact
Skjfeg5gG_22,2 : as well as how to do - > as well as how to do it,request
Skjfeg5gG_23,Fig 2 caption : through bottom-up - > through a bottom-up,request
Skjfeg5gG_24,3 : Let S be a set of state - > Let S be a set of states,request
Skjfeg5gG_25,3 : form of task graph - > form of a task graph,request
Skjfeg5gG_26,3 : In addtion - > In addition,request
Skjfeg5gG_27,4 : which is propagates - > which propagates,request
Skjfeg5gG_28,5 : investigated following - > investigated the following,request
HJge1dvgz_0,"Paper summary : The authors propose a number of tricks to enable training policies for pick and place style tasks using a combination of GAIL-based imitation learning and hand-specified rewards , as well as use of unobserved state information during training and hand-designed curricula .",fact
HJge1dvgz_1,"The results demonstrate manipulation policies for stacking blocks and moving objects , as well as preliminary results for zero-shot transfer from simulation to a real robot for a picking task and an attempt at a stacking task .",fact
HJge1dvgz_2,"Review summary : The paper proposes a limited but interesting contribution that will be especially of interest to practitioners ,",evaluation
HJge1dvgz_3,"but the scope of the contribution is somewhat incremental in light of recent work ,",evaluation
HJge1dvgz_4,"and the results , while interesting , could certainly be better .",evaluation
HJge1dvgz_5,"In the balance , I think the paper should be accepted ,",evaluation
HJge1dvgz_6,"because it will be of value to practitioners , and I appreciate the detail and real-world experiments .",evaluation
HJge1dvgz_7,"However , some of the claims should be revised to better reflect what the paper actually accomplishes :",request
HJge1dvgz_8,"the contribution is a bit limited in places ,",evaluation
HJge1dvgz_9,but that 's * OK * -- the authors should just be up-front about it .,request
HJge1dvgz_10,Pros : - Interesting tasks that combine imitation and reinforcement in a logical ( but somewhat heuristic ) way,evaluation
HJge1dvgz_11,- Good simulated results on a variety of pick-and-place style problems,evaluation
HJge1dvgz_12,"- Some initial attempt at real-world transfer that seems promising , but limited",evaluation
HJge1dvgz_13,- Related work is very detailed and I think many will find it to be a very valuable overview,evaluation
HJge1dvgz_14,Cons : - Some of the claims ( detailed below ) are a bit excessive in my opinion,evaluation
HJge1dvgz_15,- The paper would be better if it was scoped more narrowly,request
HJge1dvgz_16,- Contribution is a bit incremental and somewhat heuristic,evaluation
HJge1dvgz_17,- The experimental results are difficult to interpret in simulation,evaluation
HJge1dvgz_18,- The real-world experimental results are not great,evaluation
HJge1dvgz_19,- There are a couple of missing citations ( but overall related work is great ),fact
HJge1dvgz_20,"Detailed discussion of potential issues and constructive feedback : > "" Our approach leverages demonstration data to assist a reinforcement learning agent in learning to solve a wide range of tasks , mainly previously unsolved . """,evaluation
HJge1dvgz_21,>> This claim is a bit peculiar .,evaluation
HJge1dvgz_22,"Picking up and placing objects is certainly not "" unsolved , "" there are many examples .",evaluation
HJge1dvgz_23,"If you want image-based pick and place with demonstrations for example , see Chebotar '17 ( not cited ) .",non-arg
HJge1dvgz_24,"If you want stacking blocks , see Nair ' 17 .",non-arg
HJge1dvgz_25,"While it 's true that there is a particular combination of factors that does n't exactly appear in prior work , the statement the authors make is way too strong .",evaluation
HJge1dvgz_26,"Chebotar '17 shows picking and placing a real-world objective with a much higher success rate than reported here , without simulation .",fact
HJge1dvgz_27,"Nair '17 shows a much harder stacking task , but without images --",fact
HJge1dvgz_28,would that method have worked just as well with image-based distillation ?,non-arg
HJge1dvgz_29,Very likely .,evaluation
HJge1dvgz_30,Rajeswaran '17 shows tasks that arguably are much harder .,fact
HJge1dvgz_31,"Maybe a more honest statement is that this paper proposes some tasks that prior methods do n't show , and some prior methods show tasks that the proposed method ca n't solve .",request
HJge1dvgz_32,"But as-is , this statement misrepresents prior work .",evaluation
HJge1dvgz_33,"> Previous RL-based robot manipulation policies ( Nair et al. , 2017 ; Popov et al. , 2017 ) largely rely on low-level states as input , or use severely limited action spaces that ignore the arm and instead learn Cartesian control of a simple gripper .",quote
HJge1dvgz_34,"This limits the ability of these methods to represent and solve more complex tasks ( e.g. , manipulating arbitrary 3D objects ) and to deploy in real environments where the privileged state information is unavailable .",evaluation
HJge1dvgz_35,>> This is a funny statement .,evaluation
HJge1dvgz_36,"Some use images , some do n't .",fact
HJge1dvgz_37,There is a ton of prior work on RL-based robot manipulation that does use images .,fact
HJge1dvgz_38,"The current paper does use object state information during training , which some prior works manage to avoid .",fact
HJge1dvgz_39,The comments about Cartesian control are a bit peculiar ...,evaluation
HJge1dvgz_40,"the proposed method controls fingers , but the hand is simple .",fact
HJge1dvgz_41,"Some prior works have simpler grippers ( e.g. , Nair ) and",evaluation
HJge1dvgz_42,"some have much more complex hands ( e.g. , Rajeswaran ) .",evaluation
HJge1dvgz_43,So this one falls somewhere in the middle .,evaluation
HJge1dvgz_44,"That 's fine , but again , this statement overclaims a bit .",evaluation
HJge1dvgz_45,> To sidestep the constraints of training on real hardware we embrace the sim2real paradigm which has recently shown promising results,quote
HJge1dvgz_46,"( James et al. , 2017 ; Rusu et al. , 2016a ) .",reference
HJge1dvgz_47,">> Probably should cite Sadeghi et al. and Tobin et al. in regard to randomization , both of which precede James ' 17 .",request
HJge1dvgz_48,"> we can , during training , exploit privileged information about the true system state",quote
HJge1dvgz_49,>> This was done also in Pinto et al. and many of the cited GPS papers,fact
HJge1dvgz_50,> our policies solve the tasks that the state-of-the-art reinforcement and imitation learning can not solve,quote
HJge1dvgz_51,>> I do n't think this statement is justified without much wider comparisons --,evaluation
HJge1dvgz_52,"the authors do n't attempt any comparisons to prior work , such as Chebotar '17 ( which arguably is closest in terms of demonstrated behaviors ) , Nair '17 ( which is also close but does n't use images , though it likely could ) .",fact
HJge1dvgz_53,> An alternative strategy for dealing with the data demand is to train in simulation and transfer,quote
HJge1dvgz_54,">> Aside from previously mentioned citations , should probably cite Devin "" Towards Adapting Deep Visuomotor Representations """,request
HJge1dvgz_55,> Sec 3.2.1,non-arg
HJge1dvgz_56,>> This method seems a bit heuristic .,evaluation
HJge1dvgz_57,"It 's logical , but can you say anything about what this will converge to ?",request
HJge1dvgz_58,"GAIL will try to match the demonstration distribution , and RL will try to maximize expected reward .",fact
HJge1dvgz_59,What will this method do ?,non-arg
HJge1dvgz_60,> Experiments,non-arg
HJge1dvgz_61,>> Would it be possible to indicate some measure of success rate for the simulated experiments ?,non-arg
HJge1dvgz_62,"As-is , it 's hard to tell how well either the proposed method or the baselines actually work .",evaluation
HJge1dvgz_63,> Transfer,non-arg
HJge1dvgz_64,>> My reading of the transfer experiments is that they are basically unsuccessful .,evaluation
HJge1dvgz_65,Picking up a rectangular object with 80 % success rate is not very good .,evaluation
HJge1dvgz_66,The stacking success rate is too low to be useful .,evaluation
HJge1dvgz_67,"I do appreciate the authors trying out their method on a real robotic platform ,",evaluation
HJge1dvgz_68,"but perhaps the more honest assessment of the outcome of these experiments is that the approach did n't work very well ,",evaluation
HJge1dvgz_69,and more research is needed .,request
HJge1dvgz_70,"Again , it 's * OK * to say this !",evaluation
HJge1dvgz_71,Part of the purpose of publishing a paper is to stimulate future research directions .,non-arg
HJge1dvgz_72,"I think the transfer experiments should definitely be kept , but the authors should discuss the limitations to help future work address them , and present the transfer appropriately in the intro .",request
HJge1dvgz_73,> Diverse Visuomotor Skills,non-arg
HJge1dvgz_74,>> I think this is a peculiar thing to put in the title .,evaluation
HJge1dvgz_75,Is the implication that prior work is not diverse ?,non-arg
HJge1dvgz_76,Arguably several prior papers show substantially more diverse skills .,fact
HJge1dvgz_77,"It seems that all the skills here are essentially pick and place skills , which is fine ( these are interesting skills ) ,",evaluation
HJge1dvgz_78,"but the title seems like a peculiar jab at prior work not being "" diverse "" enough , which is simply misleading .",evaluation
ryOWEcdlM_0,This paper studies the critical points of shallow and deep linear networks .,fact
ryOWEcdlM_1,The authors give a ( necessary and sufficient ) characterization of the form of critical points and use this to derive necessary and sufficient conditions for which critical points are global optima .,fact
ryOWEcdlM_2,Essentially this paper revisits a classic paper by Baldi and Hornik ( 1989 ) and relaxes a few requires assumptions on the matrices .,fact
ryOWEcdlM_3,I have not checked the proofs in detail but the general strategy seems sound .,evaluation
ryOWEcdlM_4,While the exposition of the paper can be improved,request
ryOWEcdlM_5,in my view this is a neat and concise result and merits publication in ICLR .,evaluation
ryOWEcdlM_6,The authors also study the analytic form of critical points of a single-hidden layer ReLU network .,fact
ryOWEcdlM_7,"However , given the form of the necessary and sufficient conditions the usefulness of of these results is less clear .",evaluation
ryOWEcdlM_8,Detailed comments : - I think in the title/abstract/intro the use of Neural nets is somewhat misleading as neural nets are typically nonlinear .,evaluation
ryOWEcdlM_9,This paper is mostly about linear networks .,fact
ryOWEcdlM_10,While a result has been stated for single-hidden ReLU networks .,fact
ryOWEcdlM_11,In my view this particular result is an immediate corollary of the result for linear networks .,fact
ryOWEcdlM_12,"As I explain further below given the combinatorial form of the result , the usefulness of this particular extension to ReLU network is not very clear .",evaluation
ryOWEcdlM_13,I would suggest rewording title/abstract/intro,request
ryOWEcdlM_14,"- Theorem 1 is neat , well done !",evaluation
ryOWEcdlM_15,- Page 4 p_i ’s in proposition 1,request
ryOWEcdlM_16,From my understanding the p_i have been introduced in Theorem 1,fact
ryOWEcdlM_17,but given their prominent role in this proposition they merit a separate definition ( and ideally in terms of the A_i directly ) .,request
ryOWEcdlM_18,"- Theorems 1 , prop 1 , prop 2 , prop 3 , Theorem 3 , prop 4 and 5 Are these characterizations computable i.e. given X and Y can one run an algorithm to find all the critical points or at least the parameters used in the characterization p_i , V_i etc ?",request
ryOWEcdlM_19,"- Theorems 1 , prop 1 , prop 2 , prop 3 , Theorem 3 , prop 4 and 5 Would recommend a better exposition why these theorems are useful .",request
ryOWEcdlM_20,What insights do you gain by knowing these theorems etc .,request
ryOWEcdlM_21,Are less sufficient conditions that is more intuitive or useful .,request
ryOWEcdlM_22,( an insightful sufficient condition in some cases is much more valuable than an unintuitive necessary and sufficient one ) .,evaluation
ryOWEcdlM_23,- Page 5 Theorem 2 Does this theorem have any computational implications ?,request
ryOWEcdlM_24,"Does it imply that the global optima can be found efficiently , e.g. are saddles strict with a quantifiable bound ?",request
ryOWEcdlM_25,- Page 7 proposition 6 seems like an immediate consequence of Theorem 1,fact
ryOWEcdlM_26,however given the combinatorial nature of the <VAR> it is not clear why this theorem is useful .,evaluation
ryOWEcdlM_27,e.g . back to my earlier comment w.r.t. Linear networks given Y and X can you find the parameters of this characterization with a computationally efficient algorithm ?,request
BJTS11qlz_0,The paper proposes a method for learning object representations from pixels and then use such representations for doing reinforcement learning .,fact
BJTS11qlz_1,This method is based on convnets that map raw pixels to a mask and feature map .,fact
BJTS11qlz_2,The mask contains information about the presence/absence of objects in different pixel locations and the feature map contains information about object appearance .,fact
BJTS11qlz_3,"I believe that the current method can only learn and track simple objects in a constant background , a problem which is well-solved in computer vision .",fact
BJTS11qlz_4,"Specifically , a simple method such as "" background subtraction "" can easily infer the mask ( the outlying pixels which correspond to moving objects ) while simple tracking methods ( see a huge literature over decades on computer vision ) can allow to track these objects across frames .",fact
BJTS11qlz_5,The authors completely ignore all this previous work,fact
BJTS11qlz_6,"and their "" related work "" section starts citing papers from 2016 and onwards !",fact
BJTS11qlz_7,"Is it any benefit of learning objects with the current ( very expensive ) method compared to simple methods such as "" background subtraction "" ?",evaluation
BJTS11qlz_8,"Furthermore , the paper is very badly written",evaluation
BJTS11qlz_9,since it keeps postponing the actual explanations to later sections ( while these sections eventually refer to the appendices ) .,fact
BJTS11qlz_10,This makes reading the paper very hard .,evaluation
BJTS11qlz_11,"For example , during the early sections you keep referring to a loss function which will allow for learning the objects , but you never really give the form of this loss ( which you should as soon as you mentioning it )",fact
BJTS11qlz_12,and the reader needs to search into the appendices to find out what is happening .,evaluation
BJTS11qlz_13,"Also , experimental results are very preliminary and not properly analyzed .",evaluation
BJTS11qlz_14,For example the results in Figure 3 are unclear and need to be discussed in detail in the main text .,request
HkQOWPieM_0,"This paper considers the following model of a signal <EQN> , where h is an m-dimensional random sparse vector , W is an m by n matrix , b is an n dimensional fixed bias vector .",fact
HkQOWPieM_1,"The random vector h follows an iid sparse signal model ,",fact
HkQOWPieM_2,"each coordinate independently have some probability of being zero ,",fact
HkQOWPieM_3,and the remaining probability is distributed among nonzero values according to some reasonable pdf/pmf .,fact
HkQOWPieM_4,"The task is to recover h , from the observation x via the activation functions like Sigmoid or ReLU .",fact
HkQOWPieM_5,"For example , <EQN> .",fact
HkQOWPieM_6,"The authors then show that , under the random sparsity model of h , it is possible to upper bound the probability <EQN> in terms of the parameters of the distribution of h and W and b.",fact
HkQOWPieM_7,In some cases noise can also be tolerated .,fact
HkQOWPieM_8,"In particular , if W is incoherent ( columns being near-orthonormal ) , then the guarantee is stronger .",fact
HkQOWPieM_9,"As far as I understood , the proofs make sense",evaluation
HkQOWPieM_10,- they basically use Chernoff-bound type argument .,fact
HkQOWPieM_11,It is my impression that a lot of conditions have to be satisfied for the recovery guarantee to be meaningful .,evaluation
HkQOWPieM_12,I am unsure if real datasets will satisfied so many conditions .,evaluation
HkQOWPieM_13,"Also , the usual objective of autoencoders is to denoise - i.e. recover x , without any access to W.",fact
HkQOWPieM_14,The authors approach in this vein seem to be only empirical .,evaluation
HkQOWPieM_15,Some recent works on associative memory also assume the sparse recovery model,fact
HkQOWPieM_16,- connections to this literature would have been of interest .,evaluation
HkQOWPieM_17,It is also not clear why compressed sensing-type recovery using a single ReLU or Sigmoid would be of interest :,evaluation
HkQOWPieM_18,are their complexity benefits ?,request
rkW8HjOlz_0,"The paper is easy to read for a physicist ,",evaluation
rkW8HjOlz_1,but I am not sure how useful it would be for ICLR ...,evaluation
rkW8HjOlz_2,it is not clear for me it there is an interest for quantum problems in this conference .,evaluation
rkW8HjOlz_3,This is something I will let to the Area Chair to deceede .,evaluation
rkW8HjOlz_4,"Other than this , the paper is interesting , certainly correct , and provides a nice perspective on the future of learning with quantum computers .",evaluation
rkW8HjOlz_5,"I like the quantum "" boltzmann machine "" problems .",evaluation
rkW8HjOlz_6,"I feel , however , but it might be a bit far from the main interest of the conference .",evaluation
rkW8HjOlz_7,"Comments : * What the authors called "" Free energy-based reinforcement learning "" seems to me just the minimization / maximiation of the free energy .",evaluation
rkW8HjOlz_8,This is simply maximum likelihood applied to the free energy,evaluation
rkW8HjOlz_9,"and I think that calling it "" reinforcement learning "" is not only wrong , but also is very confusing ,",evaluation
rkW8HjOlz_10,given this is usually reserved to an entirely different learning process .,evaluation
rkW8HjOlz_11,"* While i liked the introduction of the quantum Boltzmann machine , I would be happy to learn what they can do ?",request
rkW8HjOlz_12,"Are these useful , for instance , to study correlated fermions/bosons ?",non-arg
rkW8HjOlz_13,The paper does not explain why one should be concerns with these devices .,fact
rkW8HjOlz_14,"* The fact that the simulation on a classical computer agrees with the one on a quantum computer is promising ,",evaluation
rkW8HjOlz_15,"but I would say that this shows that , so far , there is not yet a clear advantage in using a quantum computer .",evaluation
rkW8HjOlz_16,"This might change , but in the mean time , what is the benefits for the ICLR community ?",non-arg
BJSfkUNzf_0,The paper addresses the problem of tensor decomposition,fact
BJSfkUNzf_1,which is relevant and interesting .,evaluation
BJSfkUNzf_2,The paper proposes Tensor Ring ( TR ) decomposition which improves over and bases on the Tensor Train ( TT ) decomposition method .,fact
BJSfkUNzf_3,TT decomposes a tensor in to a sequences of latent tensors where the first and last tensors are a 2D matrices .,fact
BJSfkUNzf_4,The proposed TR method generalizes TT in that the first and last tensors are also 3rd-order tensors instead of 2nd-order .,fact
BJSfkUNzf_5,I think such generalization is interesting,evaluation
BJSfkUNzf_6,but the innovation seems to be very limited .,evaluation
BJSfkUNzf_7,"The paper develops three different kinds of solvers for TR decomposition , i.e. , SVD , ALS and SGD .",fact
BJSfkUNzf_8,All of these are well known methods .,evaluation
BJSfkUNzf_9,"Finally , the paper provides experimental results on synthetic data ( 3 oscillated functions ) and image data ( few sampled images ) .",fact
BJSfkUNzf_10,I think the paper could be greatly improved by providing more experiments and ablations to validate the benefits of the proposed methods .,request
BJSfkUNzf_11,Please refer to below for more comments and questions .,non-arg
BJSfkUNzf_12,Pros : 1 . The topic is interesting .,evaluation
BJSfkUNzf_13,2 . The generalization over TT makes sense .,evaluation
BJSfkUNzf_14,Cons : 1 . The writing of the paper could be improved and more clear :,request
BJSfkUNzf_15,"the conclusions on inner product and F-norm can be integrated into "" Theorem 5 "" .",request
BJSfkUNzf_16,"And those "" theorems "" in section 4 are just some properties from previous definitions ;",fact
BJSfkUNzf_17,they are not theorems .,fact
BJSfkUNzf_18,2 . The property of TR decomposition is that the tensors can be shifted ( circular invariance ) .,fact
BJSfkUNzf_19,This is an interesting property and it seems to be the major strength of TR over TT .,evaluation
BJSfkUNzf_20,I think the paper could be significantly improved by providing more applications of this property in both theory and experiments .,request
BJSfkUNzf_21,"3 . As the number of latent tensors increase , the ALS method becomes much worse approximation of the original optimization .",fact
BJSfkUNzf_22,Any insights or results on the optimization performance vs. the number of latent tensors ?,non-arg
BJSfkUNzf_23,"4 . Also , the paper mentions Eq . 5 ( ALS ) is optimized by solving d subproblems alternatively .",fact
BJSfkUNzf_24,I think this only contains a single round of optimization .,fact
BJSfkUNzf_25,Should ALS be applied repeated ( each round solves d problems ) until convergence ?,request
BJSfkUNzf_26,5 . What is the memory consumption for different solvers ?,request
BJSfkUNzf_27,6 . SGD also needs to update at least d times for all d latent tensors .,fact
BJSfkUNzf_28,Why is the complexity <VAR> independent of the parameter d ?,request
BJSfkUNzf_29,"7 . The ALS is so slow ( if looking at the results in section 5.1 ) , which becomes not practical .",evaluation
BJSfkUNzf_30,The experimental part could be improved by providing more results and description about a guidance on how to choose from different solvers .,request
BJSfkUNzf_31,"8 . What does "" iteration "" mean in experimental results such as table 2 ?",request
BJSfkUNzf_32,"Different algorithms have different cost for "" each iteration """,fact
BJSfkUNzf_33,so comparing that seems not fair .,evaluation
BJSfkUNzf_34,The results could make more sense by providing total time consumptions and time cost per iteration .,request
BJSfkUNzf_35,also applies to table 4 .,request
BJSfkUNzf_36,9 . Why is the <VAR> in table 3 not consistent ?,request
BJSfkUNzf_37,Why not choose <EQN> and <EQN> for tensorization ?,request
BJSfkUNzf_38,"10 . Also , table 3 could be greatly improved by providing more ablations such as results for ( <EQN> , <EQN> ) , ( <EQN> , <EQN> ) , etc .",request
BJSfkUNzf_39,That could help readers to better understand the effect of TR .,evaluation
BJSfkUNzf_40,11 . Section 5.3 could be improved by providing a curve ( compression vs. error ) instead of just providing a table of sampled operating points .,request
BJSfkUNzf_41,12 . The paper mentions the application of image representation but only experiment on 32x32 images .,fact
BJSfkUNzf_42,How does the proposed method handle large images ?,request
BJSfkUNzf_43,"Otherwise , it does not seem to be a practical application .",evaluation
BJSfkUNzf_44,13 . Figure 5 : Are the RSE measures computed over the whole CIFAR-10 dataset or the displayed images ?,request
BJSfkUNzf_45,"Minor : - Typo : Page 4 Line 7 "" Note that this algorithm use the similar strategy "" : use - > uses",request
SkNxPOYlf_0,"The analyses of this paper ( 1 ) increasing the feature norm of correctly-classified examples induce smaller training loss , ( 2 ) increasing the feature norm of mis-classified examples upweight the contribution from hard examples , are interesting .",evaluation
SkNxPOYlf_1,The reciprocal norm loss seems to be reasonable idea to improve the CNN learning based on the analyses .,evaluation
SkNxPOYlf_2,"However , the presentation of this paper need to be largely improved .",request
SkNxPOYlf_3,"For example , Figure 3 seems to be not relevant to Property2",evaluation
SkNxPOYlf_4,and may be show the feature norm is lower when the samples is hard example .,fact
SkNxPOYlf_5,"Therefore , the author used reciprocal norm loss which increases feature norm as shown in Figure 4 .",fact
SkNxPOYlf_6,"However , both Figures are not explained in the main text ,",fact
SkNxPOYlf_7,and thus hard to understand the relation of Figure 3 and 4 .,evaluation
SkNxPOYlf_8,The author should refer all Figures and Tables .,request
SkNxPOYlf_9,Other issues are : - Large-margin Soft max in Figure 2 is not explained in the introduction section .,fact
SkNxPOYlf_10,"- In Eq . ( 7 ) , <VAR> is not defined .",fact
SkNxPOYlf_11,"- In the Property3 , The author wrote “ where r is lower bound of feature norm ” .",quote
SkNxPOYlf_12,"However , r is not used .",fact
SkNxPOYlf_13,"- In the experimental results , “ RN ” is not defined .",fact
SkNxPOYlf_14,"- In the Table3 , the order of \ lambda should be increasing or decreasing order .",request
SkNxPOYlf_15,- Table 5 is not referred in the main text .,fact
r18RxrXlG_0,"The authors present Hilbert-CNN , a convolutional neural network for DNA sequence classification .",fact
r18RxrXlG_1,"Unlike existing methods , their model does not use the raw one-dimensional ( 1D ) DNA sequence as input , but two-dimensional ( 2D ) images obtained by mapping sequences to images using spacing-filling Hilbert-Curves .",fact
r18RxrXlG_2,They further present a model ( Hilbert-CNN ) that is explicitly designed for Hilbert-transformed DNA sequences .,fact
r18RxrXlG_3,The authors show that their approach can increase classification accuracy and decrease training time when applied to predicting histone-modification marks and splice junctions .,fact
r18RxrXlG_4,Major comments ============= 1 . The motivation of transforming sequences into images is unclear,evaluation
r18RxrXlG_5,and claimed benefits are not sufficiently supported by experiments .,evaluation
r18RxrXlG_6,The essence of deep neural networks is to learn a hierarchy of features from the raw data instead of engineering features manually .,fact
r18RxrXlG_7,Using space filling methods such as Hilbert-curves to transform ( DNA ) sequences into images can be considered as unnecessary feature-engineering .,evaluation
r18RxrXlG_8,"The authors claim that ‘ CNNs have proven to be most powerful when operating on multi-dimensional input , such as in image classification ’ , which is wrong .",fact
r18RxrXlG_9,"Sequence-based convolutional and recurrent models have been successfully applied for modeling natural languages ( translation , sentiment classification , … ) , acoustic signals ( speech recognition , audio generation ) , or biological sequences ( e.g. predicting various epigenetic marks from DNA as reviewed in Angermueller et al ) .",fact
r18RxrXlG_10,They further claim that their method can ‘ better take the spatial features of DNA sequences into account ’ and can better model ‘ long-term interactions ’ between distant regions .,fact
r18RxrXlG_11,This is not obvious,evaluation
r18RxrXlG_12,"since Hilbert-curves map adjacent sequence characters to pixels that are close to each other as described by the authors , but distant characters to distant pixels .",fact
r18RxrXlG_13,"Hence , 2D CNN must be deep enough for modeling interactions between distant image features , in the same way as a 1D CNN .",fact
r18RxrXlG_14,Transforming sequences to images has several drawbacks .,evaluation
r18RxrXlG_15,"1 ) Since the resulting images have a small width and height but many channels ,",fact
r18RxrXlG_16,"existing 2D CNNs such as ResNet or Inception can not be applied ,",fact
r18RxrXlG_17,which also required the authors to design a specific model ( Hilbert-CNN ) .,fact
r18RxrXlG_18,2 ) Hilbert-CNN requires more memory due to empty image regions .,fact
r18RxrXlG_19,"3 ) Due to the high number of channels , convolutional filters have more parameters .",fact
r18RxrXlG_20,"4 ) The sequence-to-image transformation makes model-interpretability hard , which is in particular important in biology .",evaluation
r18RxrXlG_21,"For example , motifs of the first convolutional layers can not be interpreted as sequence motifs ( as described in Angermueller et al )",fact
r18RxrXlG_22,and it is unclear how to analyze the influence of sequence characters using attention or gradient-based methods .,evaluation
r18RxrXlG_23,"The authors should more clearly motivate their model in the introduction , tone-down the benefit of sequence-to-image transformations , and discuss drawbacks of their model .",request
r18RxrXlG_24,This requires major changes of introduction and discussion .,request
r18RxrXlG_25,2 . The authors should more clearly describe which and how they optimized hyper-parameters .,request
r18RxrXlG_26,"The authors should optimize the most important hyper-parameters of their model ( learning rate , batch size , weight decay , max vs. average pooling , ELU vs. ReLU , … ) and baseline models on a holdout validation set .",request
r18RxrXlG_27,"The authors should also report the validation accuracy for different sequence lengths , k-mer sizes , and space filling functions .",request
r18RxrXlG_28,Can their model be applied to longer sequences ( > = 1kbp ) which had been shown to improve performance ( e.g. 10.1101 / gr .200535.115 ) ?,request
r18RxrXlG_29,"Does Figure 4 show the performance on the training , validation , or test set ?",request
r18RxrXlG_30,"3 . It is unclear if the performance gain is due the proposed sequence-to-image transformation , or due to the proposed network architecture ( Hilbert-CNN ) .",evaluation
r18RxrXlG_31,It is also unclear if Hilbert-CNNs are applicable to DNA sequence classification tasks beyond predicting chromatin states and splice junctions .,evaluation
r18RxrXlG_32,"To address these points , the authors should compare Hilbert-CNN to models of the same capacity ( number of parameters ) and optimize hyper-parameters ( k-mer size , convolutional filter size , learning rate , … ) in the same way as they did for Hilbert-CNN .",request
r18RxrXlG_33,"The authors should report the number of parameters of all models ( Hilbert-CNN , Seq-CNN , 1D-sequence-CNN ( Table 5 ) , and LSTM ( Table 6 ) , … ) in an additional table .",request
r18RxrXlG_34,The authors should also compare Hilbert-CNN to the DanQ architecture on predicting epigenetic markers using the same dataset as reported in the DanQ publication ( <URL> ) .,request
r18RxrXlG_35,"The authors should also compare Hilbert-CNNs to gapped-kmer SVM , a shallow model that had been successfully applied for genomic prediction tasks .",request
r18RxrXlG_36,4 . The authors should report the AUC and area under precision-recall curve ( APR ) in additional to accuracy ( ACC ) in Table 3 .,request
r18RxrXlG_37,"5 . It is unclear how training time was measured for baseline models ( Seq-CNN , LSTM , … ) .",evaluation
r18RxrXlG_38,The authors should use the same early stopping criterion as they used for training Hilber-CNNs .,request
r18RxrXlG_39,The authors should also report the training time of SVM and gkm-SVM ( see comment 3 ) in Table 3 .,request
r18RxrXlG_40,"Minor comments ============= 1 . The authors should avoid uninformative adjectives and clutter throughout the manuscript , for example ‘ DNA is often perceived ’ , ‘ Chromatin can assume ’ , ‘ enlightening ’ , ‘ very ’ , ‘ we first have to realize ’ , ‘ do not mean much individually ’ , ‘ very much like the tensor ’ , ‘ full swing ’ , ‘ in tight communication ’ , ‘ two methods available in the literature ’ .",request
r18RxrXlG_41,The authors should point out in section two that k-mers can be overlapping .,request
r18RxrXlG_42,2 . Section 2.1 : One-hot vectors is not the only way for embedding words .,fact
r18RxrXlG_43,The authors should also mention Glove and word2vec .,request
r18RxrXlG_44,Similar approaches had been applied to protein sequences,fact
r18RxrXlG_45,( <URL> ),reference
r18RxrXlG_46,3 . The authors should more clearly describe how Hilbert-curves map sequences to images and how images are cropped .,request
r18RxrXlG_47,What does ‘ that is constructed in a recursive manner ’ mean ?,request
r18RxrXlG_48,Simply cropping the upper half of Figure 1c would lead to two disjoint sequences .,fact
r18RxrXlG_49,What is the order of Figure 1e ?,request
r18RxrXlG_50,4 . The authors should consistently use ‘ channels ’ instead of ‘ full vector of length ’ to denote the dimensionality of image pixels .,request
r18RxrXlG_51,5 . The authors should use ‘ Batch norm ’ instead of ‘ BN ’ in Figure 2 for clarification .,request
r18RxrXlG_52,6 . Hilber-CNN is similar to ResNet,evaluation
r18RxrXlG_53,"( <URL> ) ,",reference
r18RxrXlG_54,"which consists of multiple ‘ residual blocks ’ , where each block is a sequence of ‘ residual units ’ .",fact
r18RxrXlG_55,A ‘ computational block ’ in Hilbert-CNN contains two parallel ‘ residual blocks ’ ( Figure 3 ) instead of a sequence of ‘ residual units ’ .,fact
r18RxrXlG_56,"The authors should use ‘ residual block ’ instead of ‘ computational block ’ , and ‘ residual units ’ as in the original ResNet publication .",request
r18RxrXlG_57,The authors should also motivate why two residual units/blocks are applied parallely instead of sequentially .,request
r18RxrXlG_58,"7 . Caption table 1 : the authors should clarify if ‘ Output size ’ is ‘ height , width , channels ’ , and explain the notation in ‘ Description ’ ( or refer to the text . )",request
H1NffmKgz_0,This paper proposes the use of optimistic mirror descent to train Wasserstein Generative Adversarial Networks ( WGANS ) .,fact
H1NffmKgz_1,"The authors remark that the current training of GANs , which amounts to solving a zero-sum game between a generator and discriminator , is often unstable ,",fact
H1NffmKgz_2,"and they argue that one source of instability is due to limit cycles , which can occur for FTRL-based algorithms even in convex-concave zero-sum games .",fact
H1NffmKgz_3,"Motivated by recent results that use Optimistic Mirror Descent ( OMD ) to achieve faster convergence rates ( than standard gradient descent ) in convex-concave zero-sum games and normal form games , they suggest using these techniques for WGAN training as well .",fact
H1NffmKgz_4,"The authors prove that , using OMD , the last iterate converges to an equilibrium and use this as motivation that OMD methods should be more stable for WGAN training .",fact
H1NffmKgz_5,"They then compare OMD against GD on both toy simulations and a DNA sequence task before finally introducing an adaptive generalization of OMD , Optimistic Adam , that they test on CIFAR10 .",fact
H1NffmKgz_6,"This paper is relatively well-written and clear ,",evaluation
H1NffmKgz_7,"and the authors do a good job of introducing the problem of GAN training instability as well as the OMD algorithm , in particular highlighting its differences with standard gradient descent as well as discussing existing work that has applied it to zero-sum games .",evaluation
H1NffmKgz_8,"Given the recent work on OMD for zero-sum and normal form games , it is natural to study its effectiveness in training GANs .",evaluation
H1NffmKgz_9,The issue of last iterate versus average iterate for non convex-concave problems is also presented well .,evaluation
H1NffmKgz_10,"The theoretical result on last-iterate convergence of OMD for bilinear games is interesting , but somewhat wanting",evaluation
H1NffmKgz_11,"as it does not provide an explicit convergence rate as in Rakhlin and Sridharan , 2013 .",fact
H1NffmKgz_12,"Moreover , the result is only at best a motivation for using OMD in WGAN training",fact
H1NffmKgz_13,since the WGAN optimization problem is not a bilinear game .,fact
H1NffmKgz_14,"The experimental results seem to indicate that OMD is at least roughly competitive with GD-based methods ,",evaluation
H1NffmKgz_15,although they seem less compelling than the prior discussion in the paper would suggest .,evaluation
H1NffmKgz_16,"In particular , they are matched by SGD with momentum when evaluated by last epoch performance",fact
H1NffmKgz_17,( albeit while being less sensitive to learning rates ) .,fact
H1NffmKgz_18,"OMD does seem to outperform SGD-based methods when using the lowest discriminator loss ,",fact
H1NffmKgz_19,but there does n't seem to be even an attempt at explaining this in the paper .,fact
H1NffmKgz_20,"I found it a bit odd that Adam was not used as a point of comparison in Section 5 , that optimistic Adam was only introduced and tested for CIFAR but not for the DNA sequence problem ,",evaluation
H1NffmKgz_21,"and that the discriminator was trained for 5 iterations in Section 5 but only once in Section 6 ,",fact
H1NffmKgz_22,despite the fact that the reasoning provided in Section 6 seems like it would have also applied for Section 5 .,fact
H1NffmKgz_23,"This gives the impression that the experimental results might have been at least slightly "" gamed "" .",evaluation
H1NffmKgz_24,"For the reasons above , I give the paper high marks on clarity , and slightly above average marks on originality , significance , and quality .",evaluation
H1NffmKgz_25,"Specific comments : Page 1 , "" no-regret dynamics in zero-sum games can very often lead to limit cycles "" :",quote
H1NffmKgz_26,I do n't think limit cycles are actually ever formally defined in the entire paper .,fact
H1NffmKgz_27,"Page 3 , "" standard results in game theory and no-regret learning "" :",quote
H1NffmKgz_28,These results should be either proven or cited .,request
H1NffmKgz_29,Page 3 : Do n't the parameter spaces need to be bounded for these convergence results to hold ?,fact
H1NffmKgz_30,"Page 4 , "" it is well known that GD is equivalent to the Follow-the-Regularized-Leader algorithm "" :",quote
H1NffmKgz_31,"For completeness , this should probably either be ( quickly ) proven or a reference should be provided .",request
H1NffmKgz_32,"Page 5 , "" the unique equilibrium of the above game is ... for the discriminator to choose w = 0 "" :",quote
H1NffmKgz_33,Why is <EQN> necessary here ?,request
H1NffmKgz_34,"Page 6 , "" We remark that the set of equilibrium solutions of this minimax problem are pairs ( x , y ) such that x is in the null space of A ^ T and y is in the null space of A "" :",quote
H1NffmKgz_35,Why is this true ?,evaluation
H1NffmKgz_36,This should either be proven or cited .,request
H1NffmKgz_37,"Page 6 , Initialization and Theorem 1 : It would be good to discuss the necessity of this particular choice of initialization for the theoretical result .",request
H1NffmKgz_38,"In the Initialization section , it appears simply to be out of convenience .",evaluation
H1NffmKgz_39,"Page 6 , Theorem 1 : It should be explicitly stated that this result does n't provide a convergence rate , in contrast to the existing OMD results cited in the paper .",request
H1NffmKgz_40,"Page 7 , "" we considered momentum , Nesterov momentum and AdaGrad "" :",quote
H1NffmKgz_41,Why is n't Adam used in this section if it is used in later experiments ?,evaluation
H1NffmKgz_42,"Page 7-8 , "" When evaluated by .... the lowest discriminator loss on the validation set , WGAN trained with Stochastic OMD ( SOMD ) achieved significantly lower KL divergence than the competing SGD variants . "" :",quote
H1NffmKgz_43,Can you explain why SOMD outperforms the other methods when using the lowest discriminator loss on the validation set ?,request
H1NffmKgz_44,None of the theoretical arguments presented earlier in the paper seem to even hint at this .,evaluation
H1NffmKgz_45,The only result that one might expect from the earlier discussion and results is that SOMD would outperform the other methods when evaluating by the last epoch .,fact
H1NffmKgz_46,"However , this does n't even really hold ,",fact
H1NffmKgz_47,since there exist learning rates in which SGD with momentum matches the performance of SOMD .,fact
H1NffmKgz_48,"Page 8 , "" Evaluated by the last epoch , SOMD is much less sensitive to the choice of learning rate than the SGD variants "" :",quote
H1NffmKgz_49,Learning rate sensitivity does n't seem to be touched upon in the earlier discussion .,fact
H1NffmKgz_50,Can these results be explained by theory ?,non-arg
H1NffmKgz_51,"Page 8 , "" we see that optimistic Adam achieves high numbers of inception scores after very few epochs of training "" :",quote
H1NffmKgz_52,These results do n't mean much without error bars .,evaluation
H1NffmKgz_53,"Page 8 , "" we only trained the discriminator once after one iteration of generator training . The latter is inline with the intuition behind the use of optimism .... "" :",quote
H1NffmKgz_54,"Why did n't this logic apply to the previous section on DNA sequences , where the discriminator was trained multiple times ?",non-arg
HJmBCpKeG_0,The ms applies an LSTM on ECoG data and studies tranfer between subjects etc .,fact
HJmBCpKeG_1,The data includes only few samples per class .,fact
HJmBCpKeG_2,The validation procedure to obtain the model accuray is a bit iffy .,evaluation
HJmBCpKeG_3,The ms says : The test data contains ' at least 2 samples per class ' .,fact
HJmBCpKeG_4,"Data of the type analysed is highly dependend ,",fact
HJmBCpKeG_5,so it is not unclear whether this validation procedure will not provide overoptimistic results .,evaluation
HJmBCpKeG_6,"Currently , I do not see evidence for a stable training procedure in the ms.",fact
HJmBCpKeG_7,I would be curious also to see a comparison to a k-NN classifier using embedded data to gauge the problem difficulty .,request
HJmBCpKeG_8,"Also , the paper does not really decide whether it is a neuroscience contribution or an ML one .",fact
HJmBCpKeG_9,"If it were a neuroscience contribution , then it would be important to analyse and understand the LSTM representation and to put it into a biological context",evaluation
HJmBCpKeG_10,fig 5B is a first step in this direction .,fact
HJmBCpKeG_11,"If it where a ML contribution , then there should be a comprehensive analysis that indeed the proposed architecture using the 2 steps is actually doing the right thing , i.e. that the method converges to the truth if more and more data is available .",request
HJmBCpKeG_12,There is also some initial experiments in fig 3A .,fact
HJmBCpKeG_13,"Currently , I find the paper somewhat unsatisfactory and thus preliminary .",evaluation
ryBakJUlz_0,"This is a dense , rich , and impressive paper on rapid meta-learning .",evaluation
ryBakJUlz_1,"It is already highly polished ,",evaluation
ryBakJUlz_2,so I have mostly minor comments .,evaluation
ryBakJUlz_3,"Related work : I think there is a distinction between continual and life-long learning ,",fact
ryBakJUlz_4,and I think that your proposed setup is a form of continual learning ( see Ring ‘ 94 / ‘ 97 ) .,fact
ryBakJUlz_5,"Given the proliferation of terminology for very related setups ,",fact
ryBakJUlz_6,I ’d encourage you to reuse the old term .,request
ryBakJUlz_7,"Terminology : I find it confusing which bits are “ meta ” and which are not ,",evaluation
ryBakJUlz_8,and the paper could gain clarity by making this consistent .,request
ryBakJUlz_9,"In particular , it would be good to explicitly name the “ meta-loss ” ( currently the unnamed triple expectation in ( 3 ) ) .",request
ryBakJUlz_10,"By definition , then , the “ meta-gradient ” is the gradient of the meta-loss -- and not the one in ( 2 ) , which is the gradient of the regular loss .",fact
ryBakJUlz_11,Notation : there ’s redundancy/inconsistency in the reward definition :,evaluation
ryBakJUlz_12,"pick either R_T or <VAR> , not both , and maybe include R_T in the task tuple definition ?",request
ryBakJUlz_13,"It is also confusing that <VAR> is a loss , not a reward ( and is minimized )",evaluation
ryBakJUlz_14,-- maybe use another symbol ?,request
ryBakJUlz_15,"A question about the importance sampling correction : given that this spans multiple ( long ) trajectories , do n’t the correction weights become really small in practice ?",non-arg
ryBakJUlz_16,Do you have some ballpark numbers ?,non-arg
ryBakJUlz_17,Typos : - “ event their learning ”,fact
ryBakJUlz_18,- “ in such setting ”,fact
ryBakJUlz_19,- “ experience to for ”,fact
rkA0vi8gz_0,This work exploits the causality principle to quantify how the weights of successive layers adapt to each other .,fact
rkA0vi8gz_1,"Some interesting results are obtained , such as "" enforcing more independence between successive layers of generators may lead to better performance and modularity of these architectures "" .",evaluation
rkA0vi8gz_2,"Generally , the result is interesting and the presentation is easy to follow .",evaluation
rkA0vi8gz_3,"However , the proposed approach and the experiments are not convincible enough .",evaluation
rkA0vi8gz_4,"For example , it is hard to obtain the conclusion "" more independence lead to better performance "" from the experimental results .",evaluation
rkA0vi8gz_5,Maybe more justifications are needed .,request
BJGc-k9xG_0,This paper provides the analysis of empirical risk landscape for GENERAL deep neural networks ( DNNs ) .,fact
BJGc-k9xG_1,Assumptions are comparable to existing results for OVERSIMPLIFED shallow neural networks .,evaluation
BJGc-k9xG_2,The main results analyzed : 1 ) Correspondence of non-degenerate stationary points between empirical risk and the population counterparts .,fact
BJGc-k9xG_3,2 ) Uniform convergence of the empirical risk to population risk .,fact
BJGc-k9xG_4,3 ) Generalization bound based on stability .,fact
BJGc-k9xG_5,The theory is first developed for linear DNNs and then generalized to nonlinear DNNs with sigmoid activations .,fact
BJGc-k9xG_6,"Here are two detailed comments : 1 ) For deep linear networks with squared loss , Kawaguchi 2016 has shown that the global optima are the only non-degerenate stationary points .",fact
BJGc-k9xG_7,"Thus , the obtained non-degerenate stationary deep linear network should be equivalent to the linear regression model Y = XW .",fact
BJGc-k9xG_8,Should the risk bound only depends on the dimensions of the matrix W ?,non-arg
BJGc-k9xG_9,"2 ) The comparison with Bartlett & Maass ’s ( BM ) work is a bit unfair ,",evaluation
BJGc-k9xG_10,because their result holds for polynomial activations while this paper handles linear activations .,fact
BJGc-k9xG_11,"Thus , the authors need to refine BM 's result for comparison .",request
S1dRXMqxG_0,In this paper authors are summarizing their work on building a framework for automated neural network ( NN ) construction across multiple tasks simultaneously .,fact
S1dRXMqxG_1,They present initial results on the performance of their framework called Multitask Neural Model Search ( MNMS ) controller .,fact
S1dRXMqxG_2,The idea behind building such a framework is motivated by the successes of recently proposed reinforcement based approaches for finding the best NN architecture across the space of all possible architectures .,evaluation
S1dRXMqxG_3,Authors cite the Neural Architecture Search ( NAS ) framework as an example of such a framework that yields better results compared to NN architectures configured by humans .,fact
S1dRXMqxG_4,Overall I think that the idea is interesting and the work presented in this paper is very promising .,evaluation
S1dRXMqxG_5,Given the depth of the empirical analysis presented the work still feels that it ’s in its early stages .,evaluation
S1dRXMqxG_6,In its current state and format the major issue with this work is the lack of more in-depth performance analysis which would help the reader draw more solid conclusions about the generalization of the approach .,evaluation
S1dRXMqxG_7,Authors use two text classification tasks from the NLP domain to showcase the benefits of their proposed architecture .,fact
S1dRXMqxG_8,"It would be good if they could expand and analyze how well does their framework generalizes across other non-binary tasks , tasks in other domains and different NNs .",request
S1dRXMqxG_9,This is especially the case for the transfer learning task .,evaluation
S1dRXMqxG_10,"In the NAS overview section , readers would benefit more if authors spend more time in outlining the RL detail used in the original NAS framework instead of Figure 1 which looks like a space filler .",request
S1dRXMqxG_11,Across the two NLP tasks authors show that MNMS models trained simultaneously give better performance than hand tuned architectures .,fact
S1dRXMqxG_12,"In addition , on the transfer learning evaluation approach they showcase the benefit of using the proposed framework in terms of the initially retrieved architecture and the number of iterations required to obtain the best performing one .",fact
S1dRXMqxG_13,For better clarity figures 3 and 5 should be made bigger .,request
S1dRXMqxG_14,What is LSS in figure 4 ?,request
Hk9a7-qlG_0,The submission tackles an important problem of learning and transferring multiple motor skills .,evaluation
Hk9a7-qlG_1,The approach relies on using an embedding space defined by latent variables and entropy-regularized policy gradient / variational inference formulation that encourages diversity and identifiability in latent space .,fact
Hk9a7-qlG_2,The exposition is clear and the method is well-motivated .,evaluation
Hk9a7-qlG_3,I see no issues with the mathematical correctness of the claims made in the paper .,evaluation
Hk9a7-qlG_4,"The experimental results are both instructive of how the algorithm operates ( in the particle example ) , and contain impressive robotic results .",evaluation
Hk9a7-qlG_5,"I appreciated the experiments that investigated cases where true number of tasks and the parameter T differ , showing that the approach is robust to choice of T.",evaluation
Hk9a7-qlG_6,The submission focuses particularly on discrete tasks and learning to sequence discrete tasks ( as training requires a one-hot task ID input ) .,evaluation
Hk9a7-qlG_7,"I would like a bit of discussion on whether parameterized skills ( that have continuous space of target location , or environment parameters , for example ) can be supported in the current formulation , and what would be necessary if not .",request
Hk9a7-qlG_8,"Overall , I believe this is in interesting piece of work at a fruitful intersection of reinforcement learning and variational inference ,",evaluation
Hk9a7-qlG_9,and I believe would be of interest to ICLR community .,evaluation
